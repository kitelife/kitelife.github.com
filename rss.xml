<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>BitPacking</title>
        <description>精进，求诸己身</description>
        <link>https://blog.xiayf.cn/</link>
        <atom:link href="https://blog.xiayf.cn/rss.xml" rel="self" type="application/rss+xml"/>
        <pubDate>2025-04-21T00:26:00.680132+08:00</pubDate>
        <lastBuildDate>2025-04-21T00:26:00.680132+08:00</lastBuildDate>
        <generator>LingDong</generator>
        
        <item>
            <title>标量量化入门（译）</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;https://www.elastic.co/search-labs/blog/scalar-quantization-101&apos;&gt;Scalar quantization 101&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;多数嵌入（embedding）模型会输出 $float32$ 数值精度的向量。这个精度虽然提供了信息高保真，但在真正重要的信息之外也带来一些资源浪费。对于给定的数据集，嵌入不可能在单个维度需要20亿种取值，特别是对于高维度向量而言（比如：386维及以上）。量化以一种有损的方式对向量进行编码，轻微降低信息保真而明显地降低存储空间占用。&lt;/p&gt;
&lt;h2&gt;理解标量量化的分桶&lt;/h2&gt;
&lt;p&gt;标量量化使用更小的数据类型对向量每一维的取值进行分桶。本文的余下部分将假设将 $float32$ 量化到 $int8$ 。为了准确地对值进行分桶，不能简单地将值四舍五入到最近的整数。许多模型输出向量的维度取值空间为 $[-1.0, 1.0]$，如果简单粗暴地四舍五入处理，那么 0.123 和 0.321 这两个不同的向量维度取值都会向下取整到 0。最终，一个向量仅会使用 $int8$ 可用 255 个桶中的2个桶，这样就丢失太多信息了。&lt;/p&gt;
&lt;img src=&apos;../assets/float32-to-int8-buckets.jpeg&apos; title=&apos;float32-to-int8-buckets.jpeg&apos; alt=&apos;float32-to-int8-buckets.jpeg&apos; width=&apos;500&apos;&gt;
&lt;blockquote&gt;&lt;p&gt;图1：量化目标图解 - 将 -1.0 到 1.0 之间的连续值分桶到离散的 $int8$ 数值。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这种数值转换背后的数学原理并不太复杂。我们可以先计算浮点数取值区间的最小和最大值，然后使用 &lt;a href=&apos;https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization&apos;&gt;最小-最大归一化&lt;/a&gt;) 对值进行线性变换（linearly shift）。&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$int8\approx \frac{127}{max-min} \  \times \left( float32-min \right)$$&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$float32\approx \frac{max-min}{127} \times int8+min$$&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;图2：$int8$ 和 $float32$ 之间的变换公式。注意：这两个变换是有损的，并不是精确变换。下面的例子中，仅使用 $int8$ 取值空间的正数部分。Lucene 的实现也是这样的。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2&gt;标量量化的统计视角&lt;/h2&gt;
&lt;p&gt;&lt;a href=&apos;https://en.wikipedia.org/wiki/Quantile&apos;&gt;分位点（quantile）&lt;/a&gt; 是指数值分布的一个切片，这个切片包含一定数量比例的值。例如：一种浮点数取值分布下 99% 的值落在 $[-0.75, 0.86]$ 这个分位点区间内，小于 $-0.75$ 和大于 $0.86$ 的值都被视为离群值/异常值（outliers），因此将 $-0.75$ 和 $0.86$ 分别视为实际的最小值和最大值。如果量化时将离群值包含在内，就意味着那些最常见的值可用的桶偏少了，可用桶少了也就意味着精度更差，信息损失更多。&lt;/p&gt;
&lt;img src=&apos;../assets/quantile.jpeg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;blockquote&gt;&lt;p&gt;图3:图解 99%  &lt;a href=&apos;https://en.wikipedia.org/wiki/Confidence_interval&apos;&gt;置信区间(confidence interval)&lt;/a&gt;及对应的分位点数值，即 99% 的值落在 $[-0.75, 0.86]$ 这个范围内。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;不错，我们现在知道如何对浮点值进行量化了，那么又应该如何计算两个量化后向量的距离呢？就像常规的&lt;a href=&apos;https://en.wikipedia.org/wiki/Dot_product&apos;&gt;点积&lt;/a&gt;计算一样简单吗？&lt;/p&gt;
&lt;h2&gt;标量量化的代数视角&lt;/h2&gt;
&lt;p&gt;目前为止仍然缺失关键的一块拼图 - 如何计算两个量化后向量之间的距离。本文并没有有意避开数学公式，下面也会出现更多数学内容。拿出你的铅笔，回忆一下&lt;a href=&apos;https://en.wikipedia.org/wiki/Polynomial&apos;&gt;多项式&lt;/a&gt; 和基础代数。&lt;/p&gt;
&lt;p&gt;&lt;a href=&apos;https://en.wikipedia.org/wiki/Dot_product&apos;&gt;点积&lt;/a&gt;和&lt;a href=&apos;https://en.wikipedia.org/wiki/Cosine_similarity&apos;&gt;余弦相似度&lt;/a&gt;的计算逻辑是将两个向量对应维度上的浮点值相乘，然后将所有维度上的结果相加。我们已经知道如何在 $float32$ 和 $int8$ 值之间做变换，那么应用变换后的乘法公式是什么样的呢？&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$float32_{i}\times float32_{i}^{\prime}\approx \left( \frac{max-min}{127} \times int8_{i}+min \right) \times \left( \frac{max-min}{127} \times int8_{i}^{\prime}+min \right)$$&lt;/p&gt;
&lt;p&gt;将这个乘法公式展开后（为了简化，以 $\alpha$ 替代 $\frac{max-min}{127}$），如下所示：&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$\alpha^{2} \times int8_{i}\times int8_{i}^{\prime}+\alpha \times int8_{i}\times min+\alpha \times int8_{i}^{\prime}\times min+min^{2}$$&lt;/p&gt;
&lt;p&gt;接下来就更有意思了 - 这个算式中仅有一个部分要求同时提供两个变量值。然而，点积并不只是两个浮点数相乘，而是两个向量的每一维对应的浮点值相乘。假设向量的维度为 $dim$，那么以下部分算式都可以提前计算好存下来。&lt;/p&gt;
&lt;p&gt;$dim\times \alpha^{2}$ 即 $dim\times \left( \frac{max-min}{127} \right)^{2}$ ，可以提前计算好存为单个浮点数。&lt;/p&gt;
&lt;p&gt;$\sum_{i=0}^{dim-1} min\times \alpha \times int8_{i}$ 和 $\sum_{i=0}^{dim-1} min\times \alpha \times int8_{i}^{\prime}$ 都可以分别提前计算好存为单个浮点数，或者在检索时计算一次。&lt;/p&gt;
&lt;p&gt;$dim\times min^{2}$ 也可以提前计算好存为单个浮点数。&lt;/p&gt;
&lt;p&gt;那么：&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$dim\times \alpha^{2} \times dotProduct\left( int8,int8^{\prime} \right) +\sum_{i=0}^{dim-1} min\times \alpha \times int8_{i}+\sum_{i=0}^{dim-1} min\times \alpha \times int8_{i}^{\prime}+dim\times min^{2}$$&lt;/p&gt;
&lt;p&gt;点积的整个算式中仅 $dotProduct\left( int8,int8^{\prime} \right)$ 部分需要在检索时计算，加上其他提前计算好的部分就能得到结果。&lt;/p&gt;
&lt;h2&gt;量化的精度保证&lt;/h2&gt;
&lt;p&gt;那么，这样量化计算的准确性如何？量化后损失信息没有？是的，损失了一些信息，不过量化正是基于我们事实上并不需要所有信息的假设。对于训练得到的嵌入模型，向量各个维度的值分布通常不存在&lt;a href=&apos;https://en.wikipedia.org/wiki/Fat-tailed_distribution&apos;&gt;厚尾性(fat-tails)&lt;/a&gt;。这意味着值分布存在一定的局部性和一致性。此外，量化对每一维度引入的误差是相互独立的，这意味着对于向量的典型运算（比如点积），误差一定程序上会抵消。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;哟，一写就写了一堆内容。现在你应该很好地理解了量化的技术优势，其背后的数学原理，以及如何将线性变换考虑在内计算向量之间的距离。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>k-NN 乘积量化器教程-第2部分（译）</title>
            <description>&lt;p&gt;&lt;a href=&apos;http://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/&apos;&gt;本教程的第1部分&lt;/a&gt; 讲解了乘积量化器的最基础形式。本文将讲解 &lt;a href=&apos;https://github.com/facebookresearch/faiss/wiki/Getting-started-tutorial&apos;&gt;FAISS 库的 IndexIVFPQ 索引&lt;/a&gt;，该索引类型使用一个乘积量化器以及 &lt;a href=&apos;https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf&apos;&gt;2011 年发表的这篇论文&lt;/a&gt;介绍的一些额外的技术。&lt;/p&gt;
&lt;p&gt;下面先简要介绍一下该索引引入的两个特性，之后会再详细解释。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;倒排文件索引（IVF）&lt;/em&gt;  - IVF 就是一种数据集预过滤的技术，避免对所有向量进行穷举搜索。它的原理相当直观 - 使用 k-means 聚类算法提前将数据集聚类成一定数量的数据集分区，然后在检索时，先将查询向量与每个分区的质心做比较，找到最近的若干个聚类，然后只在这些聚类分区内做向量搜索。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;残差编码&lt;/em&gt; - 这是对乘积量化器基础形式的一种增强方式 - 加入 IVF 步骤的一些信息。对于每个数据库向量，不再使用 PQ 编码原始的数据库向量，而是对向量相对于所属分区的质心的偏移量（offset）进行编码。后续章节会解释其原理和收益。&lt;/p&gt;
&lt;h2&gt;倒排文件索引（Inverted File Index）&lt;/h2&gt;
&lt;p&gt;计算机科学领域，特别是信息检索领域，一个“倒排索引”是指将词汇表中的每个单词映射到数据库中所有文档中该单词出现的所有位置，它非常类似于课本中后面的索引表 - 将单词或概念映射到页号，所以大家将这种数据结构称为“倒排索引”让我有些困扰（因为于我而言它就是一种普通的索引！）。&lt;/p&gt;
&lt;p&gt;不管怎样，在当前上下文中，这个技术实际就是使用 k-means 聚类对数据集做分割，这样就可以仅对部分分区做搜索而忽略其余的。&lt;/p&gt;
&lt;p&gt;构建索引时，使用 k-means 聚类算法将数据集聚类成一定数量的分区。数据集中每个向量仅会被归属到一个聚类/分区中。每个分区包含归属于它的一组向量（也就是 FAISS 作者说的“倒排文件列表”）。也由此得到所有分区的质心组成的一个矩阵，用于计算应该对哪些分区进行搜索。&lt;/p&gt;
&lt;p&gt;按照这种方式对数据集进行分割，并不完美，因为如果一个查询向量实际位于最近聚类的边缘位置，那么查询向量的最近邻居可能实际位于多个附近的聚类中。这个问题的解决方案是简单地多搜索几个分区。搜索多个附近的分区显然会占用更多的检索时间，但是准确性也会更好。&lt;/p&gt;
&lt;p&gt;搜索的时候，将查询向量与所有分区的质心做比较，找到最近的若干个分区质心，实际的数量可以配置。一旦找到了最近的若干个分区质心，就可以仅对这些分区的数据库向量使用乘积量化器做 k-NN 搜索。&lt;/p&gt;
&lt;p&gt;注意该索引类型中使用的如下术语：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;“probe（搜寻）” 这个动词，在当前上下文中，是指选定待搜索的目标分区。因此在代码中你会看到索引参数“nprobe” - 意思就是“待搜寻的分区数量”。&lt;/li&gt;
&lt;li&gt;FAISS 的作者们喜欢使用“Voronoi 单元（cells）”这个词语，而不是我在本文中使用的“数据集分区（dataset partitions）”。一个 Voronoi 单元就是属于一个聚类的空间区域，也就是，这个空间区域涵盖了对应聚类的所有点，这些点的向量与这个聚类的质心的距离，比其他聚类的质心都要近。&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;残差编码（Encoding Residuals）&lt;/h2&gt;
&lt;p&gt;这个特性也是相对比较直观的，不过如果你没有理解的话可能会觉得有点奇怪。其想法是将 IVF 阶段的一些信息加入到乘积量化器中，借此提升准确性（因此这个概念是建立在数据集分区技术之上的）。&lt;/p&gt;
&lt;p&gt;先定义一下什么是“残差”向量。暂时抛开乘积量化器不谈（因为它会增大理解的困难，后面我们再把它加回来）。假设我们要做标准的暴力 k-NN 搜索，不过是用数据集分区技术来削减待搜索向量的数量。&lt;/p&gt;
&lt;p&gt;假设我们是用 k-means 将数据集聚类成 100 个聚类（或者叫“数据集分区”）。给定数据集中的一个向量，其残差即是它相对于所属分区的质心的偏移（offset）。也就是，将数据集中的这个向量与其所属聚类的质心的向量相减。质心即是聚类的均值，那么对一组点均减去它们的均值会发生什么？现在这些点就围绕着 0 点了。如下是一个简单的二维示例：&lt;/p&gt;
&lt;img src=&apos;../assets/residuals_one_partition.png&apos; title=&apos;residuals_one_partition.png&apos; alt=&apos;residuals_one_partition.png&apos; width=&apos;800&apos;&gt;
&lt;p&gt;开始有趣起来了。假设将一个数据集分区中的所有向量都替换为各自的残差向量，怎么从这个数据集分区中找到查询向量的最近邻？先计算查询向量的残差（相对于分区质心的偏移），然后对这个数据集分区的所有残差向量做最近邻搜索，得到的结果与使用原始向量做搜索是一样的！&lt;/p&gt;
&lt;p&gt;基于上面的图示，凭直觉可能就能理解，不过还是再看看下面的等式加深理解。&apos;x&apos; 和 &apos;y&apos; 这两个向量的长度为 &apos;n&apos;，它们的 L2 距离计算公式为：&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$dist_{L2}\left( x,y \right) =\sqrt{\sum_{i}^{n} \left( x_{i}-y_{i} \right)^{2}}$$&lt;/p&gt;
&lt;p&gt;如果对 &apos;x&apos; 和 &apos;y&apos; 都减去质心向量 &apos;c&apos;，看起来是什么样的？&lt;/p&gt;
&lt;p style=&quot;text-align: center&quot;&gt;$$dist_{L2}\left( x-c,y-c \right) =\sqrt{\sum_{i}^{n} \left( (x_{i}-c_{i})-(y_{i}-c_{i}) \right)^{2}} =\sqrt{\sum_{i}^{n} \left( x_{i}-y_{i} \right)^{2}}$$&lt;/p&gt;
&lt;p&gt;质心部分被抵消掉了！&lt;/p&gt;
&lt;p&gt;注意使用残差计算出来的距离不只是相对而言（比如距离的序）是相等的，并且确实是正确地计算出了向量之间的 L2 距离。&lt;/p&gt;
&lt;p&gt;可能你之前就使用过这种等价关系，均值归一化（对向量减去均值）是一种常见的预处理技术。&lt;/p&gt;
&lt;p&gt;不过目前为止说的都是单个分区内的计算。将不同分区内的向量做比较又会是什么情况呢？仍然管用，只要针对每个分区分别计算查询向量的残差即可。&lt;/p&gt;
&lt;p&gt;下面这个图解中包含两个数据集分区。计算残差之后，两个分区的所有点都围绕在 0 点周围了。不过现在是有两个查询向量的残差 - 一个是与蓝色点集（分区 1）比较得到的，另一个是与绿色点集（分区 2）比较得到的。&lt;/p&gt;
&lt;img src=&apos;../assets/residuals_two_partitions.png&apos; title=&apos;residuals_two_partitions.png&apos; alt=&apos;residuals_two_partitions.png&apos; width=&apos;800&apos;&gt;
&lt;p&gt;前面解释过查询向量和数据库向量之间的距离，使用原始向量计算和使用残差向量计算，结果是一样的，还记得吧？&lt;/p&gt;
&lt;p&gt;很有意思，不过目前为止还是无用功 - 尚未改变结果的准确性也没有减少计算成本。现在将 PQ 重新放进来一起考虑，就会发现好处在哪了。&lt;/p&gt;
&lt;p&gt;在训练乘积量化器之前，先计算数据集所有分区所有向量的残差向量。残差向量集合保持原有分区（不会合并在一起），不过现在所有残差向量都围绕着 0 点，相对紧凑地聚集在一起。我们抛弃掉原始的数据集向量，只存储残差向量集。&lt;/p&gt;
&lt;p&gt;在所有这些残差向量之上训练习得一个乘积量化器，不再使用原始向量。那有什么不同之处吗？回想一下：乘积量化器的训练过程是先将向量分割成子向量，在每部分子向量之上进行 k-means 聚类，学习到一组原型/质心（或者叫“码本”）用于表征所有向量。使用对应的残差向量来替换原始向量，能够降低数据集中向量的多样性（the variety in the dataset）（论文中，将此描述为：相对于原始向量，残差向量“包含更小的能量（have less energy）”）。之前，聚类存在于空间的各个区域，现在聚类都围绕着 0 点，并且相互之间还存在部分重合。降低了数据集中向量的多样性，就可能使用更少的原型/质心（或者说“代码”）来有效地表征向量！或者，换个角度来说，PQ 中数量有限的代码现在更加准确了，因为这些代码所要描述的向量，相比之前，相互之间区别更小了（less distinct）。我们得到了更多的回报（more bang for our buck）！&lt;/p&gt;
&lt;p&gt;不过，也是有代价的。回想一下：乘积量化器的魔力在于仅需要将查询向量分块与码本代码之间的部分距离计算出来存为一个相对比较小的表 - 剩下的操作就是查表和加法。&lt;/p&gt;
&lt;p&gt;现在，使用残差向量，对于每个数据集分区而言，查询向量都是不同的 - 对于每个数据集分区，查询向量对应的残差向量都需要基于分区的质心重新计算。因此，对于待搜寻的每个分区，都必须单独计算一个距离表！&lt;/p&gt;
&lt;p&gt;不过，这个取舍显然是值得的，实际应用中，IndexIVFPQ 索引的表现都很不错。&lt;/p&gt;
&lt;p&gt;就是这样。虽然数据库向量都被各自的残差向量替代了，不过对于乘积量化器来说，没什么不同。&lt;/p&gt;
&lt;p&gt;注意：数据集分区并不会考虑（factor in to）码本训练，我们仍然跨越分区使用所有数据集向量为每部分子向量习得一个码本。你也可以为每个数据集分区单独训练一个 PQ，不过 FAISS 库的作者不赞成这样做，因为分区的数量通常比较大，那么存储这些码本的内存开销会是一个问题。所以，跨分区在所有数据库向量之上训练习得一个 PQ 更好一些。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>k-NN 乘积量化器教程-第1部分（译）</title>
            <description>&lt;p&gt;乘积量化器是一种“向量量化器”（后面我会解释这是啥意思），可以用于加速近似最近邻检索。&lt;/p&gt;
&lt;p&gt;2017年3月发布的 &lt;a href=&apos;https://code.facebook.com/posts/1373769912645926/faiss-a-library-for-efficient-similarity-search/&apos;&gt;Facebook AI 相似性检索（FAISS）库&lt;/a&gt;，风靡一时，乘积量化器是其核心组件，吸引了很多人关注。&lt;/p&gt;
&lt;p&gt;本教程的第一部分将解释乘积量化器的最基础形式，ANN 检索中通常是这样实现。第二部分将解释 FAISS 中的 “IndexIVFPQ” 索引，这种索引在基础形式的乘积量化器上添加了不少特性。&lt;/p&gt;
&lt;h2&gt;近似距离穷举搜索&lt;/h2&gt;
&lt;p&gt;不同于 ANN 使用的基于树的索引，单独使用乘积量化器的 k-NN 检索仍然是一种“穷举搜索”，这意味着乘积量化器仍然需要将查询向量（query vector）和数据库中所有向量做比较。乘积量化器的核心是近似地且显著地简化向量的距离计算。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;注意：FAISS 中的 IndexIVFPQ 索引在使用乘积量化器之前会预先过滤数据集 - 第二部分会解释。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2&gt;以示例解释&lt;/h2&gt;
&lt;p&gt;乘积量化器方法的作者们是信号处理和压缩技术背景的，所以如果你是机器学习方向的，可能对他们的用词和术语比较陌生。不过，如果你熟悉 k-means 聚类（且摒弃所有压缩命名法的词汇），使用一个示例你就能轻松理解乘积量化器的基础知识。之后，我们再回头来看压缩技术相关术语。&lt;/p&gt;
&lt;h2&gt;数据集压缩&lt;/h2&gt;
&lt;p&gt;假设你有一个 50,000 张的图片集，使用一个卷积神经网络（CNN）完成一些特征的抽取。这样你现在就得到一个 50,000个特征向量的数据集，每个特征向量有 1024 维。&lt;/p&gt;
&lt;img src=&apos;../assets/image_vectors.png&apos; title=&apos;image_vectors.png&apos; alt=&apos;image_vectors.png&apos; width=&apos;300&apos;&gt;
&lt;p&gt;我们要做的第一件事情就是压缩数据集。向量的数量保持不变，但是可以减少每个向量需要的存储空间。注意：我们要做的事情不同于“降维（dimensionality reduction）”！这是因为压缩后的向量中的值其实是符号而不是数值，所以不能直接比较压缩后的向量。&lt;/p&gt;
&lt;p&gt;压缩数据集有两大好处：（1）内存访问耗时通常是处理速度的限制因素，（2）对大数据集而言内存容量是个问题。&lt;/p&gt;
&lt;p&gt;压缩的原理如下所述：对于我们的示例数据集，将所有向量一起切成8个子向量，每个子向量的长度为 128（8个子向量 $\times$ 每个子向量128维 = 原始向量的 1024 维）。这样就将数据集分成8个矩阵，每个矩阵大小为 $[50K \times 128]$。&lt;/p&gt;
&lt;img src=&apos;../assets/vector_slice.png&apos; title=&apos;vector_slice.png&apos; alt=&apos;vector_slice.png&apos; width=&apos;500&apos;&gt;
&lt;p&gt;然后对这8个矩阵的每一个单独进行 k-means 聚类，k = 256。这样，对于向量的8个子段的每1个都存在256个质心 - 一共8组质心，每组包含256个质心。&lt;/p&gt;
&lt;img src=&apos;../assets/kmeans_clustering.png&apos; title=&apos;kmeans_clustering.png&apos; alt=&apos;kmeans_clustering.png&apos; width=&apos;500&apos;&gt;
&lt;p&gt;这些质心类似于“原型”。它们代表数据集子向量中最常见的模式。&lt;/p&gt;
&lt;p&gt;可以使用这些质心来压缩向量数据集 - 使用最接近/最相似的置信来替代向量中对应的每个子部分，从而得到一个不同于原始向量的一个新向量，不过它们之间应该还是相近的。&lt;/p&gt;
&lt;p&gt;这样我们就能更加高效地存储这些向量 - 不用存储原始的浮点数值，只要存储聚类中心 ID 即可 - 对每个子向量，找到最近的质心，存储该质心的 ID。每个向量也就被替换为8个质心 ID 的一个序列。&lt;/p&gt;
&lt;p&gt;注意：对于8个子部分矩阵的每一个学习到的质心集合是不同的。使用最近质心 id 替换子向量时，只能与对应子部分的 256 个质心做比较。&lt;/p&gt;
&lt;p&gt;每个子部分只有 256 个质心，所以仅需 8 比特就能存储一个质心 ID。每个向量，原本包含 1024 个 32 浮点数（4,096 字节），现在仅是 8 个 8 比特整数的序列（每个向量只要8字节的存储空间！）。&lt;/p&gt;
&lt;img src=&apos;../assets/compression.png&apos; title=&apos;compression.png&apos; alt=&apos;compression.png&apos; width=&apos;100%&apos;&gt;
&lt;h2&gt;最近邻搜索&lt;/h2&gt;
&lt;p&gt;很棒！向量经过压缩了。不过无法对经过压缩的向量直接计算 L2 距离 - 质心 ID 之间的距离是任意且没有实际意义的！（这就是压缩与降维的不同之处）&lt;/p&gt;
&lt;p&gt;接下来说说怎么进行最近邻搜索，虽仍是穷举搜索（与所有向量计算距离并排序），不过可以更高效地计算距离 - 只需进行表查找以及某种加法即可。&lt;/p&gt;
&lt;p&gt;假设我们有一个查询向量，期望找到它的最近邻居。&lt;/p&gt;
&lt;p&gt;一种不太聪明的方式是先解压缩数据集向量，然后计算 L2 距离。也就是，通过串接不同维度的质心重建出向量。下面我们也会这样干，不过比实际地解压缩所有向量在计算上要高效得多。&lt;/p&gt;
&lt;p&gt;首先，对查询向量的每个子向量，与该子段的 256 个质心中每一个计算 L2 距离的平方。&lt;/p&gt;
&lt;p&gt;这意味着要构建一个子向量距离表，这个表有 256 行（一个质心对应一行） 8 列（一个子段对应一列）。构建这个表成本有多大？相当于计算查询向量与 256 个数据集向量的 L2 距离所需要的数学运算次数。&lt;/p&gt;
&lt;p&gt;一旦有了这个表，就可以开始计算查询向量与 50k 个数据库向量中每一个的近似距离了。&lt;/p&gt;
&lt;p&gt;每个数据库向量现在只是 8 个质心  ID 的序列。因此要计算一个数据库向量与查询向量之间的相似距离，只需使用这些质心 ID 从表中查找出对应的部分距离，并将它们加和在一起。&lt;/p&gt;
&lt;p&gt;只需要将这些部分值加起来就完成了？是的！记住我们在处理的是 L2 距离的平方，所以无需平方根操作。计算 L2 的平方，就是将每个子部分的差平方（squared differences）相加，这些加法操作的次序也无关紧要。&lt;/p&gt;
&lt;p&gt;这种查表方式，与对解压缩向量计算距离的方式，得到的结果是一样的，但是计算成本要小得多。&lt;/p&gt;
&lt;p&gt;最后一步，与常规的最近邻搜索一样 - 对距离进行排序后找到最小的距离，对应的这些数据库向量就是最近的邻居。打完收工！&lt;/p&gt;
&lt;h2&gt;压缩技术相关术语&lt;/h2&gt;
&lt;p&gt;我们理解了 PQ 的逻辑原理，现在回过头来学习相关术语就简单了。&lt;/p&gt;
&lt;p&gt;广义上而言，量化器就是能够减少变量取值空间（the number of possible values that a variable has）的一种东西。构建一个查找表来减少一张图片的颜色数量（the number of colors），应该是一个不错的例子 - 找到图片中最常见的 256 个颜色数值，放到一张表中，将 24 比特 RGB 色值映射到一个 8 比特整数。&lt;/p&gt;
&lt;p&gt;我们获取所有数据库向量的开始 128 个值（8个子段中第1个），对这些 128 个值（数量 $50k \times 128$）进行聚类训练，得到 256 个质心，这 256 个质心就构成了我们所说的“码本（codebook）”。每个质心（一个包含128个浮点数的向量）被称为一个“代码（code）”。&lt;/p&gt;
&lt;p&gt;这些质心是用来表征数据库向量的，因此这些代码也可称之为“再生产值（reproduction values）” 或“重建值（reconstruction values）”。将质心 ID 对应的代码串接成序列就能创建一个数据库向量。&lt;/p&gt;
&lt;p&gt;8 个子段是分别进行 k-means 聚类的，所以实际创建了 8 个独立的码本。&lt;/p&gt;
&lt;p&gt;基于这 8 个码本，组合代码能够创建 $256^8$ 种可能的向量！因此，实际上我们创建一个非常巨大的码本，包含 $256^8$ 个代码。直接习得并存储如此大的单个码本是不可能的事情，由此可见乘积量化器的魔力。&lt;/p&gt;
&lt;h2&gt;预过滤&lt;/h2&gt;
&lt;p&gt;&lt;a href=&apos;http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/&apos;&gt;本教程的第2部分&lt;/a&gt;中，我们将学习 FAISS 库中的 IndexIVFPQ 索引，这种索引在使用乘积量化器之前将数据集先分割为多个分区，这样对于每个查询仅需要搜索部分分区。FAISS 发布于 2017 年，IndexIVFPQ 索引使用的乘积量化器方法技术首次见于 &lt;a href=&apos;https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf&apos;&gt;2011 年的这篇论文&lt;/a&gt;。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>团队开发流程规范</title>
            <description>&lt;p&gt;&lt;em&gt;本文原是针对实际工作中团队的情况编写的一份流程规范说明，隐去敏感信息之后存放于此。&lt;/em&gt;&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2023/04/05/wQ9bOaXyNiCjh5H.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;开发流程规范 是一种团队文化，也是服务和业务稳定性的基本保障线。&lt;/p&gt;
&lt;h2&gt;一、文档&lt;/h2&gt;
&lt;blockquote&gt;&lt;p&gt;共识：“谋定而后动”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;团队的开发工作主要来自3个方面：工程优化（平响优化、性能优化、稳定性/可用性优化等）、算法业务需求、产品业务需求。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;（强制）每一项开发工作实际编码之前，都需要梳理一份文档，放在 开发文档 目录下&lt;/li&gt;
&lt;li&gt;（建议）文档命名规则如示例 “w1-20220721-xxx需求”、“w1-20220721-xxx优化”，“w1” 是文档的按序编号。&lt;/li&gt;
&lt;li&gt;（建议）每一项工作，上线/推全后，部署相关信息、工程指标变化、业务指标变化、资源成本变化、遗留待优化的非关键问题等相关信息也应补充到文档中。&lt;/li&gt;
&lt;li&gt;（建议）如果走实验流程，也在在文档中加上“实验推进板块”，记录实验推进情况，遇到的问题等，特别是对于多场景实验推进的情况。&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;1.1 工程优化 文档&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;背景/现状描述/问题分析&lt;/li&gt;
&lt;li&gt;优化方案/设计方案&lt;/li&gt;
&lt;li&gt;预期收益&lt;/li&gt;
&lt;li&gt;分工排期&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;1.2 算法业务需求文档&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;明确算法负责人，链接上算法侧相关文档（要求算法同学提供） - 文档中包含 背景、预期收益、算法逻辑/模型等要点信息&lt;/li&gt;
&lt;li&gt;明确工程方案，对于复杂的算法业务需求，应该给出设计概要&lt;/li&gt;
&lt;li&gt;预估资源成本&lt;/li&gt;
&lt;li&gt;明确项目优先级 和 排期 / Deadline&lt;/li&gt;
&lt;li&gt;对于高优紧急需求，尽可能预先明确进度风险点&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;1.3 产品业务需求文档&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;明确产品负责人，链接上产品侧相关文档（要求产品同学提供） - 文档中包含 背景、预期收益、产品规则等要点信息&lt;/li&gt;
&lt;li&gt;明确工程方案，对于复杂的产品业务需求，应该给出设计概要&lt;/li&gt;
&lt;li&gt;预估资源成本&lt;/li&gt;
&lt;li&gt;明确项目优先级 和 排期 / Deadline&lt;/li&gt;
&lt;li&gt;对于高优紧急需求，尽可能预先明确进度风险点&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;1.4 所有文档&lt;/h3&gt;
&lt;p&gt;（强制）必须包含：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;测试用例设计和全面完整的测试报告/diff 报告&lt;/li&gt;
&lt;li&gt;包含对应各个代码库变更的 MR 链接、发布版本的 tag 链接&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;二、编码&lt;/h2&gt;
&lt;h3&gt;2.1 统一代码规范&lt;/h3&gt;
&lt;blockquote&gt;&lt;p&gt;共识：新代码统一新风格，老代码风格维持不变。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4&gt;2.1.1 C++&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;https://google.github.io/styleguide/cppguide.html&apos;&gt;Google C++ Style Guide&lt;/a&gt;、&lt;a href=&apos;https://zh-google-styleguide.readthedocs.io/en/latest/google-cpp-styleguide/contents/&apos;&gt;Google C++ 风格指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;代码规范/风格检查工具：&lt;a href=&apos;https://github.com/cpplint/cpplint&apos;&gt;cpplint&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Google 官方提供的工具，用于检测 C++ 代码是否符合 Google C++ Style Guide&lt;/li&gt;
&lt;li&gt;VS Code 插件：&lt;a href=&apos;https://marketplace.visualstudio.com/items?itemName=mine.cpplint&apos;&gt;cpplint - Visual Studio Marketplace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vim 插件：&lt;a href=&apos;https://github.com/vim-syntastic/syntastic&apos;&gt;vim-syntastic/syntastic: Syntax checking hacks for vim (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clion 插件：&lt;a href=&apos;https://plugins.jetbrains.com/plugin/7871-clion-cpplint&apos;&gt;CLion-cpplint - CLion Plugin | Marketplace (jetbrains.com)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;代码格式化工具：&lt;a href=&apos;https://clang.llvm.org/docs/ClangFormat.html&apos;&gt;clang-format&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;项目根目录下放置 .clang-format 文件，编辑器/IDE 配置成编码时自动格式化或者保存时自动格式化：&lt;/li&gt;
&lt;li&gt;VS Code 插件：&lt;a href=&apos;https://marketplace.visualstudio.com/items?itemName=xaver.clang-format&apos;&gt;Clang-Format - Visual Studio Marketplace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vim 集成：&lt;a href=&apos;https://clang.llvm.org/docs/ClangFormat.html#vim-integration&apos;&gt;https://clang.llvm.org/docs/ClangFormat.html#vim-integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clion 集成：&lt;a href=&apos;https://clang.llvm.org/docs/ClangFormat.html#clion-integration&apos;&gt;https://clang.llvm.org/docs/ClangFormat.html#clion-integration &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;---
BasedOnStyle: Google
---
Language: Cpp
IndentWidth: 4
ColumnLimit: 120
DerivePointerAlignment: false
PointerAlignment: Left
SortIncludes: CaseSensitive
Standard: Auto
AccessModifierOffset: -4
SpacesBeforeTrailingComments: 2
AllowShortBlocksOnASingleLine: Never
AllowShortIfStatementsOnASingleLine: Never
AllowShortLoopsOnASingleLine: false
AllowShortFunctionsOnASingleLine: Empty
AlignTrailingComments: true
BinPackParameters: false
AllowAllParametersOfDeclarationOnNextLine: false&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;2.1.2 Java&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;https://google.github.io/styleguide/javaguide.html&apos;&gt;Google Java Style Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;https://www.jetbrains.com/idea/&apos;&gt;IDE - Jetbrains IDEA&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;下载代码风格定义文件：&lt;a href=&apos;https://raw.githubusercontent.com/google/styleguide/gh-pages/intellij-java-google-style.xml&apos;&gt;intellij-java-google-style.xml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;导入 IDEA&lt;/li&gt;
&lt;li&gt;安装 Save Actions 插件&lt;/li&gt;
&lt;li&gt;配置 Save Actions 插件&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;2.1.3 Python&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;Python 3 + &lt;a href=&apos;https://docs.python.org/3/library/venv.html&apos;&gt;venv 虚拟环境&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;https://google.github.io/styleguide/pyguide.html&apos;&gt;Google Python Style Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;代码规范/风格检查工具：&lt;a href=&apos;https://pylint.pycqa.org/en/latest/&apos;&gt;pylint&lt;/a&gt;、&lt;a href=&apos;https://google.github.io/styleguide/pyguide.html#21-lint&apos;&gt;https://google.github.io/styleguide/pyguide.html#21-lint&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Vim 集成：&lt;a href=&apos;https://github.com/vim-syntastic/syntastic&apos;&gt;vim-syntastic/syntastic: Syntax checking hacks for vim (github.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VS Code 集成：&lt;a href=&apos;https://code.visualstudio.com/docs/python/linting#_pylint&apos;&gt;Linting Python in Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyCharm 插件：&lt;a href=&apos;https://plugins.jetbrains.com/plugin/11084-pylint&apos;&gt;Pylint - IntelliJ IDEA &amp; PyCharm Plugin | Marketplace (jetbrains.com)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;代码格式化工具：&lt;a href=&apos;https://github.com/google/yapf/&apos;&gt;yapf&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Vim 集成：&lt;a href=&apos;https://github.com/google/yapf/tree/main/plugins#vim&apos;&gt;https://github.com/google/yapf/tree/main/plugins#vim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VS Code 集成：&lt;a href=&apos;https://code.visualstudio.com/docs/python/editing#_formatting&apos;&gt;Editing Python Code in Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyCharm 集成：&lt;a href=&apos;https://github.com/google/yapf/issues/631&apos;&gt;How do I install yapf in pycharm · Issue #631 · google/yapf (github.com)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;2.2 代码分支管理&lt;/h3&gt;
&lt;blockquote&gt;&lt;p&gt;共识：一个代码库一个主分支；所有开发分支在测试/实验验证之后都需要合入主分支&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;（强制）基本的 git 工作流：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;每项开发工作，都从主分支签出一个（公共）开发分支&lt;/li&gt;
&lt;li&gt;如果是多人协作的开发工作，则基于新签出的公共开发分支，每个人各自签出个人的开发分支，进行独立开发&lt;/li&gt;
&lt;li&gt;个人开发自测完成后，合入公共开发分支，进行集成测试联调&lt;/li&gt;
&lt;li&gt;如果走实验流程，则使用（公共）开发分支的 sandbox 镜像部署实验集群&lt;/li&gt;
&lt;li&gt;实验推全反转下线后，（公共）开发分支合入主分支，并基于主分支上的正式 tag 镜像发布基准集群&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;（建议）分支命名规范：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;（公共）开发分支：
&lt;ul&gt;&lt;li&gt;常规需求开发分支：feature/[目标服务名]-[文档 ID]&lt;/li&gt;
&lt;li&gt;紧急需求开发分支：urgent/[目标服务名]-[文档 ID]&lt;/li&gt;
&lt;li&gt;紧急修复开发分支：hotfix/[目标服务名]-[文档 ID]&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;个人开发分支：在（公共）开发分支后带上个人 ID，示例：feature/[目标服务名]-[文档 ID]-[个人 ID]&lt;/li&gt;
&lt;li&gt;对紧急需求和紧急修复开发分支，可以先创建一个文档，拿到文档 ID，内容可能来不及写得非常完善，但之后应该进行补充完善&lt;/li&gt;
&lt;li&gt;不符合规范的开发分支，不能推送到远程代码库（不能对团队其他人可见），通过 git hook 来强行限制&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;（建议）所有（公共）开发分支、（需要代码评审的）个人开发分支，都应该创建对应的 MR，邀请其他人进行代码评审时，提供对应的 MR&lt;/p&gt;
&lt;h3&gt;2.3 Commit 规范&lt;/h3&gt;
&lt;p&gt;&lt;a href=&apos;https://www.conventionalcommits.org/zh-hans/v1.0.0/&apos;&gt;约定式提交 (conventionalcommits.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;（建议）参考 Apache 顶级项目的实践（如 apache/arrow），commit log 的“描述”部分先带上文档链接。&lt;/p&gt;
&lt;h3&gt;2.4 MR 与代码评审&lt;/h3&gt;
&lt;blockquote&gt;&lt;p&gt;共识：质量把关、经验传承&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;从团队和长远来看，Code Review 的重要性再怎么强调都不为过。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;（强制）每个开发分支的工作在开实验或者合入主分支之前，都需要邀请至少 3 名资深同学进行 Code Review&lt;/li&gt;
&lt;li&gt;（强制）评审员在确认没有意见或者所有优化意见都已得到合理解决后对 MR 进行点赞 👍&lt;/li&gt;
&lt;li&gt;（强制）开发分支/MR 需要攒 3 个以上的点赞才能开实验流量或者合入主分支&lt;/li&gt;
&lt;li&gt;（强制）对于基于分支开实验流量的分支代码，也必须经 code review 后，基于分支进行打实验 tag，并基于实验 tag 进行线上发布。&lt;/li&gt;
&lt;li&gt;（建议）适当约束分支合入权限，仅资深同学（具体名单？）才能将开发分支/MR 合入主分支&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-markdown&quot;&gt;&lt;code&gt;变更应该组织成1个或多个补丁/MR，视变更大小而定，组织方式遵循以下规范指南：
- MR 应该小一点
- MR 应该都可以独立编译并且是正确的（所有测试用例都通过）。不需要你验证这一点，但是需要在代码中放入一种标记来反映 MR 是否违反了这条规则。例如：在 fix 一个问题之前，先引入一个回归测试用例用例。
- MR 应该自包含（高内聚），并且只做一件事情。
- 每个 MR 都应该包含一份描述性的提交日志（commit log）。
- MR 的描述信息不应该假设代码评审人是一个专家。它应该包含足够的上下文信息，确保即使一个普通的小白也能理解。
- MR 的描述信息应该自包含，也不要引用无法保持关联的讨论信息（“如每日沟通达成的一致结论”）。
- MR 应该包含变更的动机（背景）。不能简单地说一句“让 X 完成 Y”，而应该仔细解释为什么要这么做。 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另附：&lt;a href=&apos;https://google.github.io/eng-practices/&apos;&gt;Google Engineering Practices Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;2.5 CI 约束&lt;/h3&gt;
&lt;p&gt;因当前一个代码库支持产出不同服务的二进制程序和 Docker 镜像，为加速 CI，使用了条件编译，根据不同条件触发不同 CI 流水线。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;根据分支名的目标服务名对应触发不同 CI 流水线（all 则触发所有的 CI 流水线）&lt;/li&gt;
&lt;li&gt;MR 合入 master 分支时，必须触发所有 CI 流水线。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;基于 .gitlab-ci.yml 配置强行约束。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;2.6 版本 Tag 规范&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;正式tag
&lt;ul&gt;&lt;li&gt;只能在 master 分支上打 tag，tag 命名规范为 v主版本号.次版本号.修订号，基于 语义化版本&lt;/li&gt;
&lt;li&gt;tag 内容必须包含必要的描述性信息，声明此次变更的内容，包含相关的 MR、文档链接&lt;/li&gt;
&lt;li&gt;hotfix 版本 tag，应该以 fix 之前的 tag 为前缀，加上“-hotfix” 后缀，示例：v10.0.1-hotfix；如果对同一个 tag 的代码发生了不只一次 hotfix，则继续补充上秒级的时间戳作为后缀&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;实验tag
&lt;ul&gt;&lt;li&gt;在分支上打tag进行线上实验开量，tag命名规范：exp.文档编号.迭代版本号，示例：exp.w88.0&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;2.7 监控打点和日志&lt;/h3&gt;
&lt;blockquote&gt;&lt;p&gt;共识：以尽可能小的性能开销最大化系统的可观测性&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;&lt;li&gt;（建议）多使用 ROC 打点，借助多维数据分析，方便问题定位分析&lt;/li&gt;
&lt;li&gt;（强制）注意日志级别（DEBUG、INFO、ERROR、FATAL）的语义
&lt;ul&gt;&lt;li&gt;不要使用非 DEBUG 级别来输出 DEBUG 日志&lt;/li&gt;
&lt;li&gt;注意 FATAL 的实际影响&lt;/li&gt;
&lt;li&gt;不同环境使用不同的日志级别，生产环境不要输出 DEBUG 日志，https://github.com/google/glog#setting-flags&lt;/li&gt;
&lt;li&gt;（建议）使用 glog vlog 来进一步控制不同环境/场景下的日志量，https://github.com/google/glog#verbose-logging&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（强制）不要默默地失败
&lt;ul&gt;&lt;li&gt;ERROR/WARNING 日志&lt;/li&gt;
&lt;li&gt;Event 打点&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（建议）使用 Event 打点充分表现输入量、输入类型、输出量、变化趋势
&lt;ul&gt;&lt;li&gt;请求 QPS、增量消息 TPS、不同类型增量消息的 TPS、。。。&lt;/li&gt;
&lt;li&gt;基准数据的统计计数&lt;/li&gt;
&lt;li&gt;请求来源：客户端类型粒度、客户端 ip 粒度、场景维度、媒体维度、。。。&lt;/li&gt;
&lt;li&gt;trigger 数、各类过滤器数目统计&lt;/li&gt;
&lt;li&gt;检索结果数量、截断后/实际返回给客户端的结果数量&lt;/li&gt;
&lt;li&gt;各类过滤器的过滤量&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（建议）使用 Transaction 表现耗时分布 - 整体耗时多少、时间都花在哪些环节（包括 RPC 框架内的等待时延）：Tt = T1 + T2 + ...&lt;/li&gt;
&lt;li&gt;（建议）使用 Metric 表现跨节点/集群/场景的业务指标变化趋势&lt;/li&gt;
&lt;li&gt;（建议）性能考虑，打点接口调用次数应小于等于请求 QPS、消息 TPS&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;2.8 最佳实践&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）尽早严格检测请求、数据、配置等输入的合法性&lt;/li&gt;
&lt;li&gt;（建议）尽量不使用配置中心的配置监听&lt;/li&gt;
&lt;li&gt;（建议）多了解使用基础库 - boost、abseil-cpp 等&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;三、测试&lt;/h2&gt;
&lt;h3&gt;3.1 单元测试&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;明确单元测试与集成测试的区别，&lt;a href=&apos;https://zh.wikipedia.org/wiki/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95&apos;&gt;单元测试 - 维基百科，自由的百科全书 (wikipedia.org)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;单元测试的作用：
&lt;ul&gt;&lt;li&gt;验证某个逻辑的当前实验是否符合预期&lt;/li&gt;
&lt;li&gt;更重要的是对以后的代码变更可能造成的非预期影响/破坏进行一定的防御&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GoogleTest，用好 Mocking&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;（强制）CI 强制要求新增/变更代码的测试覆盖率。（基于 .gitlab-ci.yml 配置强行约束？）&lt;/p&gt;
&lt;h3&gt;3.2 集成测试&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）任何代码变更，都必须经过自测/集成测试
&lt;ul&gt;&lt;li&gt;任务可以正常跑起来，变更的逻辑已生效，产出的结果确认符合预期&lt;/li&gt;
&lt;li&gt;服务可以正常运行起来，变更的逻辑已生效，可以正常加载数据/索引，请求响应的结果确认符合预期&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（建议）测试环境/工具
&lt;ul&gt;&lt;li&gt;独立的测试验证集群&lt;/li&gt;
&lt;li&gt;开发机环境下的 Docker 环境 + 代码库中的 Dockerfile
&lt;ul&gt;&lt;li&gt;docker build -t image-name .&lt;/li&gt;
&lt;li&gt;docker run -dp 8003:8003 image-name&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;易用的脚本 - 一键启动+索引数据加载、易用的客户端工具、易用的数据校验工具&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;3.3 批量 Diff 测试&lt;/h3&gt;
&lt;p&gt;（强制）如果代码变更影响了索引或者检索逻辑，则应该进行充分的 diff：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;如果走实验，则部署新实验集群后开实验之前，进行 基准集群 VS. 实验集群 的请求结果 diff&lt;/li&gt;
&lt;li&gt;如果不走实验，则基于独立的测试验证集群，进行 生产集群 VS. 测试集群 的请求结果 diff
&lt;ul&gt;&lt;li&gt;diff 完成后，即刻释放测试集群资源&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;对于 diff 结果，不管最后的 diff 率有多小，只要有 diff，就需要确认 diff 来源/原因是否符合预期&lt;/li&gt;
&lt;li&gt;diff 的不同请求数量必须达到 1000 以上&lt;/li&gt;
&lt;li&gt;统一使用易用且功能完善的 diff 工具&lt;/li&gt;
&lt;li&gt;生产集群发版或者实验集群开流量之前，在周知相关人员时，必须一并提供 diff 结果以及已确认 diff 来源符合预期&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;四、发布&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;（强制）流量高峰期不发版&lt;/li&gt;
&lt;li&gt;（建议）周五晚上及周末不发版&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;4.1 发布之前&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）评估确认好本次发布前后服务集群负载是否会有变化，如果变更会影响集群负载上涨，则应提前扩容&lt;/li&gt;
&lt;li&gt;（强制）在相关大群内进行通告，通告模板如下：&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;变更通知：
- 变更内容：xxxx
- 涉及集群和场景：xxx
- 操作人：xxx
- 相关文档（含需求背景、工程方案、代码 MR、diff / 测试报告等信息） 或 实验链接：xxx&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;4.2 发布期间&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）先灰度发布一行或者少量行，确认各项工程指标+业务指标无异常后，再全量发布&lt;/li&gt;
&lt;li&gt;（强制）发布期间需要保持关注告警以及关键工程指标和业务指标&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;4.3 发布之后&lt;/h3&gt;
&lt;p&gt;（强制）发版完成后，确认各项工程指标/业务指标正常后，在相关大群内周知发版完成，确认各项指标平稳/符合预期&lt;/p&gt;
&lt;h2&gt;五、实验&lt;/h2&gt;
&lt;h3&gt;5.1 实验开量之前&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）人工刷 demo，全链路验证&lt;/li&gt;
&lt;li&gt;（强制）实验集群资源预估准备，包括评估链路中间环节资源负载变化&lt;/li&gt;
&lt;li&gt;（强制）在相关大群内通告，通告信息：实验名、实验链接&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;5.2 实验期间&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）实验刚开量时，关注实时报表 10~30 分钟变化趋势：&lt;/li&gt;
&lt;li&gt;（建议）每天及时查看实验平台上当前实验的天级实验报表，如果有较明显的负向指标，则应及时通告出来，与相关同学一起分析可能的原因&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;5.3 实验扩量或推全之前&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）实验集群资源预估准备，包括评估链路中间环节资源负载变化&lt;/li&gt;
&lt;li&gt;（强制）与相关算法同学确认好实验时长与实验效果是否达到扩量要求&lt;/li&gt;
&lt;li&gt;（强制）在相关大群内通告，通告信息：实验名、实验链接、流量从多少变化到多少&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;5.4 实验扩量或推全操作期间&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;（强制）对实验集群负载情况保持关注&lt;/li&gt;
&lt;li&gt;（建议）对于推全操作，尽可能灰度推全，中间步骤留一定时间确认集群负载和流量变化是否符合预期&lt;/li&gt;
&lt;li&gt;（建议）先开反转，再进行推全，避免不必要的资源腾挪扩缩，也能一定程度上控制风险&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;六、规范落地&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;（建议）规范流程支持工具和平台不断优化，尽可能减少规范流程造成的人力负担&lt;/li&gt;
&lt;li&gt;（建议）不断更新完善规范&lt;/li&gt;&lt;/ul&gt;</description>
        </item>
        
        <item>
            <title>与一个前 leader 的交流笔记</title>
            <description>&lt;p&gt;如下这份笔记，是 19 年和一个前 leader 交流后记录下来的。说是交流，其实是针对我当时工作中存在的问题，他给我提出的一些改进建议。如今再看看，仍然能引起自己的思考。&lt;/p&gt;
&lt;h2&gt;2019-10-27&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;如何讨论需求/技术方案&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;系统性理解需求
&lt;ul&gt;&lt;li&gt;全面梳理技术方案，论证方案的不合理之处&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;如何证明方案不合理？
&lt;ul&gt;&lt;li&gt;资源成本（机器、人力），重复工作也是对资源的一种浪费&lt;/li&gt;
&lt;li&gt;对效果（收入等）的影响&lt;/li&gt;
&lt;li&gt;要站在“公正”的角度来论证，从大家共有的认知(机器、带宽、内存)出发&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;最后才是什么系统复杂度（说到这个点，很容易扯淡，谁都会说自己复杂）&lt;/p&gt;
&lt;p&gt;讨论问题的时候，自己不做，但不要从“硬推给别人的角度”去讨论，而是要从“为啥自己不应该做”的角度出发。比如做了，导致系统耦合，复杂，做了导致多余的调用量更多等 。&lt;/p&gt;
&lt;h2&gt;2019-12-17&lt;/h2&gt;
&lt;ol&gt;&lt;li&gt;跨团队工作讨论时，围绕应该怎么做（怎么做最合理）来讨论，而不是围绕如果自己来做会有什么困难来讨论&lt;/li&gt;
&lt;li&gt;项目/系统要有 目标/长远架构图（最终做成什么样子，核心 KPI）&lt;/li&gt;
&lt;li&gt;做好向上汇报&lt;/li&gt;
&lt;li&gt;不要拿“做的过程中的困难”来搪塞不做，而是要从客观事实的角度来问应不应该做&lt;/li&gt;
&lt;li&gt;不要聚焦于解决现实世界的一个个问题，而是要靠具体的一个个问题，抽象出一张大图&lt;/li&gt;
&lt;li&gt;把自己的系统做成链路上最极致的那个，而不是等别人做好后来反推自己变革&lt;/li&gt;
&lt;li&gt;要区分重要和不重要的事情，核心的事情要仔细揣摩，没有人是傻子，那些资深的人说出的话更要揣摩，理解&lt;/li&gt;
&lt;li&gt;要有规划和愿景，leader 没有这些，团队走不远&lt;/li&gt;&lt;/ol&gt;</description>
        </item>
        
        <item>
            <title>读文笔记：关于 MMAP 与 SSD</title>
            <description>&lt;p&gt;设计一种存储，第一要明确应用场景和存储系统的工作负载，第二要了解底层硬件的特点。&lt;/p&gt;
&lt;h2&gt;1、&lt;a href=&apos;https://db.cs.cmu.edu/mmap-cidr2022/&apos;&gt;Are You Sure You Want to Use MMAP in Your Database Management System? &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MMAP（Memory-mapped file I/O）是操作系统提供的一种功能特性 - 将二级存储（磁盘、SSD）上一个文件的内容映射到程序/进程的地址空间，然后程序就可以以指针访问内存页的方式来访问文件内容。当程序访问到某个内存页时，操作系统就会自动将对应文件内容加载到该内存页中，当内存用满了，也会自动剔除某些内存页。&lt;/p&gt;
&lt;p&gt;MMAP 其实就一个现成的缓冲池（buffer pool），核心特点就是简单易用，不需要重复开发，缺点是在需要时无法精确控制其行为。&lt;/p&gt;
&lt;p&gt;使用 MMAP 的优势是由操作系统封装了如下功能：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;从磁盘读数据&lt;/li&gt;
&lt;li&gt;不同线程读相同数据的并发处理&lt;/li&gt;
&lt;li&gt;缓存和缓冲管理（Caching and buffer management）&lt;/li&gt;
&lt;li&gt;从内存中剔除/驱逐内存页&lt;/li&gt;
&lt;li&gt;同一个机器上不同进程之间可以友好交互 🤔&lt;/li&gt;
&lt;li&gt;跟踪脏页以及将脏页写入磁盘 🤔&lt;/li&gt;
&lt;li&gt;相比 read/write 系统调用，mmap 不需要将内存页从内核空间拷贝到用户空间，而是直接从操作系统的内存页缓存中直接访问内存页，有一定的性能优势&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;MMAP 相关 POSIX API：mmap、madvise、mlock、msync。&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/U7xMgzj36dZDGaO.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;① A program calls mmap and receives a pointer to the memory-mapped file contents.&lt;/p&gt;
&lt;p&gt;② The OS reserves part of the program’s virtual address space but does not load any part of the file.&lt;/p&gt;
&lt;p&gt;③ The program accesses the file’s contents using the pointer.&lt;/p&gt;
&lt;p&gt;④ The OS attempts to retrieve the page.&lt;/p&gt;
&lt;p&gt;⑤ Since no valid mapping exists for the specified virtual address, the OS triggers a page fault to load the referenced part of the file from secondary storage into a physical memory page.&lt;/p&gt;
&lt;p&gt;⑥ The OS adds an entry to the page table that maps the virtual address to the new physical address.&lt;/p&gt;
&lt;p&gt;⑦ The initiating CPU core also caches this entry in its local translation lookaside buffer (TLB) to accelerate future accesses.&lt;/p&gt;
&lt;p&gt;不过论文作者认为 mmap 存在一些数据安全性和系统性性能问题，为解决这些问题而引入的工程成本会抵消掉简单性：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、事务安全性（Transactional Safety）&lt;/strong&gt;：由于透明的页式调度机制，操作系统可能会任意时刻将一个脏页刷到二级存储中，不管写事务是否已提交。DBMS 无法组织这种内存数据刷出，并且发生时也不会接收到任何信号。所以基于 mmap 的数据库系统只能采用复杂的协议来确保（写/更新）事务安全，手段上大概分3种：操作系统写时复制、用户空间写时复制、影子页管理（shadow paging）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、I/O 停顿（I/O Stalls）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;mmap 不支持异步读；自己搞缓冲池的话，可以使用异步 I/O（比如 libaio、io_uring）来避免查询时阻塞线程&lt;/li&gt;
&lt;li&gt;对于 mmap，因为操作系统会自动/透明地剔除一些内存页，这样可能导致 - 如果某些只读查询命中了已被剔除的内存页，就会无法预知地触发阻塞性的页错误/缺页处理&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;解决方案：1、使用 mlock，不过操作系统对一个进程能锁住的内存页数量有限制；2、使用 madvise 的 MADV_SEQUENTIAL 标记，仅能问题的一部分；3、使用额外的线程来进行内存页预取，避免主线程被阻塞，不过会引入较大的复杂性&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、错误处理&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;DBMS 的核心职责之一是确保数据完整性 - 比如：校验磁盘数据是否有损坏&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;使用 mmap，DBMS 需要在每次内存页访问时检查校验和（checksum），因为内存页一次访问之后操作系统可能会将该页驱逐到磁盘&lt;/li&gt;
&lt;li&gt;如果 DBMS 是使用非内存安全的编程语言编写的，就有可能在指针操作时损坏内存页内容，所以需要在内存页刷到二级存储之前进行错误检测，mmap 会默默地将损坏的内存页持久化到二级存储&lt;/li&gt;
&lt;li&gt;使用 mmap 也更难优雅地进行 I/O 错误处理，和 mmap 内存交互的任何代码都可能抛出 SIGBUS 信号，DBMS 必须使用信号处理器（signal handlers）来处理 ❓&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4、性能问题（最重大）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;大家普遍认为 mmap 性能优于传统文件 I/O（read/write），因为它避免两个开销：(1) 显式调用 read/write 系统调用的开销 (2) mmap 会返回指向操作系统页缓存的页指针，因此避免了到用户内存空间的一次内存拷贝，也因此降低了内存占用；由此，大家也认为在 SSD 上 mmap 的性能优势会进一步扩大。&lt;/li&gt;
&lt;li&gt;不过实验测试发现：对于高带宽的二级存储设备（比如 SSD），DBMS 管理的数据量大于内存空间时，操作系统的页驱逐机制（page eviction mechanisms）多线程扩展性比较差（备注：因为 SSD I/O 带宽大、访问速度快，页驱逐机制就可能成了瓶颈）（we have found that the OS’s page eviction mechanisms cannot scale beyond a few threads for larger-thanmemory DBMS workloads on high-bandwidth secondary storage devices. We believe that one of the main reasons these performance issues have gone largely unnoticed is due to historically limited file I/O bandwidth）。瓶颈源于3个因素：
&lt;ul&gt;&lt;li&gt;页表争用（page table contention）/ 锁（备注：当前 linux 内核实现优化了这个问题，&lt;a href=&apos;https://github.com/torvalds/linux/blob/master/Documentation/mm/split_page_table_lock.rst&apos;&gt;linux/split_page_table_lock.rst at master · torvalds/linux (github.com)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;单线程页驱逐（single-threaded page eviction），&lt;a href=&apos;https://biriukov.dev/docs/page-cache/4-page-cache-eviction-and-page-reclaim/&apos;&gt;Page Cache eviction and page reclaim | Viacheslav Biriukov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;旁路转换缓冲击落（TLB shootdowns）：TLB shootdowns occur during page eviction when a core needs to invalidate mappings in a remote TLB. Whereas flushing the local TLB is inexpensive, issuing interprocessor interrupts to synchronize remote TLBs can take thousands of cycles
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;https://juejin.cn/post/6844904084957315086&apos;&gt;深入理解 Linux 内核--jemalloc 引起的 TLB shootdown 及优化 - 掘金 (juejin.cn)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;https://www.kernel.org/doc/html/v4.18/core-api/cachetlb.html&apos;&gt;Cache and TLB Flushing Under Linux — The Linux Kernel documentation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;实验分析：&lt;/p&gt;
&lt;p&gt;As a baseline, we used the fio storage benchmarking tool (v3.25) with direct I/O (O_DIRECT) to bypass the OS page cache. Our analysis focused exclusively on read-only workloads, which represent the best-case scenario for mmap-based DBMSs.&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/FlAJqH8E4K1UWvp.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/qhz3OQHlgaZ2YMn.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;600&apos;&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/oczkfgLw9EMR1ue.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;600&apos;&gt;
&lt;p&gt;那么，到底要不要使用 mmap 呢？直接使用 ssd？还是自己搞一个 buffer pool？&lt;/p&gt;
&lt;p&gt;论文作者给出这样的结论：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;以下情况不要使用 mmap：
&lt;ul&gt;&lt;li&gt;需要以事务安全的方式进行更新操作&lt;/li&gt;
&lt;li&gt;希望处理缺页错误时不会阻塞在慢 I/O 上，或者希望明确控制哪些数据应该在内存中&lt;/li&gt;
&lt;li&gt;关心错误处理，也希望始终返回正确的数据结果&lt;/li&gt;
&lt;li&gt;要求在快速持久化存储设备（比如 SSD）上获得高吞吐&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;以下情况可能应该使用 mmap：
&lt;ul&gt;&lt;li&gt;内存可以容纳整个数据工作集（或者说整个数据库），并且是只读的工作负载&lt;/li&gt;
&lt;li&gt;希望将一个产品快速推向市场，也不关注数据一致性或者长期的工程技术债&lt;/li&gt;
&lt;li&gt;Otherwise, never 🤣🙃&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;彩蛋：论文的奇数页页眉  😂&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/AIqHKTC213FYaxX.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100&apos;&gt;
&lt;h2&gt;2、&lt;a href=&apos;https://ayende.com/blog/196161-C/re-are-you-sure-you-want-to-use-mmap-in-your-database-management-system&apos;&gt;re: Are You Sure You Want to Use MMAP in Your Database Management System? &lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本文作者认为自己实现一个内存分页管理器/缓冲池比较复杂，使用 mmap 来实现存储系统会很快。&lt;/p&gt;
&lt;p&gt;不过吐槽了原论文没有给出可供选择的替代方案，基准测试和结论之间也没太多相关性（compares apples to camels），也低估了自己实现一个替代 mmap 的缓冲池的复杂性。&lt;/p&gt;
&lt;p&gt;不用 mmap 的话，论文提及的那些问题，也是要解决的（If you aren’t using mmap, on the other hand, you still need to handle all those issues. That is a key point that I believe isn’t addressed in the paper. Solving those issues properly (and efficiently) is a seriously challenging task. Given that you are building a specialized solution, you can probably do better than the generic mmap, but it will absolutely have a cost. That cost is both in terms of runtime overhead as well as increased development time.）。&lt;/p&gt;
&lt;p&gt;原论文的实验说明了 mmap 的性能问题，但是换个 buffer pool 的实现能不能获得更好的性能呢？该文作者持悲观态度。&lt;/p&gt;
&lt;p&gt;存储系统实际面对的工作负载不会是完全的随机读（写）或顺序扫描，通常都具有一定的热点数据，这时 buffer pool 的优势就会体现出来？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;关于“问题 1 - 事务安全性” - 不用 mmap，解决这个问题的方案没什么不同（I don’t actually care if the data is written to memory behind my back. What I care about is MVCC (a totally separate concern than buffer management). The fact that I’m copying the modified data to the side means that I Can support concurrent transactions with far greater ease.）&lt;/li&gt;
&lt;li&gt;关于“问题 2 - I/O 停顿” - 该文作者认为确实个问题（not having control over the I/O means that you may incur a page fault at any time），并且是基于 mmap 的系统要面对的最大问题。不过实际情况是 linux 系统中 io_uring 之外的异步 I/O 方案在某些时候异步操作也会是阻塞的。是否使用 mmap，这个问题的解决方案也没有本质区别。&lt;/li&gt;
&lt;li&gt;关于“问题 3 - 错误处理” - 该文作者以自己开发的 Voron 为例说明即使使用 mmap，也可以较好地实现校验和检查（使用一个 bitmap 来记录哪些内存页被访问过，在第一次访问某个内存页的时候检查校验和），并认为程序一次运行过程中对于指定的一个内存页检查一次就可以，其他情况下的检查都是没有意义的。When you use read() to get data from the disk, you have no guarantees that the data wasn’t fetched from a cache along the way. So you may validate the data you read is “correct”, while the on disk representation is broken. For that reason, we only do the check once, instead of each time. 🤔 好像不是这个理？
&lt;ul&gt;&lt;li&gt;至于发现 I/O 错误，如何处理？该文作者认为答案只有一个 - 让它崩溃然后重新恢复并运行（Crash and then run recovery from scratch），因为如果 I/O 系统返回了一个错误，应用逻辑也不会有任何方式知道 I/O 系统的当前状态是什么，应该怎么解决，唯一的方式就是停下来，重新加载一切（应用 WAL 进行恢复），回到一个稳定状态。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;关于“问题 4 - 性能问题” -
&lt;ul&gt;&lt;li&gt;页表争用（page table contention）：linux 内核已优化解决&lt;/li&gt;
&lt;li&gt;单线程页驱逐（single-threaded page eviction）：如果写/更新频率不高，脏页不多的话，也不会成为性能瓶颈&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;https://stackoverflow.com/questions/3748384/what-is-tlb-shootdown&apos;&gt;旁路转换缓冲击落（TLB shootdowns）&lt;/a&gt;：在一定条件下才会成为性能瓶颈，当你真的遇到时应该也会多花钱买内存来解决 🤣🙃（In order to actually observe the cost of TLS Shootdown in a significant manner, you need to have: (1) really fast I/O (2)working set that significantly exceeds memory (3) no other work that needs to be done for processing a request; In practice, if you have really fast I/O, you spent money on that, you’ll more likely get more RAM as well. And you typically need to do something with the data you read, which means that you won’t notice the TLB shootdown as much）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;3、&lt;a href=&apos;https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&apos;&gt;WiscKey: Separating Keys from Values in SSD-Conscious Storage&lt;/a&gt; /   &lt;a href=&apos;https://dgraph.io/blog/post/badger/&apos;&gt;Introducing Badger: A fast key-value store written purely in Go&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;WiscKey 是一个基于 LSM (Log Structured Merge)树的 KV 存储引擎，针对 SSD 的随机读和顺序读性能特点，将 Key 和 Value 分开存储，以尽可能缩小 I/O 放大问题，提升性能。&lt;/p&gt;
&lt;p&gt;Value 存放在 Log 文件中，LSM 树仅存储 Key 和 Value 在 Log 文件的位置以及长度/大小。&lt;/p&gt;
&lt;p&gt;对于 Value 比较大的应用场景，性能优势明显。&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/y84VKqJZWMSjBu7.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;因为 LSM 树不存储 Value 本身，通常比较小，可以全部放在内存中，所以不考虑获取 Value 的值，点查和范围查找速度非常快。&lt;/p&gt;
&lt;p&gt;LSM 树比较小，所以 Compaction 次数少，速度快（特别是如果整个 LSM 都放在内存中的话）。Compaction 过程中不需要读写 Value 的值，所以 I/O 放大倍数要小很多。&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/2tvp5LROXG4UYrw.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;点查过程，从 LSM 树中获取 Value 的位置和大小后，需要从 Value Log 中获取 Value 值。因为 LSM 树小，所以读放大倍数小，综合起来看，WiscKey 点查的性能优势依旧明显。&lt;/p&gt;
&lt;p&gt;范围查找/遍历过程，先从 LSM 树中获得目标范围内的所有 Value 的位置和大小，放入队列，使用多线程进行并发预取，利用 SSD 随机读的吞吐能力随并发树近线性增长的特点。&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/APbQlhq3nWsf47j.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/j357k6XMfYO2udm.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;400&apos;&gt;
&lt;p&gt;Value Log 也需要 GC，清理掉无用的 Value 值。LSM 树的 Compaction 主要是为了提升查找/检索的性能/效率，控制读放大，减少内存/磁盘空间占用是次要的。Value Log 的 GC 则主要是为了减少内存/磁盘空间占用。&lt;/p&gt;
&lt;p&gt;为了实现在线的轻量级 GC，Value Log 中也存储了 Key，&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/JMKxWiaCovyhgr8.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;tail 指向 Value Log 有效值范围内时序上最先写入的那个 value 的位置。&lt;/p&gt;
&lt;p&gt;head 指向 Value Log 中下一个新 value 写入的位置。&lt;/p&gt;
&lt;p&gt;tail 和 head 及其对应的位置信息均作为 kv 存入 LSM 树中（head 的指向什么时候会更新？：每次有新值写入的时候都更新？还是在 GC 的过程中才会更新？）&lt;/p&gt;
&lt;p&gt;GC 的流程为：&lt;/p&gt;
&lt;p&gt;① 从 tail 指向的位置开始顺序扫描一块数据，根据其中的 key 检索 LSM 树，确认当前 value 是否有效（没有被删除也没有被覆盖）&lt;/p&gt;
&lt;p&gt;② 如果有效则插入到 head 指向的位置，head 指向移动到下一个位置；如果无效，则直接丢弃&lt;/p&gt;
&lt;p&gt;③ 一块数据处理完成后，移动 tail 指向到下一个位置，释放/回收原来数据块的存储空间；然后继续循环处理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;疑问&lt;/strong&gt;：1. GC 过程中每次向 head 插入有效值时，是否先要从 LSM 树中查询最新指向的位置？2. 在根据 key 从 LSM 树查询后将有效值写入 head 位置前，有新数据写入的话，怎么处理？GC 期间是否要停止正常的写操作？还是说对某个临界区加锁？&lt;/p&gt;
&lt;p&gt;LSM 树原理：&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/k6cHFOUluo1Ibfp.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/17WubUhqSpCjMr5.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/Tomk3VOEs7jduQl.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;&lt;a href=&apos;https://zhuanlan.zhihu.com/p/181498475&apos;&gt;LSM树详解 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;4、&lt;a href=&apos;http://www.vldb.org/pvldb/vol14/p364-didona.pdf&apos;&gt;Toward a Better Understanding and Evaluation of Tree Structures on Flash SSDs（VLDB）&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;PTS - Persistent tree data structure&lt;/p&gt;
&lt;p&gt;使用 LSM 树（RocksDB）和 B+ 树（WiredTiger）来分析 SSD 基准测试（Benchmarking）中可能踩到的 7 个坑（pitfall）：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）Running short tests / 测试过于短平快&lt;/strong&gt; ⚡️&lt;/p&gt;
&lt;p&gt;随着使用时间的增加，SSD 的性能会一定的动态变化。&lt;/p&gt;
&lt;p&gt;Because both the PTS and SSD performance vary over time, short-lived tests are unable to capture how the systems will behave under a continuous（non-bursty）workload.&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/HSuzIeC8itjndGw.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;WA-A（应用/存储系统的写放大） increases over time while the levels of the LSM-Tree fills up, and its curve flattens once the layout of the LSM tree has stabilized.&lt;/p&gt;
&lt;p&gt;WA-D（SSD 设备的写放大） increases over time because of the effect of garbage collection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（2）Ignoring the device write amplification (WA-D) / 忽视了 SSD 设备本身的写放大&lt;/strong&gt; ⚡️&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;WA-D directly affects the throughput of the device, which strongly correlates with the application-level throughput.&lt;/li&gt;
&lt;li&gt;WA-D is an essential measure of the I/O efficiency of a PTS. 端到端的写放大倍数应该是 WA-A 乘以 WA-D&lt;/li&gt;
&lt;li&gt;WA-D measures the flash-friendliness of a PTS.
&lt;ul&gt;&lt;li&gt;A low WA-D indicates that a PTS generates a write access that does not incur much garbage collection overhead in the SSD.&lt;/li&gt;
&lt;li&gt;以前大家可能认为 LSM 树（顺序写）相比 B+ 树（随机写）对 SSD 更友好，但实测 WA-D 颠覆了这个认知&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（3）Ignoring the internal state of the SSD / 忽视了 SSD 的初始内部状态&lt;/strong&gt; ⚡️&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/pFyqSCVZnrHB1g9.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Trim(Discard)的出现主要是为了提高GC的效率以及减少写入放大的发生，最大作用是清空待删除的无效数据&lt;/strong&gt;。在SSD执行读、擦、写步骤的时候，预先把擦除的步骤先做了，这样才能发挥出SSD的性能，通常SSD掉速很大一部分原因就是待删除的无效数据太多，每次写入的时候主控都要先做清空处理，所以性能受到了限制。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The steady-state performance of a PTS can greatly differ depending on the initial state of the drive, this is surprising.&lt;/p&gt;
&lt;p&gt;This phenomenon is caused by how the LBA (logic block address)  access patterns of RocksDB and WiredTiger intertwine with the SSD garbage collection mechanism as a function of the initial state the drive.&lt;/p&gt;
&lt;p&gt;WiredTiger only writes to a limited portion of the logical block address space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（4）Ignore the dataset size / 忽视了数据集大小&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/SlVWJfP5rO7xpNQ.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;The amount of data stored by the SSD changes its behavior and affects overall performance.&lt;/p&gt;
&lt;p&gt;The performance degradation brought by the larger dataset is primarily due to the idiosyncrasies（特质/特点） of the SSD: larger datasets lead to more valid pages in each flash block, which increases the amount of data being relocated upon performing garbage collection, i.e., the WA-D&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（5）Ignoring the extra storage capacity a PTS needs to manage data and store additional meta-data / 未考虑空间放大（Space amplification）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;涉及存储成本&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（6）Ignoring SSD over-provisioning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;成本与性能之间的权衡折中 - 可以预留一部分 SSD 空间给 SSD 做 GC 使用，这部分空间对文件系统不可见。&lt;/p&gt;
&lt;img src=&apos;https://s2.loli.net/2022/11/16/YEFp5g69mnqBe3w.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;&lt;strong&gt;（7）Ignoring the effect of the underlying storage technology on performance&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;5、SSD 原理相关资料&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;https://zhuanlan.zhihu.com/p/102089411&apos;&gt;浅谈分布式存储之SSD基本原理 - 知乎 (zhihu.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;https://mp.weixin.qq.com/s/_uiCsFXWjepeHSdgiUABhg&apos;&gt;聊聊 SSD 的基本原理 (qq.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://xiongduo.cn/posts/coding-for-ssds-part-1-introduction-and-table-of-contents.html&apos;&gt;为SSD编程（1）：简介和目录 (xiongduo.cn)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://www.ssdfans.com/?p=8077&apos;&gt;SSD背后的秘密：SSD基本工作原理 (ssdfans.com)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description>
        </item>
        
        <item>
            <title>读文笔记：Kafka 官方设计文档</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;http://kafka.apache.org/documentation/#design&apos;&gt;http://kafka.apache.org/documentation/#design&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;数据持久化&lt;/h2&gt;
&lt;h4&gt;不用惧怕文件系统&lt;/h4&gt;
&lt;p&gt;磁盘的读写速度，取决于如何读写。对于线性读写方式，操作系统做了充分的优化：提前读 - 预取若干数据块，滞后写 - 将小的逻辑写操作合并成一个大的物理写操作。&lt;/p&gt;
&lt;p&gt;&lt;a href=&apos;http://queue.acm.org/detail.cfm?id=1563874&apos;&gt;研究&lt;/a&gt;表明：&lt;a href=&apos;http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg&apos;&gt;顺序读写磁盘（sequential disk access）的速度有些时候比随机访问内存还要快&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;现代操作系统激进地尽可能将空闲内存用作磁盘缓存。所有磁盘读写都经过操作系统提供的统一缓存。这个特性没法轻易关闭，除非直接 I/O （direct I/O），因此，如果程序在用户进程中进行数据缓存，缓存的数据通常也是和操作系统页缓存重复的，缓存两遍，没啥意义，也浪费内存。&lt;/p&gt;
&lt;p&gt;而且，Kafka 是构建在 JVM 之上的，了解 Java 内存使用方式的人应该都知道：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;对象的内存开销非常高，通常是实际数据大小的2倍（甚至更多）&lt;/li&gt;
&lt;li&gt;随着堆上数据量增大，Java 的 GC 表现也会更糟糕&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;因此，使用文件系统并依赖于操作系统内存页缓存，优于在程序中维护一块内存缓存或其它结构。至少操作系统内存页缓存的可用内存翻倍了。另外，如果使用紧凑的字节结构来缓存数据，相比使用对象，可用内存可能还会翻倍。在 32GB 内存的机器上这么搞，缓存可用到 20-30GB，还不会对 GC 造成了什么坏影响。并且，即使服务重启，这块缓存空间也是热的（除非机器重启），用户进程内的内存缓存在服务重启后得重建（10GB的数据缓存可能需要10分钟左右）。&lt;/p&gt;
&lt;p&gt;这样也可以简化代码逻辑，因为缓存和文件系统之间的一致性由操作系统来保证了。&lt;/p&gt;
&lt;p&gt;这样一分析，设计就简单了：我们反其道而行之，所有数据都直接写到文件系统上持久化日志文件中，不需要在程序中使用内存缓存，也不必确保将数据刷到磁盘。这实际意味着数据转移到了内核的内存页缓存。&lt;/p&gt;
&lt;h4&gt;常量时间就能搞定&lt;/h4&gt;
&lt;p&gt;B 树的 O(log N) 时间复杂度，对于磁盘操作来说，并不能等同于常量时间复杂度。&lt;/p&gt;
&lt;p&gt;Kafka 采用日志文件方式，确保读写操作的时间复杂度是 O(1)。&lt;/p&gt;
&lt;p&gt;Kafka 不会在消息一被消费就立即删除，而是保留一段时间，这样对于消费者来说也更灵活一些。&lt;/p&gt;
&lt;h2&gt;效率&lt;/h2&gt;
&lt;p&gt;对于 Kafka 这类系统而言，即使像前述那样消除了糟糕的磁盘访问模式，也会遇到两个导致数据效率低的问题：&lt;strong&gt;过多的小 I/O 操作&lt;/strong&gt;，以及&lt;strong&gt;过多的字节拷贝&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;小 I/O 问题在客户端与服务端之间，以及服务端内部的数据持久化操作中都会发生。对此，Kafka 协议建立在 “消息集” （即一批消息）的抽象之上，这样网络请求读写的是一批一批的消息，减少了网络往返的时间开销（注：消息处理的实时性会相对差一点）。服务端也是一次将一批消息写到日志文件中，消费者也按序一次获取一批消息。这一简单的优化可以将吞吐能力提升几个数量级。&lt;/p&gt;
&lt;p&gt;对于过多的字节拷贝问题，在消息量大的时候，影响比较明显。Kafka 采用了一种标准化的二进制消息格式，producer、broker、consumer 都使用这种格式，这样数据块在传输期间不需要变动。&lt;/p&gt;
&lt;p&gt;broker 维护的消息日志只是一个目录下的一堆文件，文件内容是按序写入的消息集，消息集的数据格式同于 producer、consumer 使用的。共用一种数据格式方便了一个重要的操作优化：持久化日志块的网络传输。对于从内存页缓存（pagecache）到网络套接字（socket）的数据传输操作，现代 UNIX 操作系统提供了一种高度优化的代码执行路径。Linux 中使用 &lt;a href=&apos;http://man7.org/linux/man-pages/man2/sendfile.2.html&apos;&gt;sendfile 系统调用&lt;/a&gt; 可以利用这个优化。&lt;/p&gt;
&lt;p&gt;要理解 sendfile 的收益，需要先理解从文件到套接字传输数据的常规代码执行路径：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;操作系统从磁盘将数据读到内核空间的内存页缓存（pagecache）&lt;/li&gt;
&lt;li&gt;应用程序从内核空间减数据读到用户空间缓冲区&lt;/li&gt;
&lt;li&gt;应用程序将数据从用户空间缓冲区读到内核空间的套接字缓冲区&lt;/li&gt;
&lt;li&gt;操作系统将数据从套接字缓冲区读到 NIC 缓冲区，网卡从 NIC 缓冲区读取数据通过网络发出去&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;这一代码执行路径，涉及 4 次数据拷贝和 2 次系统调用，很显然是低效的。使用 sendfile，可以避免内核空间和用户空间之间一些不必要的数据拷贝，操作系统可以直接将数据从内存页缓存发送到网络。&lt;/p&gt;
&lt;p&gt;进一步了解 sendfile 以及 Java 平台如何支持零拷贝，可以阅读&lt;a href=&apos;https://developer.ibm.com/articles/j-zerocopy/&apos;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;生产者（The Producer）&lt;/h2&gt;
&lt;h4&gt;负载均衡&lt;/h4&gt;
&lt;p&gt;消息应该发到哪个分区（partition）由客户端根据哈希算法（或者随机）决定，并且消息是直接由 producer 发到目标分区的 leader broker，没有任何中间路由层。&lt;/p&gt;
&lt;p&gt;所有 Kafka 节点都可以响应元数据请求 - 告知客户端（producer 或 consumer）哪些服务节点还存活以及某个 topic 的各个分区 leader 分别是哪个节点（疑惑：如果某个分区 leader 节点挂掉之后，客户端如何获知？何时可以获知？）&lt;/p&gt;
&lt;h2&gt;消息交付语义&lt;/h2&gt;
&lt;p&gt;producer 和 consumer 之间的消息交付语义，分 3 种：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;最多消费一次 - 消息可能会丢失，但不会被重复消费&lt;/li&gt;
&lt;li&gt;最少消费一次 - 消息不会丢，但可能被重复消费&lt;/li&gt;
&lt;li&gt;仅消费一次 - 每个消息都会被消费且仅消费一次&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;这个问题可以分成两个阶段的问题：&lt;strong&gt;producer 向 broker 发布一个消息时的持久性保证&lt;/strong&gt; 以及 &lt;strong&gt;consumer 消费一个消息时的语义保证&lt;/strong&gt; （the durability guarantees for publishing a message and the guarantees when consuming a message）。&lt;/p&gt;
&lt;p&gt;producer 向 Kafka 集群发消息时，会提供一个请求参数 &lt;code&gt;acks&lt;/code&gt;：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;acks=0：表示 producer 不需要等分区 leader broker 返回任何响应，将消息存入套接字缓冲区（socket buffer）就当做消息已经发送成功。所以可靠性是没有保证的。&lt;/li&gt;
&lt;li&gt;acks=1：表示 分区 leader broker 将消息写入自己的本地日志文件，就向 producer 响应成功，不必等待分区副本 broker 同步好消息。&lt;/li&gt;
&lt;li&gt;acks=-1 或 acks=all：表示 分区 leader broker 需要等待所有同步副本 broker 同步好消息并响应成功，才向 producer 响应成功&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;第 2 种情况，如果分区 leader broker 挂掉/不存活，则副本未来得及同步的消息会丢失。&lt;/p&gt;
&lt;p&gt;第 3 种情况，只要有同步副本正常同步消息，那么即使 leader 挂了也不会丢数据。&lt;/p&gt;
&lt;p&gt;如果 leader 被系统判定为不存活，则会从（同步）副本中选举一个新的 leader，那么 Kafka 如何判定一个节点是否存活？存活判定依赖 2 个条件：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;节点必须维持与 Zookeeper 的 session 连接（通过 Zookeeper 的心跳机制）&lt;/li&gt;
&lt;li&gt;如果是一个从节点（follower），则必须不断从 leader 节点同步消息数据，且同步进度没有落后太多&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;如果 producer 在发送消息的过程中发生网络问题，它没法判定分区 leader 是否收到消息。0.11.0.0 版本之前，producer 只能重发消息，别无他法，因此只能提供“最少消费一次的”交付语义。0.11.0.0 版本之后，Kafka producer 支持一个幂等交付功能选项，可以确保消息重发不会导致 Kafka 的消息日志中出现重复的条目：broker 为每个 producer 分配一个 ID，然后基于消息序号来去重。&lt;/p&gt;
&lt;p&gt;也是从 0.11.0.0 版本开始，Producer 支持以类事务的语义向多个 topic 分区发送消息：要么所有消息都发送成功，要么都不成功。这个能力主要用于实现 Kafka topic 之间的仅处理一次语义。&lt;/p&gt;
&lt;p&gt;从 consumer 角度来看，同一个分区的所有副本，日志数据相同，消费进度也一样。consumer 可以控制自己对分区日志数据的消费位置。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;如果 consumer 读取消息后，先向 kafka 提交消费位置，再处理消息；如果该 consumer 挂掉或重启，会可能导致丢消息，从而只能满足“最多处理一次”交付语义。&lt;/li&gt;
&lt;li&gt;如果 consumer 读取消息后，是先处理，再提交消费位置；如果该 consumer 挂掉或重启，则可能导致重复消费消息，从而只能满足“最少处理一次”交付语义。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;如何实现“仅处理一次”语义？借助 Producer 的事务能力。&lt;/p&gt;
&lt;h2&gt;复制&lt;/h2&gt;
&lt;p&gt;复制的粒度/单元是 topic 分区。Kafka 集群中，每个分区都有一个 leader broker 节点，0个或多个从节点（follower）。分区读写都是由 leader broker 处理。&lt;/p&gt;
&lt;p&gt;如同一个普通的 consumer，从节点从 leader broker 拉取（pull）消息，然后写到自己的消息日志文件中。让从节点以 pull 的方式获取 leader 的消息数据，好处在于批量读写。&lt;/p&gt;
&lt;p&gt;对于 follower 节点而言，“是否存活”的实际含义是“是否顺利地从 leader 同步消息”，leader 节点会追踪“同步中”节点集（ISRs）。如果一个 follower 挂掉了/卡住了/同步落后太多了，则将其从这个 ISRs 中移除。follow 是否卡住或者同步落后太多，依据 &lt;code&gt;replica.lag.time.max.ms&lt;/code&gt; 配置参数判定。&lt;/p&gt;
&lt;p&gt;将某消息写到某个分区，如果该分区所有同步中副本都已经将该消息写到自己的消息日志文件中，则可以认为该消息的写操作已提交（committed），也就是真正的写成功。&lt;/p&gt;
&lt;p&gt;只有写提交的消息才会分发给 consumer。&lt;/p&gt;
&lt;p&gt;producer 可以选择是否等待消息写操作提交，在延迟（latency）和持久性（durability）之间权衡。&lt;/p&gt;
&lt;p&gt;Kafka 集群在某分区的 leader 节点挂掉之后，会快速进行失败转移（a short fail-over period），选举出新的分区 leader 节点，可用性不会受到影响。但如果发生网络分区（network partitions）问题，则无法保证可用性。CAP - C（Consistency）：一致性，A（Availability）：可用性，P（Partition Tolerance）：分区容错性 - 放弃了 分区容错性。&lt;/p&gt;
&lt;h4&gt;日志数据复制：仲裁成员集（Quorums）、同步中副本集（ISRs）和状态机&lt;/h4&gt;
&lt;p&gt;（备注：这一节我理解得还不太透彻。）&lt;/p&gt;
&lt;p&gt;一类常见的分布式系统是主从模式的，由主节点决定状态变化的顺序（the order of a series of values）。从节点通过日志复制（replicated log）方式同步状态数据。对于提交决策（commit decision）和选主（leader election），通常是基于多数人投票的机制。假设副本个数（注：个人理解包含主节点）为 2f+1，那么只有当 f+1 个副本写入成功，主节点才会将这个写操作标记为已提交（committed）。当主节点挂掉之后，基于 f 个状态最新的副本节点，可以选举出新的主节点，且状态不会有任何丢失。&lt;/p&gt;
&lt;p&gt;多数人投票方式，有一个优点：延迟取决于速度快的节点，而不是慢的。缺点是：对于实际的生产系统，抗风险能力还不够，而且不够灵活，不能让使用者做权衡。&lt;/p&gt;
&lt;p&gt;Kafka 选择仲裁成员集（quorum set）的方式与此不同，而不是基于多数人投票，而是动态维护一组同步中副本（ISR），这些副本与主节点保持同步。只有这组副本中的成员才有资格当选为主节点。ISR 集发生变化时会持久化到 Zookeeper 上。&lt;/p&gt;
&lt;p&gt;基于 ISR 模型，如果 topic 分区有 f+1 个副本，则可以容忍 f 个节点挂掉，也不会丢失任何已提交的消息。&lt;/p&gt;
&lt;p&gt;与 Kafka ISR 模型实际实现最相近的学术论文是微软的 &lt;a href=&apos;http://research.microsoft.com/apps/pubs/default.aspx?id=66814&apos;&gt;PacificA&lt;/a&gt;。&lt;/p&gt;
&lt;h4&gt;可用性和持久性保证&lt;/h4&gt;
&lt;p&gt;注意：producer 发送消息时设定 &lt;code&gt;acks=all&lt;/code&gt; 并不是要求所有的副本都确认写入成功，而是在当前同步中副本（ISR）都确认写入成功时，分区 leader 就向 producer 响应成功。例如：某个 topic 被设置为 2 个副本，然后其中一个副本节点挂掉，此时要求 &lt;code&gt;acks=all&lt;/code&gt; 的写操作也会成功。如果剩下的副本节点也挂了，那么就会丢消息啦。&lt;/p&gt;
&lt;p&gt;为了方便用户在 可用性 和 持久性 之间权衡，Kafka 提供两个 topic 级别的配置，用于 持久性 比 可用性 重要的情况：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&apos;http://kafka.apache.org/documentation/#design_uncleanleader&apos;&gt;禁用脏 leader 选举&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;指定一个最小 ISR 集大小（&lt;code&gt;min.insync.replicas&lt;/code&gt; 参数设置）：只有当 ISR 集大小大于设定的最小值，分区 [leader] 才会接受消息写入。这个设置只有当 producer 使用 &lt;code&gt;acks=all&lt;/code&gt; 时才会生效。（注：在我们生产环境中，分区副本数通常申请为 3（包含 leader），那么 &lt;code&gt;min.insync.replicas&lt;/code&gt; 应该设定为 2，但默认是 1。使用 1，那么当分区只有一个副本（即 leader），producer 也能写入成功，但如果这个副本又挂了，就会丢数据。）&lt;/li&gt;&lt;/ol&gt;
&lt;h4&gt;副本管理&lt;/h4&gt;
&lt;p&gt;一个 Kafka 集群上一般会有多个 topic，每个 topic 又有多个 partition，为了节点之间负载均衡，通常以&lt;strong&gt;循环（round-robin）方式&lt;/strong&gt;在所有节点上分布 partition 和 分区 leader 角色。&lt;/p&gt;
&lt;p&gt;另外，在分区 leader 节点之后重新选出 leader 之前，存在一段不可用的时间窗口，为了缩短这个时间窗口，Kafka 会从所有 broker 中选择一个作为“控制器（controller）”，这个控制器会检测 broker 级别的问题（failures），在发现某个 broker 挂掉之后，负责为受影响的分区指定新的 leader，而不是每个分区自己负责重新选主，这样的选主过程更轻量更快。如果控制器节点挂了，还存活的 broker 中的一个会成为新的控制器。&lt;/p&gt;
&lt;h2&gt;消费者消费进度跟踪&lt;/h2&gt;
&lt;p&gt;Kafka 为每个消费组（consumer group）指定一个 broker 来存储目标 topic 各个分区的消费进度（offsets），这个 broker 称为 &lt;strong&gt;组协调器（group coordinator）&lt;/strong&gt;。这个消费组中的任一消费者实例都应该将消费进度提交到这个组协调器，或者从这个组协调器获取启动之前上次的消费进度。Kafka 基于消费组的名称为消费组分配协调器。消费者可以向任一 broker 发送 FindCoordinatorRequest 请求来查找自己的协调器，并从 FindCoordinatorResponse 响应中获取协调器的详细信息。&lt;/p&gt;
&lt;p&gt;在组协调器接收到一个 OffsetCommitRequest 请求后，会将请求数据写到一个特殊的&lt;a href=&apos;http://kafka.apache.org/documentation/#compaction&apos;&gt;经压实的（compacted）&lt;/a&gt; Kafka topic - &lt;em&gt;__consumer_offsets&lt;/em&gt;。在目标分区的所有副本都确认收到了，协调器才会向消费者发送进度提交成功的响应。这个 topic 的消息日志数据会定期进行压实（compact），因为只需要为每个分区维护最新的消费进度。协调器也会在内存中缓存消费进度，方便快速响应消费进度查询请求。&lt;/p&gt;
&lt;p&gt;注：如果消费者/消费组特别多（例如：我们广告引擎服务，读取正排消息 topic，一个机器实例就是一个 consumer group，数量在几百到几千不等），那么组协调器的压力会比较大，那么确保组协调器的角色均匀分配到集群的所有 broker，比较关键。另外，&lt;em&gt;__consumer_offsets&lt;/em&gt; 这个 topic 的分区数量不能太少，最好和 broker 数量相同或者整数倍数量。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>读文笔记：日志 - 每个软件工程师都应该了解的实时数据统一抽象</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&apos;&gt;The Log: What every software engineer should know about real-time data&apos;s unifying abstraction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概括，这篇文章细说了 Kafka 的本质原理、解决的问题、适用性等。&lt;/p&gt;
&lt;p&gt;Kafka 本质上是提供日志数据流。&lt;/p&gt;
&lt;p&gt;日志是客观世界的事件记录。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;日志数据的特点是：只增不改，自带时间戳，数据存储的先后顺序即（大致）是实际发生的时间先后顺序。&lt;/p&gt;
&lt;p&gt;数据库可以基于日志来还原历史操作行为，并最终生成最新状态，主从同步就是这么干的。&lt;/p&gt;
&lt;p&gt;对于分布式系统而言，日志可以解决 2 个问题：按序改变状态和分发数据（ordering changes and distributing data）。&lt;/p&gt;
&lt;p&gt;状态机复制原则：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;分布式系统中各个节点可以依据日志来同步状态，达到（最终）一致性。并且，可以依据节点处理到哪行日志即可确定/表达该节点的状态。&lt;/p&gt;
&lt;p&gt;（日志）事件流（events）和数据表（tables）是一体两面（a facinating duality）：数据表的变更操作即是一个日志事件流，基于日志事件流可以生成数据表，并将其状态不断更新到最新，数据表的状态是日志事件流的在某个时间点的切面。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;events -&gt; table -&gt; events = events &lt;-&gt; table&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The magic of the log is that if it is a complete log of changes, it holds not only the contents of the final version of the table, but also allows recreating all other versions that might have existed. It is, effectively, a sort of backup of every previous state of the table.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;源码版本控制系统（比如 git）也是基于日志实现的分布式系统，一次 commit 相当于一次日志记录。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;对于互联网/金融等行业的公司来说，数据是重要资产，如何尽可能发挥数据的潜在价值为公司增收，至关重要。因为管理、技术上的原因，公司通常分多个业务部门，各业务部门提供若干服务，各个服务都会产出数据，这些数据很可能需要跨部门跨服务流通，流通的速度越快，周期越短，收益越大。&lt;/p&gt;
&lt;p&gt;以前，数据的处理方式主要是批处理，并不是因为没有流处理的技术，而是数据流通的基础设施跟不上，没做到持续的数据流。&lt;strong&gt;（注：这个说法，我个人只部分认同，很多时候，批处理的时延和收益可以满足大部分需求，实时流处理的边际效益可能并不明显）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;流式处理是批处理的泛化形式（stream processing is a generalization of batch processing, and, given the prevalence of real-time data, a very important generalization）。&lt;/p&gt;
&lt;p&gt;为了避免因数据流通导致各个服务之间的直接耦合，新增一个统一的数据通道中间服务，各个服务只管对数据通道进行写入或读出，不用关心数据是哪个服务写入的，或者哪些服务在消费/使用自己产出的数据。&lt;strong&gt;（解耦）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外，消费者消费数据的速率可能不一样，也可能会经历异常重启等情况，让消费者来控制速率，并且多个消费者之间不会相互干扰，会更好。&lt;/p&gt;
&lt;p&gt;基于数据流通的需求和日志的理念，Linkedin 设计开发了 Kafka。&lt;/p&gt;
&lt;p&gt;因为日志数据量可能会很大，日志数据本质上是有序串行的，如果支持数据分片，分片之间并行消费，分片内日志数据全局有序，数据流通的吞吐能力就可以无限扩展。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;基于 Kafka 这类数据管道，服务之间可以实现多级串联。（注：我们现在做的服务就是这么干）&lt;/p&gt;
&lt;p&gt;这种分布式系统架构中，至少涉及 producer（生产者）、broker（中间人）、consumer（消费者）三个角色，角色之间在某些工作上如何分工也是值得思考的：&lt;/p&gt;
&lt;p&gt;生产者产生的数据应该是什么样的？ - 统一格式/编码、方便解析&lt;/p&gt;
&lt;p&gt;中间人（kafka）需要解决什么问题？- 对于单个生产者写入的数据，保证按写入顺序有序地分发给消费者；解决数据高可用，高吞吐能力；支持回溯/重复消费（因此数据需要保留指定时间长度），从而消费者出问题后可以从头消费数据恢复状态。因为支持很多消费者消费同一个数据流，所以平均下来，Kafka 服务的成本会比较低。&lt;/p&gt;
&lt;p&gt;消费者按各自的需求进行数据转换存储。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;对于实现高吞吐能力，除了分片，Kakfa 还充分利用了攒批处理：生产者可以批量发送，中间人将数据攒批写入磁盘日志文件 等等。&lt;/p&gt;
&lt;p&gt;此外，由于涉及大量的磁盘文件和网络之间数据读写，Kafka 还充分利用操作系统内核的零拷贝传输能力。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>读文笔记：Photon - Fault-tolerant and Scalable Joining of Continuous Data Streams</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41318.pdf&apos;&gt;Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Photon 是谷歌广告系统中用于 join 广告曝光日志流和点击日志流的一套系统。&lt;/p&gt;
&lt;p&gt;数据流 join 为什么没用 flink 这类通用的流式处理框架？&lt;/p&gt;
&lt;p&gt;数据流 join，特别是广告数据流 join，技术上难在哪里？&lt;/p&gt;
&lt;p&gt;任一条流都可能乱序或延迟，广告点击涉及计费的问题，计费不能多算广告主的钱，也要尽可能避免漏计费，降低广告收入损失。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;该系统在谷歌生产环境中每分钟处理百万级的事件，端到端延迟小于 10 秒（注：对于广告实时竞价的广告主而言，这个延迟的长短很重要）。&lt;/p&gt;
&lt;p&gt;广告曝光、点击整体流程为：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;用户搜索某个关键词时，谷歌的服务器会返回广告和搜索结果。广告服务器会将广告 query 和结果数据作为日志发送到多个日志数据中心（multiple logs-datacenters），最终持久化存储在 GFS 上。每次 query 都会被赋予一个唯一性 ID &lt;em&gt;query_id&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;对于搜索结果中的广告，用户可能会点击。广告点击会触发一次请求，谷歌的后端服务器将请求重定向到广告主的网站。在重定向之前，谷歌服务器会将点击事件记录到日志中，发送到多个日志数据中心。点击事件日志中包含广告曝光的 query_id，点击事件也会被赋予一个唯一性 ID &lt;em&gt;click_id&lt;/em&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;计费是在点击之后，但计费所需要的广告出价等信息是在曝光请求中记录的，出于数据敏感性、带宽、请求处理延迟等多方面的考量，计费相关的信息并不会返回到用户客户端，也就是说点击请求中不会包含计费直接相关的信息，需要将 点击日志 和 曝光日志 做一次 join，得到一条完整的上下文日志，才方便做后续计费等处理。&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/EDOy6VKxeJUcgAW.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;论文中提了到该系统解决了几个技术挑战点：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;仅处理一次语义（exactly-once semantics）：实际上达到的是 最多处理一次语义 （At most once），也就是绝对不能多算钱，然后尽可能避免少算钱&lt;/li&gt;
&lt;li&gt;自动化的跨数据中心容错：也就是多数据中心部署，如果有一个数据中心不可用（比如 网络问题），也不会影响系统正常处理数据&lt;/li&gt;
&lt;li&gt;横向扩展性高：也就是加机器就能应对消息量增长&lt;/li&gt;
&lt;li&gt;低时延&lt;/li&gt;
&lt;li&gt;流乱序&lt;/li&gt;
&lt;li&gt;主流延迟（delayed primary stream）：这里说的”主流“是曝光日志流&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;在我看来，该系统的亮点主要在前 3 点，后边细说。&lt;/p&gt;
&lt;p&gt;为解决 1、2 挑战点，系统引入一个服务模块：IdRegistry，这个服务的功能：提供点击事件 id（&lt;em&gt;click_id&lt;/em&gt;） 的存储和查询，如果某个 click_id 可以从 IdRegistry 中查到，则表示该点击事件已经处理过了，不要再次处理。&lt;/p&gt;
&lt;p&gt;并且，多数据中心都部署一套 Photon，但 IdRegistry 共享一个，多套 Photon 系统的输入相同，那么 IdRegistry 除了提供去重的功能，还提供了负载均衡的功能。正常情况下，假设 N 个数据中心，每个数据中心 Photon join 产出的日志数据量为总量的 1/N。&lt;/p&gt;
&lt;p&gt;当某个数据中心的 Photon 不可用时，相当于其负载动态地重新分配到其它数据中心，虽然总体能力上降低了，但只要处理能力有冗余，就不会影响正常处理。&lt;/p&gt;
&lt;p&gt;那么很明显，IdRegistry 很可能成为系统的短板；另外，曝光/点击的唯一性 ID 如何生成？如果由一个中心服务来提供唯一性 ID 的生成，那么这个服务也会成为系统的短板。&lt;/p&gt;
&lt;p&gt;所以，系统没有选择一个中心服务来生成唯一性 id，而是将 id 设计为包含3个部分：&lt;em&gt;ServerIP&lt;/em&gt;、&lt;em&gt;ProcessID&lt;/em&gt;、&lt;em&gt;Timestamp&lt;/em&gt;。由于日志文件中行之间大致是按照时间戳有序的，所以 id 中包含时间戳的一个额外好处是：根据 id 即可大致定位日志内容。另外，还有一个和横向扩展性相关的好处，后边细说。&lt;/p&gt;
&lt;p&gt;IdRegistry 的角色至关重要，所以将其实现为一个基于 Paxos 协议的分布式系统，根据 CAP 原则，可用性（此处是指&lt;strong&gt;吞吐能力&lt;/strong&gt;）受限。解决方案是：&lt;/p&gt;
&lt;p&gt;1、提高单机处理能力：服务端攒批处理，尽可能减少网络往返次数导致的等待（特别是：由于 IdRegistry 是跨地域分布式，部署上节点之间最大延迟是 100ms 左右）&lt;/p&gt;
&lt;p&gt;2、分片（Sharding）：根据 click_id 进行分片，但如果是固定分片，那么随着以后业务量增大，不好扩展。Photon 使用了一种基于时间段动态分片方案，这个方案基于 click_id 自带时间戳。大致逻辑是：使用一个配置，内容大致如下：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/VRlgr2CoD8zw7Mm.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;对于每个 click 日志，先根据 click_id 中的时间戳，判断分片数，并计算对应的分片 id。&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/JMWyQvrDZCaAEdH.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;hr&gt;
&lt;p&gt;系统的模块关系图如下所示：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/zghk6oVaqZ5uEs3.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;Photon 的 Dispatcher 模块并没有以 Kafka 这种消息队列作为输入，而是直接监听文件系统中的日志文件变更，这一点有点奇怪，不是特别理解。&lt;/p&gt;
&lt;p&gt;Joiner 负责实际的 join 工作，由于 3、4 都比较耗时，所以为了尽可能减少 Joiner 的工作量，Dispatcher 将点击事件日志发送给 Joiner，会先到 IdRegistry 中查一下该事件是否已被处理过，从而起到过滤作用。因多数据中心部署，实际过滤比为：$ \frac{N-1}{N} $。&lt;/p&gt;
&lt;p&gt;为了确保 Joiner 高可用，Joiner 是无状态的，向 Dispatcher 提供 RPC 接口，Joiner 内部有限流，以保证不会因为单个 Joiner 负载过大，导致处理时延增大。Dispatcher 调用 Joiner 失败后会重试，重试使用的是指数退避算法。但处理失败的点击事件，是另外存储在 GFS 上，应该是由另外的线程来负责重试，不会影响正常的事件处理。&lt;/p&gt;
&lt;p&gt;当 Joiner 收到一个点击事件的处理请求时，会根据点击日志数据中的 &lt;em&gt;query_id&lt;/em&gt; 从 EventStore 查询曝光日志详情，但因为曝光日志数据流可能会有延迟，所以可能会查不到，查不到且发现 click_id 中的时间戳早于某个阈值（比如是 N天前的一个事件），Joiner 会将该 click_id 标记为不可 join，然后向 Dispatcher 返回成功；如果 click_id 中的时间戳不早于阈值，则向 Dispatcher 返回失败，由 Dispatcher 来重试。&lt;/p&gt;
&lt;p&gt;为了确保不会多计费，Joiner 在将 join 结果写入 Joined Click Logs 之前，会向 IdRegistry 注册 click_id。&lt;/p&gt;
&lt;p&gt;假设注册实际上已成功，但因网络原因或 RPC 调用超时 Joiner 未收到成功响应，此时怎么办？Joiner 向 IdRegistry 注册 click_id 时，会附带一个额外的 唯一性 token，也包含3个部分：Joiner 服务器地址、进程 ID、时间戳，IdRegistry 会把这个唯一性 token 作为值存储下来，所以对于这种情况，Joiner 可以重复发注册请求，如果 IdRegistry 根据 token 发现已注册成功的 click_id 和当前收到的 click_id 来自同一个 Joiner，则也会返回注册成功。&lt;/p&gt;
&lt;p&gt;假设注册成功，合并结果写入异常，异常分为 2 种，需要解决：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;写入之前，Joiner 节点宕机或重启&lt;/li&gt;
&lt;li&gt;合并结果实际写入成功，但因为网络原因，Joiner 未收到响应&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;为尽可能减少因某个 Joiner 节点硬件异常导致的 join 结果丢失，IdRegistry 对于单个 Joiner 的请求有限流，这个限流会间接导致 Joiner 对 Dispatcher 限流。&lt;/p&gt;
&lt;p&gt;为了进一步减少因为上面2种异常情况以及其它异常导致的 Join 结果丢失，Photon 还提供一个校验系统：获取原始点击事件日志，如果该日志 click_id 在 IdRegistry 中存在，但合并结果中不存在，则根据 IdRegistry 存储的对应 click_id 的 token，判断对应的 Joiner 是否存活，如果存活，则交于该 Joiner 重新处理，如果对应的 Joiner 已不存在，则从 IdRegistry 中删除该 click_id 记录，然后交于任一 Joiner 来处理，都一样。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;EventStore 获取原始的曝光日志，向 Joiner 提供查询接口，返回原始的曝光日志内容。&lt;/p&gt;
&lt;p&gt;基于时间局部性，EventStore 内部分 2 层，第一层为 CacheEventStore - 一个类似 Memcached 的 KV 内存映射，K 是 query_id，V 是曝光日志内容，基于一致性哈希算法根据 query_id 进行分片，缓存几分钟最新的曝光日志数据，可以命中 90% 左右的查询请求。&lt;/p&gt;
&lt;p&gt;如果 CacheEventStore 查询 miss，则交于第二层 LogsEventStore 来处理。LogsEventStore 对 query_id 和 曝光日志所在的日志文件及目标起始行（因为日志文件数据大致按时间戳有序，根据 query_id 中的时间戳大致可以知道查询的起始行）建立索引（实际存储在 BigTable 中），查询时，先根据 query_id，查到目标日志文件和起始行，然后从日志文件中读取原始曝光日志内容。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Lucene 查询解析器语法（译）</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;http://lucene.apache.org/core/8_2_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package.description&apos;&gt;Query Parser Syntax&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;概览&lt;/h2&gt;
&lt;p&gt;Lucene 除了提供 API 方便开发者创建查询请求，还通过一个查询解析器（一个词法分析器，使用 JavaCC 将一个字符串翻译成一个 Lucene 查询）提供一种功能丰富的查询语言。&lt;/p&gt;
&lt;p&gt;一般来说，查询解析器支持的语法在不同发布版本之间可能会有变化。当前这个文档页面描述的是当前这个发布版本的语法。如果你正在使用一个不同版本的 Lucene，请参考该版本自带的 docs/queryparsersyntax.html 文档。&lt;/p&gt;
&lt;p&gt;在选择使用这个查询解析器之前，请考虑以下 3 点：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;如果你准备以编程的方式生成一个查询字符串，然后使用查询解析器来解析它。那么，你应该认真考虑一下是否应该直接使用查询 API 来构建查询。换句话说，查询解析器专门用于人类输入的文本，而不是程序生成的文本。&lt;/li&gt;
&lt;li&gt;不可分词（untokenized）的域（译者注：抱歉，此处没太理解）最好直接添加到查询中，而不是通过查询解析器来解析。如果一个域的值是通过应用自动生成的，那么应该为这个域自动生成查询子句。分析器（查询解析器所使用的）是专门用于将人类输入的文本转换成一些词（terms），那么程序自动生成的值，也应该由程序自动添加到查询中。&lt;/li&gt;
&lt;li&gt;从查询形式来看，如果域的值是普通文本，则应该使用查询解析器。所有其它值类型，比如：日期范围、关键词等等，最好通过查询 API 直接添加。如果一个域的值仅限于一个有限的集合（可以通过一个下拉菜单指定），则不应该添加到查询字符串（后续会被解析）中，而是应该作为一个 TermQuery 子句添加到查询中。&lt;/li&gt;&lt;/ol&gt;
&lt;h2&gt;词（Terms）&lt;/h2&gt;
&lt;p&gt;一个查询语句可以拆解成 词（terms） 和 操作符（operators）。词又分为两种：单个词（single Terms）和短语（Phrases）。&lt;/p&gt;
&lt;p&gt;单个词是指 ”test“ 或 ”Hello“ 这类单词。&lt;/p&gt;
&lt;p&gt;短语是指以双引号包围起来的一组单词，比如：”hello dolly“。&lt;/p&gt;
&lt;p&gt;多个词（Multiple terms）可以使用布尔操作符组合在一起，实现一个更加复杂的查询（如下文所示）。&lt;/p&gt;
&lt;p&gt;备注：用于创建索引的解析器也会用于解析查询字符串中的词和短语。因此，选择合适的解析器很重要，否则解析器可能会被查询字符串中的词干扰（译者注：这句应该是指英文解析器可能无法对中文进行正确分词的问题）。&lt;/p&gt;
&lt;h2&gt;域（Fields）&lt;/h2&gt;
&lt;p&gt;Lucene 支持分多个字段/域的数据。搜索时，可以指定一个域，也可以使用默认域。域的名称以及默认域与具体实现相关。&lt;/p&gt;
&lt;p&gt;输入域的名称，后跟一个冒号（:），以及目标搜索词，即可对任意一个域进行搜索。&lt;/p&gt;
&lt;p&gt;举例来说，假设一个 Lucene 索引包含 2 个域：title 和 text，text 是默认域。若想查找标题为 ”The Right Way“ 且文本内容包含 ”don&apos;t go this way“ 的文档，可以输入：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;title:&amp;amp;quot;The Right Way&amp;amp;quot; AND text:go&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或者：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;title:&amp;amp;quot;The Right Way&amp;amp;quot; AND go&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;因为 text 是默认域，所以域的标志可以省略。&lt;/p&gt;
&lt;p&gt;注意：指定的域仅对紧跟其后的词生效，因此，如下查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;title:The Right Way&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;将对 title 域仅查找 ”The“，并对默认域（当前这个例子中是指 text 域）查找 ”Right“ 和 ”Way“。&lt;/p&gt;
&lt;h2&gt;词修饰语（Term Modifiers）&lt;/h2&gt;
&lt;p&gt;Lucene 支持修饰查询词（modifying query terms）来提供多种搜索方式。&lt;/p&gt;
&lt;h3&gt;通配符搜索&lt;/h3&gt;
&lt;p&gt;Lucene 支持对单个词(single terms)（不是短语查询 phrase queries）进行单个字符和多个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;?&lt;/code&gt; 符号进行单个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;*&lt;/code&gt; 符号进行多个字符的通配搜索。&lt;/p&gt;
&lt;p&gt;单字符通配搜索用于查找替换单个字符即可匹配的词。举例来说，若要搜索 ”text“ 或 ”test“，可以如下查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;te?t&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;多字符通配搜索用于查找替换0个或多个字符即可匹配的词。举例来说，若要搜索 ”test“、”tests“ 或 ”tester“，可以如下查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;test*&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也可以对词的中间部分进行通配搜索：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;te*t&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;备注：不可以将 &lt;code&gt;*&lt;/code&gt; 或 &lt;code&gt;?&lt;/code&gt; 符号用作一次搜索的首个字符。&lt;/p&gt;
&lt;h3&gt;正则表达式搜索&lt;/h3&gt;
&lt;p&gt;Lucene 支持正则表达式搜索，匹配斜杠（&lt;code&gt;/&lt;/code&gt;） 之间的模式。正则表达式的语法在不同的发布版本之间可能会有差异，目前支持的语法在 &lt;a href=&apos;http://lucene.apache.org/core/8_2_0/core/org/apache/lucene/util/automaton/RegExp.html?is-external=true&apos;&gt;RegExp&lt;/a&gt; 类文档中有说明。举例来说，查找包含 ”moat“ 或 ”boat“ 的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;/[mb]oat/&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;模糊搜索&lt;/h3&gt;
&lt;p&gt;Lucene 支持基于 Damerau-Levenshtein 编辑距离的模糊搜索。在单个词的最后添加波浪符（~）即可进行模糊搜索。举例来说，使用模糊搜索查找拼写上近似 ”roam“ 的词：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;roam~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个查询语句会找到 foam 和 roams 这类词。&lt;/p&gt;
&lt;p&gt;模糊搜索可以通过一个额外（可选）的参数来指定允许的最大编辑次数。这个参数值界于 0 和 2 之间，例如：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;roam~1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果未指定该参数，则默认使用 2 个编辑距离。&lt;/p&gt;
&lt;p&gt;以前，这里还允许使用浮点数。现在这个语法已被考虑弃用，将于 Lucene 5.0 中移除。&lt;/p&gt;
&lt;h3&gt;邻近搜索&lt;/h3&gt;
&lt;p&gt;Lucene 支持查找指定距离的邻近词。在短语的最后添加拨浪符（~）即可进行邻近搜索。举例来说，在文档中搜索 ”apache“ 和 ”jakarta“ 相距 10 个词的模式：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot;~10&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;范围搜索&lt;/h3&gt;
&lt;p&gt;范围查询可以要求域的值在范围查询语句指定的上下界之间。范围查询对于上下界可以包含也可以不包含。排序按照字典序进行。&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;mod_date:[20020101 TO 20030101]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个查询语句会查找 mod_date 域的值在 20020101 和 20030101 （包含上下界） 之间的文档。注意：范围查询并不是仅适用于日期域，也可以对非日期的域进行范围查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;title:{Aida TO Carmen}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个查询语句会查找到 title 域的值在 Aida 和 Carmen （不包含上下界）之间的所有文档。&lt;/p&gt;
&lt;p&gt;包含上下界的范围查询使用方括号来表示。不包含上下界的范围查询使用大括号来表示。&lt;/p&gt;
&lt;h3&gt;词加权（Boosting a term）&lt;/h3&gt;
&lt;p&gt;Lucene 会基于文档中找到的词对匹配到的文档提供相关性级别（译者疑问：基于向量余弦来计算相关性？）。可以在目标搜索词之后紧接一个脱字符 “^”，后跟一个加权系数（一个数字）来提升该搜索词的相关性权重。加权系统越高，查询命中的文档与该词的相关性越强。&lt;/p&gt;
&lt;p&gt;加权操作允许对词进行加权控制文档的相关性。例如，假设你正在搜索：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;jakarta apache&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后希望搜索结果和词 ”jakarta“ 更相关一些，则可以使用 ”^“ 符号后跟一个加权系数对这个词进行加权，即如下这样查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;jakarta^4 apache&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这会使得查找到的文档和词 ”jakarta“ 看起来更相关一些。也可以对短语进行加权，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot;^4 &amp;amp;quot;Apache Lucene&amp;amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认，加权系数是 1。加权系统可以小于 1（比如：0.2），但必须大于 0。&lt;/p&gt;
&lt;h2&gt;布尔操作符&lt;/h2&gt;
&lt;p&gt;布尔操作符允许使用逻辑操作符组合多个词。Lucene 支持的布尔操作符包含 &lt;code&gt;AND&lt;/code&gt;、&lt;code&gt;+&lt;/code&gt;、&lt;code&gt;OR&lt;/code&gt;、&lt;code&gt;NOT&lt;/code&gt; 及 &lt;code&gt;-&lt;/code&gt;（备注：布尔操作符必须全部是大写字母）。&lt;/p&gt;
&lt;h3&gt;OR&lt;/h3&gt;
&lt;p&gt;“OR” 操作符是默认的连接操作符。这意味着如果两个词之间没有布尔操作符，则使用 “OR” 操作符。OR 操作符链接两个词，并匹配包含其中任意一个词的文档。这相当于集合的并集操作。“||” 符合可用于替代单词 “OR”。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta apache” 或仅是 “jakarta” 的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot; jakarta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;或：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot; OR jakarta&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;AND&lt;/h3&gt;
&lt;p&gt;&quot;AND&quot; 操作符会匹配文本内容中同时存在两个词（因为 AND 是二元操作符）的文档。这相当于集合的交集操作。“&amp;&amp;” 符号可用于替代单词 “AND”。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta apache” 和 “Apache Lucene” 的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot; AND “Apache Lucene”&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;+&lt;/h3&gt;
&lt;p&gt;“+”（必需）操作符要求文档的某个域中包含 “+” 符号之后的词。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索（必须）包含 “jakarta” 以及可能包含 “lucene”（包不包含都可以）的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;+jakarta lucene&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;NOT&lt;/h3&gt;
&lt;p&gt;若文档包含”NOT“之后的词，”NOT“ 操作会排查该文档。这相当于集合的差集操作。”!“ 符号可用于替代单词 ”NOT“。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句搜索包含 ”jakarta apache“ 但不包含 ”Apache Lucene“ 的文档”：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot; NOT &amp;amp;quot;Apache Lucene&amp;amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;备注：“NOT” 操作符不可以用于单个词。例如，如下搜索不会返回任何结果：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;NOT &amp;amp;quot;jakarta apache&amp;amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;-&lt;/h3&gt;
&lt;p&gt;如果文档包含”-“符号之后的词，那么”-“（禁止）操作符会排除这些文档。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来查询包含 ”jakarta apache“ 但不包含 ”Apache Lucene“ 的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;quot;jakarta apache&amp;amp;quot; -&amp;amp;quot;Apache Lucene&amp;amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;分组&lt;/h2&gt;
&lt;p&gt;Lucene 支持使用圆括号对子句进行分组，构成子查询。如果你想控制一个查询语句的布尔逻辑，这对非常有用。&lt;/p&gt;
&lt;p&gt;比如，使用如下查询语句来搜索包含 “jakarta” 或 “apache”，以及 “website” 的文档：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;(jakarta OR apache) AND website&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如此就消除了任何困惑，确保你想表达是：必须存在 ”website“，以及可能存在词 ”jakarta“ 或 ”apache“。&lt;/p&gt;
&lt;h2&gt;域分组&lt;/h2&gt;
&lt;p&gt;Lucene 支持使用圆括号对单个域的多个子句进行分组。&lt;/p&gt;
&lt;p&gt;例如，若想搜索一个 title 中既包含单词“return”且包含短语“pink panther”，可以使用如下查询：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;title:(+return +&amp;amp;quot;pink panther&amp;amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;特殊字符转义&lt;/h2&gt;
&lt;p&gt;Lucene 支持对查询语法使用的特殊字符进行转移。目前这些特殊字符如下列表所示：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;+ - &amp;amp;amp;&amp;amp;amp; || ! ( ) { } [ ] ^ &amp;amp;quot; ~ * ? : \ /&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在特殊字符之前加 &lt;code&gt;\&lt;/code&gt; 来转义。例如，使用如下查询语句来搜索 &lt;code&gt;(1+1):2&lt;/code&gt;：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;\(1\+1\)\:2&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        
        <item>
            <title>一个 Python 小项目的小结</title>
            <description>&lt;p&gt;前段时间临时接手一个 Python 小项目，这个项目实现的类似一个管控平台，其中核心功能是为算法同学提供机器学习模型训练任务的全流程管理，平台后端基于 Flask 框架实现，前端基于 Ant Design Pro 实现。&lt;/p&gt;
&lt;p&gt;代码稍微有些乱，所以做了部分代码的重构，在此做点经验小结。&lt;/p&gt;
&lt;h3&gt;1、并行化或异步化&lt;/h3&gt;
&lt;p&gt;部分请求处理逻辑，由于比较耗时，故使用线程池来加速，或者使用独立线程异步处理，或者先存储一个中间状态，由后台定时任务来完成实际的处理工作。对于异步处理结果，前端通过轮询来获取。&lt;/p&gt;
&lt;p&gt;线程池的使用，主要使用 map 方法：&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;from multiprocessing.dummy import Pool

input_list = [...]
pool: Pool = Pool(len(input_list))
pool.map(func, input_list)
pool.close()
pool.join()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;独立线程异步处理：&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;import multiprocessing

p = multiprocessing.Process(target=func, args=(...))
p.start()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;定时任务，基于 apscheduler 库实现：&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;from apscheduler.schedulers.background
import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_join(func, &amp;amp;apos;interval&amp;amp;apos;, seconds=1)
scheduler.start()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;因为对于 Python 应用，通常会使用 gunicorn 这种 WSGI HTTP 服务器以多进程启动多个应用实例，提升请求吞吐能力。但是对于定时任务我们希望只有一个实例，对此，如果使用 gunicorn，可以基于它的 preload 机制来实现：&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;# wsgi.py
import app

if __name__ == &amp;amp;quot;__main__&amp;amp;quot;:
    app.run()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# 注意其中的 --preload 参数
gunicorn --workers=4 --preload --log-level=info --access-logfile=access.log -b 0.0.0.0:8080 wsgi:app&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;preload 机制简单来说，就是 import app 类所在的模块及其依赖的各个模块（import 过程中会执行其中的语句），然后 fork 出多个进程，每个进程都执行 app.run()。&lt;/p&gt;
&lt;h4&gt;2、实现一些通用方案对异常进行捕获或重试&lt;/h4&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def exception_try(times: int = 3, sleep_then_try_seconds=None):
    def decorator(f):
        def wrapper(*args, **kwargs):
            count = 0
            exception = None
            while count &amp;amp;lt; times:
                try:
                    return f(*args, **kwargs)
                except Exception as e:
                    exception = e
                    count += 1
                    logging.exception(&amp;amp;quot;Try {} times&amp;amp;quot;.format(count))
                    if (sleep_then_try_seconds is not None) and count &amp;amp;lt; times:
                        time.sleep(sleep_then_try_seconds)
            raise exception
        return wrapper
    return decorator&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;@exception_try(times=3, sleep_then_try_seconds=0.5)
def connect(self):
    return pymysql.connect(host=self.host, user=self.user, password=self.password, db=self.db, charset=self.charset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个装饰器方法用于实现异常重试，并且可以指定重试的时间间隔，实际使用下来效果较好。而且也不会因为 &lt;code&gt;try...except&lt;/code&gt; 导致大块代码缩进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;确保数据库连接关闭（其它类似资源也可以这样实现）&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def with_db(db: Connection, exception_callback=None):
    def decorator(f):
        def db_context(*a, **kw):
            try:
                return f(db, *a, **kw)
            except Exception as e:
                logging.exception(str(e))
                if exception_callback is not None:
                    exception_callback(e)
            finally:
                try:
                    db.close()
                except:
                    pass
        return db_context

    return decorator&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;# 将 conf.db.connect() 对象作为 delete_task_from_job_queue 的第一个参数注入，task_id 这个参数以不定参数的方式传入 delete_task_from_job_queue
with_db(conf.db.connect())(delete_task_from_job_queue)(task_id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个装饰器方法用于确保数据库连接在异常发生也能正常关闭，防止资源泄露。&lt;/p&gt;
&lt;h4&gt;3、循环等待或超时&lt;/h4&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;class TimeoutCondition(object):

    def __init__(self, condition_func, timeout_seconds):
        self.condition = condition_func
        self.timeout = timeout_seconds
        self.begin = None
        self.timeout_false = True
        self.cond_true = True

    def __bool__(self):
        if self.begin is None:
            self.begin = timeit.default_timer()
        self.cond_true = self.condition()
        self.timeout_false = self.timeout &amp;amp;lt;= 0 or (timeit.default_timer() - self.begin) &amp;amp;lt; self.timeout
        return self.cond_true and self.timeout_false

    def is_timeout(self):
        return self.cond_true and not self.timeout_false&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;cond = TimeoutCondition(lambda : len(service_list) == 0, 5)
while cond:
    time.sleep(1)
    service_list = get_service_list()
if cond.is_timeout():
    return None, None&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TimeoutCondition&lt;/code&gt; 用于实现循环等待某个条件满足，但为了避免死循环，所以加一个超时条件判断。实例化参数第一个是原始的条件判断 lambda 语句，第二个是一个超时设置。另外，借助魔术方法 &lt;code&gt;__bool__&lt;/code&gt;，让 TimeoutCondtion 的实例用起来像是一个布尔变量，调用 &lt;code&gt;is_timeout()&lt;/code&gt; 方法可以区分循环等待退出是因为原始条件满足，还是超时退出的。&lt;/p&gt;
&lt;h4&gt;4、按部署环境配置应用的行为&lt;/h4&gt;
&lt;p&gt;应用在不同的环境（开发、测试、生产）中应该允许加载不同的配置，配置不同的行为。&lt;/p&gt;
&lt;p&gt;当前应用处于什么环境，可以通过环境变量来配置，应用初始化时最先检测当前处于什么环境，之后的初始化流程就可以依据环境配置来加载配置，定制应用行为。&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;# conf/__init__.py
class AppConfig(object):
    app_env = os.getenv(&amp;amp;apos;APP_ENV&amp;amp;apos;, &amp;amp;apos;development&amp;amp;apos;)
    is_prod = app_env == &amp;amp;apos;production&amp;amp;apos;
    is_dev = app_env == &amp;amp;apos;development&amp;amp;apos;
    is_testing = app_env == &amp;amp;apos;testing&amp;amp;apos;
    
    # 其余应用配置项
    ...

conf = AppConfig()


def _load_config_by_env(env: str):
    &amp;amp;apos;&amp;amp;apos;&amp;amp;apos;
    不同环境加载不同的配置文件
    配置目录结构：
    conf/
        __init__.py
        development.py
        production.py
        testing.py
    &amp;amp;apos;&amp;amp;apos;&amp;amp;apos;
    module = importlib.import_module(&amp;amp;apos;conf.{}&amp;amp;apos;.format(env))
    if not hasattr(module, &amp;amp;apos;Config&amp;amp;apos;):
        logging.warning(&amp;amp;apos;Not find {} config&amp;amp;apos;.format(env))
        return
    for name, value in getattr(module, &amp;amp;apos;Config&amp;amp;apos;).__dict__.items():
        if name.startswith(&amp;amp;apos;__&amp;amp;apos;):
            continue
        conf.__dict__[name] = value&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;# 根据环境配置日志级别
log_level = logging.INFO if conf.is_prod else logging.DEBUG
logging.basicConfig(format=consts.LOG_FORMAT, level=log_level)&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;5、方便排查问题的日志输出&lt;/h4&gt;
&lt;p&gt;日志是问题排查的主要信息来源，所以日志记录得好不好，很关键。&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;# https://github.com/python/cpython/blob/3.7/Lib/logging/__init__.py#L457
# 日志时间 - 日志级别 - 代码文件路径 - 行号 - 进程 ID - 线程名称 - 日志内容
LOG_FORMAT = &amp;amp;apos;%(asctime)-15s - %(levelname)s - %(pathname)s - %(lineno)d - %(process)d - %(threadName)s - %(message)s&amp;amp;apos;&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;6、API 规范与异常提示&lt;/h4&gt;
&lt;p&gt;为了统一前端 API 响应处理，有必要对 API 响应体的结构指定标准。以我个人的习惯，所有从应用代码中返回的响应，HTTP 状态码都应该是 200，具体当前 API 请求成功还是失败，如果失败，失败的原因是什么都应该包含在响应体中，响应体大致的结构为：&lt;/p&gt;
&lt;pre class=&quot;language-json&quot;&gt;&lt;code&gt;{
    &amp;amp;quot;code&amp;amp;quot;: ...,
    &amp;amp;quot;msg&amp;amp;quot;: &amp;amp;quot;...&amp;amp;quot;,
    &amp;amp;quot;data&amp;amp;quot;: ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;code 表示请求处理失败时，data 字段可选，code 表示请求处理成功时，msg 字段可选。&lt;/p&gt;
&lt;p&gt;前端配合对响应体进行统一检测和提示：&lt;/p&gt;
&lt;pre class=&quot;language-javascript&quot;&gt;&lt;code&gt;import { notification } from &amp;amp;apos;antd&amp;amp;apos;;

function defaultHTTPCodeHandler(response) {
  if (response.status &amp;amp;gt;= 400) {
    // 注意 clone
    response.clone().text().then(respBody =&amp;amp;gt; {
      notification.error({message: &amp;amp;apos;API 异常响应&amp;amp;apos;, description: `${response.status}, ${respBody}`, duration: null});
      console.log(`${response.status}, ${respBody}`);
    });
  }
}

function defaultMsgCodeHandler(response) {
  if (response.status === 200) {
    // 注意 clone
    response.clone().json().then(jsonBody =&amp;amp;gt; {
      // 0、200、10000 都属于成功响应
      if (jsonBody !== undefined &amp;amp;amp;&amp;amp;amp; jsonBody.code !== undefined &amp;amp;amp;&amp;amp;amp; jsonBody.code !== 0 &amp;amp;amp;&amp;amp;amp; jsonBody.code !== 200 &amp;amp;amp;&amp;amp;amp; jsonBody.code != 10000) {
        notification.error({message: &amp;amp;apos;请求失败&amp;amp;apos;, description: `${jsonBody.code}, ${jsonBody.msg}`, duration: null});
        console.log(`${jsonBody.code}, ${jsonBody.msg}`);
      }
    });
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;并且统一封装发起请求的逻辑：&lt;/p&gt;
&lt;pre class=&quot;language-javascript&quot;&gt;&lt;code&gt;export function corsFetch(url, init, httpCodeCallback, msgCodeCallback) {
  const host = myHost();
  let urlPrefix = host;
  // 自带 host，则不额外补充 host 前缀
  if (url.startsWith(&amp;amp;quot;http://&amp;amp;quot;) || url.startsWith(&amp;amp;quot;https://&amp;amp;quot;)) {
    urlPrefix = &amp;amp;apos;&amp;amp;apos;;
  }
  const httpCodeHandler = httpCodeCallback === undefined ? defaultHTTPCodeHandler : httpCodeCallback;
  const msgCodeHandler = msgCodeCallback === undefined ? defaultMsgCodeHandler : msgCodeCallback;
  // 对于线上环境或者测试环境，不跨域
  if (host === PROD_ENV_HOST || host === TEST_ENV_HOST) {
    const promise = fetch(urlPrefix + url, init);
    promise.then((response) =&amp;amp;gt; httpCodeHandler(response));
    promise.then((response) =&amp;amp;gt; msgCodeHandler(response));
    return promise;
  }
  // 对于本地测试环境，跨域访问预发环境 API 数据，方便测试
  let corsInit = {
    credentials: &amp;amp;apos;include&amp;amp;apos;,
    mode: &amp;amp;apos;cors&amp;amp;apos;,
    redirect: &amp;amp;apos;follow&amp;amp;apos;,
  };
  if (init !== undefined) {
    corsInit = { ...corsInit, ...init };
  }
  if (urlPrefix !== &amp;amp;apos;&amp;amp;apos;) {
    urlPrefix = TEST_ENV_HOST;
  }
  const promise = fetch(urlPrefix + url, corsInit);
  promise.then((response) =&amp;amp;gt; httpCodeHandler(response));
  promise.then((response) =&amp;amp;gt; msgCodeHandler(response));
  return promise;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其中为了方便本地开发测试，允许本地开发环境跨域访问测试环境（最好不要直接跨越访问生产环境），并且自动区分，corsFetch 调用方无感知。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Reactor 官方文档翻译简化版</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;https://projectreactor.io/docs/core/release/reference/&apos;&gt;Reactor 3 Reference Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;1. 起步&lt;/h2&gt;
&lt;h3&gt;1.1 Reactor 简介&lt;/h3&gt;
&lt;p&gt;Reactor 是为 JVM 准备的一个完全非阻塞的反应式编程基础组件，支持高效的需求管理（以管理“反压”的形式），直接与 Java 8 的函数式 API 集成，尤其是 &lt;code&gt;CompletableFuture&lt;/code&gt;、&lt;code&gt;Stream&lt;/code&gt; 以及 &lt;code&gt;Duration&lt;/code&gt;，提供可组合的异步序列 API - &lt;code&gt;Flux&lt;/code&gt;（适用于 N 个元素的序列）和 &lt;code&gt;Mono&lt;/code&gt;（适用于 0 或 1个元素的序列）--- 并且全面地（extensively）实现了 &lt;a href=&apos;https://www.reactive-streams.org/&apos;&gt;反应式流（Reative Streams）&lt;/a&gt; 规范。&lt;/p&gt;
&lt;p&gt;借助 &lt;code&gt;reactor-netty&lt;/code&gt; 项目，Reactor 也支持进程间的非阻塞通信，适用于微服务架构。&lt;code&gt;reactor-netty&lt;/code&gt; 为 HTTP（包括 Websockets）、TCP 以及 UDP 提供支持反压的网络引擎，完全支持反应式编码解码。&lt;/p&gt;
&lt;h3&gt;1.2 理解 BOM&lt;/h3&gt;
&lt;p&gt;Reactor 3 开始采用 BOM （Bill of Materials，物料清单）发布模型（自 &lt;code&gt;reactor-core 3.0.4&lt;/code&gt; 开始，使用 &lt;code&gt;Aluminium&lt;/code&gt;（铝）版本序列），一个版本包含一组相关组件的版本，这些版本组件之间兼容性非常好，允许这些组件采用不同的版本命名方式。&lt;/p&gt;
&lt;p&gt;BOM 发布模型本身也是版本化的，以一个代号后接一个修饰词来命名一个版本序列。如下是一个示例列表：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;Aluminium-RELEASE
Californium-BUILD-SNAPSHOT
Aluminium-SR1
Bismuth-RELEASE
Californium-SR32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;代号等价于常规的 &lt;code&gt;大版本号.小版本号&lt;/code&gt; 形式，通常以字母升序方式取自 &lt;a href=&apos;https://en.wikipedia.org/wiki/Periodic_table#Overview&apos;&gt;元素周期表&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;按照时间顺序，修饰词分别为如下几个：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;BUILD-SNAPSHOT：为开发测试构建的版本。&lt;/li&gt;
&lt;li&gt;M1 .. N：里程碑版本或者开发者预览版本。&lt;/li&gt;
&lt;li&gt;RELEASE：一个代号系列中的首个 GA（General Availability 通用）发行版。&lt;/li&gt;
&lt;li&gt;SR1 .. N：一个代号系列中的后续 GA 发行版 - 相当于一个补丁版本。（SR 代表 “Service Release”（服务版本））&lt;/li&gt;&lt;/ul&gt;
&lt;h3&gt;1.3 如何获取 Reactor&lt;/h3&gt;
&lt;h4&gt;1.3.1 以 Maven 管理依赖包&lt;/h4&gt;
&lt;p&gt;Maven 原生支持 BOM 模型概念。首先，在你的 &lt;code&gt;pom.xml&lt;/code&gt; 文件添加如下代码片段来引入 BOM：&lt;/p&gt;
&lt;pre class=&quot;language-xml&quot;&gt;&lt;code&gt;&amp;amp;lt;dependencyManagement&amp;amp;gt; 
    &amp;amp;lt;dependencies&amp;amp;gt;
        &amp;amp;lt;dependency&amp;amp;gt;
            &amp;amp;lt;groupId&amp;amp;gt;io.projectreactor&amp;amp;lt;/groupId&amp;amp;gt;
            &amp;amp;lt;artifactId&amp;amp;gt;reactor-bom&amp;amp;lt;/artifactId&amp;amp;gt;
            &amp;amp;lt;version&amp;amp;gt;Bismuth-RELEASE&amp;amp;lt;/version&amp;amp;gt;
            &amp;amp;lt;type&amp;amp;gt;pom&amp;amp;lt;/type&amp;amp;gt;
            &amp;amp;lt;scope&amp;amp;gt;import&amp;amp;lt;/scope&amp;amp;gt;
        &amp;amp;lt;/dependency&amp;amp;gt;
    &amp;amp;lt;/dependencies&amp;amp;gt;
&amp;amp;lt;/dependencyManagement&amp;amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果顶部标签（&lt;code&gt;dependencyManagement&lt;/code&gt;）已经存在，则只添加上面该标签的内部内容。&lt;/p&gt;
&lt;p&gt;接下来，将依赖包添加到项目中，和一般依赖包一样，不过没有 &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt;，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-xml&quot;&gt;&lt;code&gt;&amp;amp;lt;dependencies&amp;amp;gt;
    &amp;amp;lt;dependency&amp;amp;gt;
        &amp;amp;lt;groupId&amp;amp;gt;io.projectreactor&amp;amp;lt;/groupId&amp;amp;gt;
        &amp;amp;lt;artifactId&amp;amp;gt;reactor-core&amp;amp;lt;/artifactId&amp;amp;gt; 
    &amp;amp;lt;/dependency&amp;amp;gt;
    &amp;amp;lt;dependency&amp;amp;gt;
        &amp;amp;lt;groupId&amp;amp;gt;io.projectreactor&amp;amp;lt;/groupId&amp;amp;gt;
        &amp;amp;lt;artifactId&amp;amp;gt;reactor-test&amp;amp;lt;/artifactId&amp;amp;gt; 
        &amp;amp;lt;scope&amp;amp;gt;test&amp;amp;lt;/scope&amp;amp;gt;
    &amp;amp;lt;/dependency&amp;amp;gt;
&amp;amp;lt;/dependencies&amp;amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;1.3.2 以 Gradle 管理依赖包&lt;/h4&gt;
&lt;p&gt;Gradle 核心并不支持 Maven BOM，不过可以借助 Spring 的 &lt;a href=&apos;https://github.com/spring-gradle-plugins/dependency-management-plugin&apos;&gt;gradle-dependency-management&lt;/a&gt; 插件。&lt;/p&gt;
&lt;p&gt;首先，应用插件，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-gradle&quot;&gt;&lt;code&gt;plugins {
    id &amp;amp;quot;io.spring.dependency-management&amp;amp;quot; version &amp;amp;quot;1.0.6.RELEASE&amp;amp;quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后使用它来引入 BOM，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-gradle&quot;&gt;&lt;code&gt;dependencyManagement {
     imports {
          mavenBom &amp;amp;quot;io.projectreactor:reactor-bom:Bismuth-RELEASE&amp;amp;quot;
     }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后将依赖添加到项目中，无需指定版本号，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-gradle&quot;&gt;&lt;code&gt;dependencies {
     compile &amp;amp;apos;io.projectreactor:reactor-core&amp;amp;apos; 
}&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 反应式编程简介&lt;/h2&gt;
&lt;p&gt;Reactor 是反应式编程范式的一个实现。反应式编程的定义归纳起来，如下所示：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;反应式编程是一个异步编程范式，关注数据流和变化的传播。这意味着通过被采用编程语言可以轻松地表达静态（比如 数组）或动态（比如 事件发射器）数据流。 --- https://en.wikipedia.org/wiki/Reactive_programming&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;反应式编程方向的首个重要工作是：微软在 .NET 生态体系中创建了反应式扩展（Rx）库，然后 RxJava 在 JVM 上实现了反应式编程。时光飞逝，经 Reative Streams 的大力推进，Java 社区终于出现了反应式编程标准，该规范定义了一组接口以及 JVM 上反应式编程库之间的交互规则。Java 9 标准库已将这组接口集成到 &lt;code&gt;Flow&lt;/code&gt; 类（译注：见&lt;a href=&apos;https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/Flow.html&apos;&gt;https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/Flow.html&lt;/a&gt;）中。&lt;/p&gt;
&lt;p&gt;反应式编程范式在面向对象语言中通常表现为一个观察者设计模式的扩展。你也可以将主流的反应式流模式（reactive streams pattern）和大家熟知的迭代器设计模式做对比，所有这些库中都存在对标于 &lt;code&gt;Iterable&lt;/code&gt; - &lt;code&gt;Iterator&lt;/code&gt; 的概念（译注：比如 发布者-消费者）。主要差别在于：迭代器是基于 pull 方式，反应式流则基于 push 方式。&lt;/p&gt;
&lt;p&gt;使用迭代器是一个命令式编程的模式，即使如何访问数据（accessing values）完全是 &lt;code&gt;Iterable&lt;/code&gt; 的职责，但实际上，何时访问序列中的下一个（&lt;code&gt;next()&lt;/code&gt;）值取决于开发者的选择。在反应式流中，上述 &lt;code&gt;Iterable&lt;/code&gt; - &lt;code&gt;Iterator&lt;/code&gt; 对的等价物为 &lt;code&gt;Publisher&lt;/code&gt; - &lt;code&gt;Subscriber&lt;/code&gt;。不过，在出现新的数据/事件时，由 &lt;code&gt;Publisher&lt;/code&gt; 通知 &lt;code&gt;Subscriber&lt;/code&gt;，这个“推”特性也是实现反应式的关键之处。并且，在被推送的值上应用哪些操作是声明式表达而不是命令式表达的：程序员表达的是计算逻辑而不是描述精确的控制流。&lt;/p&gt;
&lt;p&gt;除了“推”的特性，反应式流也良好地定义了如何处理错误和结束流。一个 &lt;code&gt;Publisher&lt;/code&gt; 可以向它的 &lt;code&gt;Subscriber&lt;/code&gt; 推送新的值（通过调用订阅者的 &lt;code&gt;onNext&lt;/code&gt; 方法），也可以推送错误（调用 &lt;code&gt;onError&lt;/code&gt; 方法）或结束（调用 &lt;code&gt;onComplete&lt;/code&gt;方法）信号。错误和结束信号都可以终结事件序列。简而言之，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;onNext x 0..N [onError | onComplete]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个方式非常灵活。这个模式支持“没有值”、“一个值”或“n个值”（包括值无限的序列，比如时钟的持续滴答事件）的各种使用场景。&lt;/p&gt;
&lt;p&gt;但是，起初，我们为什么需要这样一个异步的反应式的编程库？&lt;/p&gt;
&lt;h3&gt;2.1 阻塞即是资源浪费&lt;/h3&gt;
&lt;p&gt;现代的软件应用，并发用户量非常巨大，即使现代硬件的处理能力一直在提升，软件的性能仍旧是一个关键问题。&lt;/p&gt;
&lt;p&gt;宽泛来讲，提升一个程序的性能，有两种方式：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;并行化&lt;/strong&gt; 使用更多的线程和更多的硬件资源。&lt;/li&gt;
&lt;li&gt;对于当前的硬件资源，&lt;strong&gt;寻求更高效的使用方式&lt;/strong&gt;。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;通常，Java 开发者会使用阻塞性的代码编写程序，这种代码编写方式容易触及性能瓶颈，然后引入更多的线程来运行相似的阻塞性代码。但是，这种资源利用的扩展方式很快就会引发竞态（contention）和并发的问题。&lt;/p&gt;
&lt;p&gt;更糟糕的是，阻塞就意味着浪费资源。如果你稍加分析，就会发现一旦程序牵涉一些等待延迟（尤其是 I/0 操作，比如等待一个数据库请求或者一个网络调用），资源就会被浪费，因为此时线程（可能是大量线程）是空闲的，等待着数据。&lt;/p&gt;
&lt;p&gt;因此，并行化方式并非银弹。为了压榨出硬件的全部能力，并行化是必要的，但并行化的代码理解（reason about）起来也非常复杂，实际威力也会因为资源浪费而大打折扣。&lt;/p&gt;
&lt;h3&gt;2.2 异步可以解决问题吗？&lt;/h3&gt;
&lt;p&gt;前面提到的第二种方式 - 寻求更高效的使用方式 - 是资源浪费问题的一个解决方案。通过编写异步非阻塞的代码，在发生阻塞等待时，切换执行另一个活跃任务，活跃任务使用的是相同的底层资源，然后在异步处理过程结束后再切回到当前进程来执行。&lt;/p&gt;
&lt;p&gt;但是我们如何编写在 JVM 上异步执行的代码？ Java 提供了两种异步编程模型：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;回调&lt;/strong&gt;：异步方法没有返回值，但接受一个额外的 &lt;code&gt;callback（回调）&lt;/code&gt;参数（一个 lambda 表达式或匿名类），在得到异步处理结果时会调用这个回调。一个众所周知的例子是 Swing 的 &lt;code&gt;EventListener&lt;/code&gt; 派生类。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Future&lt;/strong&gt;：这种异步方法在调用时会&lt;em&gt;即刻&lt;/em&gt;返回一个 &lt;code&gt;Future&amp;lt;T&amp;gt;&lt;/code&gt;。这个异步过程会计算出一个 &lt;code&gt;T&lt;/code&gt; 类型的值，不过需要通过 &lt;code&gt;Future&lt;/code&gt; 对象来访问。计算出来的值不能立即可用，可以对 &lt;code&gt;Future&lt;/code&gt; 对象进行探询直到值计算出来。例如：&lt;code&gt;ExecutorService&lt;/code&gt; 运行 &lt;code&gt;Callable&amp;lt;T&amp;gt;&lt;/code&gt; 任务就是提供 &lt;code&gt;Future&lt;/code&gt; 对象来获取异步结果。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;那么这两种技术方案就足够好了吗？在很多使用场景下并不理想，这两种方式都有局限。&lt;/p&gt;
&lt;p&gt;多个回调难以组合使用，容易导致代码难以阅读和维护（就是所谓的“回调地狱”）。&lt;/p&gt;
&lt;p&gt;来看一个例子：在界面上为用户显示他最喜爱的5个物件，如果用户还没有任何喜欢的物件，则给出建议物件。这个逻辑涉及3个服务（第一个服务提供物件 ID，第二个服务获取物件的详细信息，第三个服务提供建议物件的详细信息），如下所示：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;回调地域的示例&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;userService.getFavorites(userId, new Callback&amp;amp;lt;List&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt;() { // 1
    public void onSuccess(List&amp;amp;lt;String&amp;amp;gt; list) { // 2
        if (list.isEmpty()) { // 3
            suggestionService.getSuggestions(new Callback&amp;amp;lt;List&amp;amp;lt;Favorite&amp;amp;gt;&amp;amp;gt;() {
                public void onSuccess(List&amp;amp;lt;Favorite&amp;amp;gt; list) { // 4
                    UiUtils.submitOnUiThread(() -&amp;amp;gt; { // 5
                        list.stream()
                            .limit(5)
                            .forEach(uiList::show); // 6
                    })
                }
                
                public void onError(Throwable error) { // 7
                    UiUtils.errorPopup(error);
                }
            });
        } else {
            list.stream() // 8
                .limit(5)
                .forEach(favId -&amp;amp;gt; favoriteService.getDetails(favId, // 9
                    new Callback&amp;amp;lt;Favorite&amp;amp;gt;() {
                        public void onSuccess(Favorite details) {
                            UiUtils.submitOnUiThread(() -&amp;amp;gt; uiList.show(details));
                        }

                        public void onError(Throwable error) {
                            UiUtils.errorPopup(error);
                        }
                    }
                ));
        }
    }
    
    public void onError(Throwable error) {
        UiUtils.errorPopup(error);
    }
})&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;基于回调的服务：&lt;code&gt;Callback&lt;/code&gt; 接口定义了两个方法，异步处理成功时调用其中的 &lt;code&gt;onSuccess&lt;/code&gt;，异步处理发生错误时调用 &lt;code&gt;onError&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;第一个服务以其结果 - 喜爱物件的 ID 列表 - 调用回调方法。&lt;/li&gt;
&lt;li&gt;如果列表为空，则必须转到 &lt;code&gt;suggestionService&lt;/code&gt; 来处理。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;suggestionService&lt;/code&gt; 向第二个回调传递一个 &lt;code&gt;List&amp;lt;Favorite&amp;gt;&lt;/code&gt; 列表。&lt;/li&gt;
&lt;li&gt;对于 UI 渲染，必须让消费数据的代码运行在 UI 的线程中。&lt;/li&gt;
&lt;li&gt;这里我们使用了 Java 8 的 &lt;code&gt;Stream&lt;/code&gt; 将建议物件的数量限制为5个，然后在 UI 中渲染成一个图形化列表。&lt;/li&gt;
&lt;li&gt;在每个回调层级，我们都以相同的方式处理错误：在弹出框中显示错误信息。&lt;/li&gt;
&lt;li&gt;回到 喜爱物件 ID 列表的层级。如果 &lt;code&gt;userService&lt;/code&gt; 服务返回一个不为空的 ID 列表，则转到 &lt;code&gt;favoriteService&lt;/code&gt; 去获取带详细信息的 &lt;code&gt;Favorite&lt;/code&gt; 对象。因为只需要5个喜爱物件，所以先使用流式处理将 ID 数量限制为 5 个。&lt;/li&gt;
&lt;li&gt;再一次，使用一个回调。这一次我们获取到完整的 &lt;code&gt;Favorite&lt;/code&gt; 对象，并在 UI 线程中将其在 UI 上渲染出来。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;看看有多少代码，理解起来也有点困难，其中也有一些重复的代码片段。再来看看使用 Reactor 如何来实现这段逻辑：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;和回调实现方式等价的 Reactor 实现&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;userService.getFavorite(userId) // 1
            .flatMap(favoriteService::getDetails) // 2
            .switchIfEmpty(suggestionService.getSuggestions()) // 3
            .take(5) // 4
            .publishOn(UiUtils.uiThreadScheduler()) // 5
            .subscribe(uiList::show, UiUtils::errorPopup); // 6&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;开启一个喜爱物件 ID 的流。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;异步地&lt;/em&gt;将 ID 转换成带详细信息的 &lt;code&gt;Favorite&lt;/code&gt; 对象（&lt;code&gt;flatMap&lt;/code&gt;）。至此我们得到一个 &lt;code&gt;Favorite&lt;/code&gt; 对象流。&lt;/li&gt;
&lt;li&gt;如果 &lt;code&gt;Favorite&lt;/code&gt; 流为空，则切换到备选处理方式 &lt;code&gt;suggestionService&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;我们只关注产出流中的前（最多）5个元素。&lt;/li&gt;
&lt;li&gt;最后，在 UI 线程中处理每份数据。&lt;/li&gt;
&lt;li&gt;真正触发流的处理：描述了如何处理最终的数据（显示为一个 UI 列表），以及在发生错误时如何处理（显示一个弹出框）。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;如果希望确保在 800ms 以内获取到喜爱物件 ID 列表，如果超时，则从缓存中获取数据，如何实现？基于回调的代码实现，这是一个复杂的任务。使用 Reactor，只需在操作链中添加一个 &lt;code&gt;timeout&lt;/code&gt; 算子就能轻松搞定，如下所示：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;超时回退处理的 Reactor 代码示例&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;userService.getFavorites(userId)
            .timeout(Duration.ofMillis(800)) // 1
            .onErrorResume(cacheService.cachedFavoritesFor(userId)) // 2
            .flatMap(favoriteService::getDetails)
            .switchIfEmpty(suggestionService.getSuggestions())
            .take(5)
            .publishOn(UiUtils.uiThreadScheduler())
            .subscribe(uiList::show, UiUtils::errorPopup);&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;如果前置处理超过 800ms 还没输出任何事件，则下发一个错误。&lt;/li&gt;
&lt;li&gt;在收到错误事件时，回退到调用 &lt;code&gt;cacheService&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;操作链的余下部分和前一个例子类似。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;使用 &lt;code&gt;Future&lt;/code&gt; 对象相比回调更好一点，不过组合使用起来也不太方便，尽管 Java 8 引入 &lt;code&gt;CompletableFuture&lt;/code&gt; 改善了这一问题。将多个 &lt;code&gt;Future&lt;/code&gt; 对象组织在一起，可行但并不容易。另外，&lt;code&gt;Future&lt;/code&gt; 还有其它问题：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;容易碰到另一个阻塞的情况：调用 &lt;code&gt;Future&lt;/code&gt; 对象的 &lt;code&gt;get()&lt;/code&gt; 方法。&lt;/li&gt;
&lt;li&gt;不支持惰性计算。&lt;/li&gt;
&lt;li&gt;对多个值的处理和高级错误处理缺乏支持。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;来看看另一个例子：先获取一个 ID 列表，然后根据 ID 获取一个名字以及获取一个统计数值，再将名字和统计数值组合起来使用，这几个步骤都必须是异步的。如下示例以一组 &lt;code&gt;CompletableFuture&lt;/code&gt; 来实现这个逻辑：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;CompletableFuture&amp;amp;lt;List&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt; ids = ifhIds(); // 1

CompletableFuture&amp;amp;lt;List&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt; results = ids.thenComposeAsync(l -&amp;amp;gt; { // 2
    Stream&amp;amp;lt;CompletableFuture&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt; zip = 
            l.stream().map(i -&amp;amp;gt; { // 3
                CompletableFuture&amp;amp;lt;String&amp;amp;gt; nameTask = ifhName(i); // 4
                CompletableFuture&amp;amp;lt;Integer&amp;amp;gt; statTask = ifhStat(i); // 5
                return nameTask.thenCombineAsync(statTask, (name, stat) -&amp;amp;gt; &amp;amp;quot;Name &amp;amp;quot; + name + &amp;amp;quot; has stats &amp;amp;quot; + stat); // 6
            });
    List&amp;amp;lt;CompletableFuture&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt; combinationList = zip.collect(Collectors.toList()); // 7
    CompletableFuture&amp;amp;lt;String&amp;amp;gt;[] combinationArray = combinationList.toArray(new CompletableFuture[combinationList.size()]);
    
    CompletableFuture&amp;amp;lt;Void&amp;amp;gt; allDone = CompletableFuture.allOf(combinationArray); // 8
    return allDone.thenApply(v -&amp;amp;gt; combinationList.stream()
                    .map(CompletableFuture::join) // 9
                    .collect(Collectors.toList()));
});

List&amp;amp;lt;String&amp;amp;gt; results = result.join(); // 10
assertThat(results).contains(
		&amp;amp;quot;Name NameJoe has stats 103&amp;amp;quot;,
		&amp;amp;quot;Name NameBart has stats 104&amp;amp;quot;,
		&amp;amp;quot;Name NameHenry has stats 105&amp;amp;quot;,
		&amp;amp;quot;Name NameNicole has stats 106&amp;amp;quot;,
		&amp;amp;quot;Name NameABSLAJNFOAJNFOANFANSF has stats 121&amp;amp;quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;一开始获得一个 &lt;code&gt;Future&lt;/code&gt; 结果 - 为后续处理提供一个 &lt;code&gt;id&lt;/code&gt; 列表。&lt;/li&gt;
&lt;li&gt;一旦获得 &lt;code&gt;id&lt;/code&gt; 列表就可以开始进一步的异步处理。&lt;/li&gt;
&lt;li&gt;逐个处理列表中的元素。&lt;/li&gt;
&lt;li&gt;异步获取关联的名字。&lt;/li&gt;
&lt;li&gt;异步获取关联的统计数值。&lt;/li&gt;
&lt;li&gt;组合两个异步结果。&lt;/li&gt;
&lt;li&gt;至此我们得到一个 &lt;code&gt;Future&lt;/code&gt; 对象列表，表示所有的组合任务。&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;Future&lt;/code&gt; 对象数组传给 &lt;code&gt;CompletableFuture.allOf&lt;/code&gt; 方法，这个方法会输出一个 &lt;code&gt;Future&lt;/code&gt; 对象，当 &lt;code&gt;Future&lt;/code&gt; 对象数组代表的异步任务都完成时，这个 &lt;code&gt;Future&lt;/code&gt; 对象代表的异步任务也就完成了。&lt;/li&gt;
&lt;li&gt;此处的特殊之处在于：在（&lt;code&gt;allOf&lt;/code&gt; 返回的）&lt;code&gt;CompletableFuture&amp;lt;Void&amp;gt;&lt;/code&gt; 对象表示的异步任务结束时，遍历 &lt;code&gt;Future&lt;/code&gt; 对象列表（combinationList），使用 &lt;code&gt;join()&lt;/code&gt; 方法（此次不会阻塞，因为 &lt;code&gt;allOf&lt;/code&gt; 会确保所有异步任务都已完成）获取收集异步任务结果。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;10. 触发执行整个异步处理流水线（调用 &lt;code&gt;join()&lt;/code&gt; 方法），然而等着异步处理完成并返回一个结果列表，就可以进行断言判断了。&lt;/p&gt;
&lt;p&gt;Reactor 自带了很多组合算子，可以简化这个处理过程的实现，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; ids = ifhrIds(); // 1

Flux&amp;amp;lt;String&amp;amp;gt; combinations = 
        ids.flatMap(id -&amp;amp;gt; { // 2
            Mono&amp;amp;lt;String&amp;amp;gt; nameTask = ifhrName(id); // 3
            Mono&amp;amp;lt;Integer&amp;amp;gt; statTask = ifhrStat(id); // 4
            
            return nameTask.zipWith(statTask, // 5
                    (name, stat) -&amp;amp;gt; &amp;amp;quot;Name &amp;amp;quot; + name + &amp;amp;quot; has stats &amp;amp;quot; + stat);
        });
        
Mono&amp;amp;lt;List&amp;amp;lt;String&amp;amp;gt;&amp;amp;gt; result = combinations.collectList(); // 6

List&amp;amp;lt;String&amp;amp;gt; results = result.block(); // 7
assertThat(results).containsExactly( // 8
    &amp;amp;quot;Name NameJoe has stats 103&amp;amp;quot;,
    &amp;amp;quot;Name NameBart has stats 104&amp;amp;quot;,
    &amp;amp;quot;Name NameHenry has stats 105&amp;amp;quot;,
    &amp;amp;quot;Name NameNicole has stats 106&amp;amp;quot;,
    &amp;amp;quot;Name NameABSLAJNFOAJNFOANFANSF has stats 121&amp;amp;quot;
);&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;这次，一开始我们得到一个异步提供的字符串序列（&lt;code&gt;ids&lt;/code&gt;）（一个 &lt;code&gt;Flux&amp;lt;String&amp;gt;&lt;/code&gt; 对象）。&lt;/li&gt;
&lt;li&gt;对于序列中的每个元素，异步处理两次（在 &lt;code&gt;flatMap&lt;/code&gt; 的 lambda 参数值中）。&lt;/li&gt;
&lt;li&gt;获取关联的名字。&lt;/li&gt;
&lt;li&gt;获取关联的统计值。&lt;/li&gt;
&lt;li&gt;异步组合两个值&lt;/li&gt;
&lt;li&gt;在异步处理的结果可用时，将它们聚合到一个 &lt;code&gt;List&lt;/code&gt; 对象中。&lt;/li&gt;
&lt;li&gt;在实际项目中，我们通常会继续异步处理 &lt;code&gt;Flux&lt;/code&gt;，比如：异步组合使用它或者直接订阅它。最可能的是，返回这个 &lt;code&gt;Mono&lt;/code&gt; 类型的 &lt;code&gt;result&lt;/code&gt;。因为这里只是个测试，所以使用了 block，等待处理结束，直接返回值的聚合列表。&lt;/li&gt;
&lt;li&gt;对结果进行断言判断。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;使用回调和 &lt;code&gt;Future&lt;/code&gt; 对象的问题是类似的，反应式编程以 &lt;code&gt;发布者（Publisher）- 订阅者（Subscriber）&lt;/code&gt; 解决了这些问题。&lt;/p&gt;
&lt;h3&gt;2.3 从命令式到反应式编程&lt;/h3&gt;
&lt;p&gt;反应式编程库，比如 Reactor，目标是解决 JVM 上“经典”异步处理方式的弊端，同时也专注于提供以下几个方面的特性：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;可组合性&lt;/strong&gt; 和 &lt;strong&gt;代码可读性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;将数据视作一个&lt;strong&gt;流&lt;/strong&gt;，并提供丰富的&lt;strong&gt;算子&lt;/strong&gt;来操作流&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;订阅（subscriber）&lt;/strong&gt;之前不会实际做任何事情&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反压&lt;/strong&gt; 或者说 消费者通知生产者流速过高的能力&lt;/li&gt;
&lt;li&gt;与并发无关（concurrency-agnostic）的&lt;strong&gt;高阶（high level）&lt;/strong&gt;抽象，&lt;strong&gt;适用性强（high value）&lt;/strong&gt;（译注：并发无关是指这种抽象对于并发非并发的场景都适用）&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;3. Reactor 核心特性&lt;/h2&gt;
&lt;p&gt;Reactor 项目的主要成果是 &lt;code&gt;reactor-core&lt;/code&gt; - 一个遵循&lt;a href=&apos;https://www.reactive-streams.org/&apos;&gt;反应式流&lt;/a&gt;规范并支持 Java 8 的反应式编程库。&lt;/p&gt;
&lt;p&gt;Reactor 引入 2 个可组合的反应式类型（实现了 &lt;code&gt;Publisher&lt;/code&gt; 接口并且提供丰富的算子）： &lt;code&gt;Flux&lt;/code&gt; 和 &lt;code&gt;Mono&lt;/code&gt;。一个 &lt;code&gt;Flux&lt;/code&gt; 对象代表包含 0 到 N 个元素的反应式序列，&lt;code&gt;Mono&lt;/code&gt; 对象代表单值或空（0或1个元素）的结果。&lt;/p&gt;
&lt;h3&gt;3.1 Flux - 0-N 个值的异步序列&lt;/h3&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/oKMX4rTvUViZRHj.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;h3&gt;3.2 Mono - 包含 0 或 1 个值的异步结果&lt;/h3&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/8WxGgH9UkcQwuX4.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;h3&gt;3.3 创建一个 Flux 或 Mono 并进行订阅的一些简单方法&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Flux&lt;/code&gt; 和 &lt;code&gt;Mono&lt;/code&gt; 的类中包含大量的工厂方法，上手使用 Reactor 最简单的方式是从中选择一个用起来。&lt;/p&gt;
&lt;p&gt;例如，创建一个 &lt;code&gt;String&lt;/code&gt; 序列，可以逐个列举出这些字符串，或者将这些字符串放到一个集合中，然后基于这个集合创建一个 Flux，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; seq1 = Flux.just(&amp;amp;quot;foo&amp;amp;quot;, &amp;amp;quot;bar&amp;amp;quot;, &amp;amp;quot;foobar&amp;amp;quot;);

List&amp;amp;lt;String&amp;amp;gt; iterable = Arrays.asList(&amp;amp;quot;foo&amp;amp;quot;, &amp;amp;quot;bar&amp;amp;quot;, &amp;amp;quot;foobar&amp;amp;quot;);
Flux&amp;amp;lt;String&amp;amp;gt; seq2 = Flux.fromIterable(iterable);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其它一些工厂方法的使用示例如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Mono&amp;amp;lt;String&amp;amp;gt; noData = Mono.empty();
Mono&amp;amp;lt;String&amp;amp;gt; data = Mono.just(&amp;amp;quot;foo&amp;amp;quot;);
Flux&amp;amp;lt;Integer&amp;amp;gt; numbersFromFiveToSeven = Flux.range(5, 3);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于订阅操作，&lt;code&gt;Flux&lt;/code&gt; 和 &lt;code&gt;Mono&lt;/code&gt; 借助了 Java 8 的 lambda 表达式。有大量 &lt;code&gt;.subscribe()&lt;/code&gt; 的重载方法/变种方法（variants）可选选择使用，使用 lambda 表达式来实现回调的不同组合，如下所示是这些方法的签名：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Flux 中基于 lambda 表达式的订阅方法变种&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;subscribe();

subscribe(Consumer&amp;amp;lt;? super T&amp;amp;gt; consumer);

subscribe(Consumer&amp;amp;lt;? super T&amp;amp;gt; consumer,
          Consumer&amp;amp;lt;? super Throwable&amp;amp;gt; errorConsumer);

subscribe(Consumer&amp;amp;lt;? super T&amp;amp;gt; consumer,
          Consumer&amp;amp;lt;? super Throwable&amp;amp;gt; errorConsumer,
          Runnable completeConsumer);

subscribe(Consumer&amp;amp;lt;? super T&amp;amp;gt; consumer,
          Consumer&amp;amp;lt;? super Throwable&amp;amp;gt; errorConsumer,
          Runnable completeConsumer,
          Consumer&amp;amp;lt;? super Subscription&amp;amp;gt; subscriptionConsumer);&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;&lt;p&gt;这些订阅方法都会返回一个订阅操作的引用，当不再需要更多的数据时，可以使用这个引用来取消订阅。一旦取消，数据源就应该停止产出数据，并清理使用的所有资源。这一 “取消并清理” 行为在 Reactor 中以通用的 &lt;code&gt;Disposable&lt;/code&gt; 接口来表现。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4&gt;3.3.1 lambda 表达式的替代方案：BaseSubscriber&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Flux&lt;/code&gt; 和 &lt;code&gt;Mono&lt;/code&gt; 提供了一个相比上面那么订阅方法更通用的 &lt;code&gt;subscribe&lt;/code&gt; 方法，其参数是一个完整的 &lt;code&gt;Subscriber&lt;/code&gt; 实例，而不是根据几个 lambda 表达式组合出一个 &lt;code&gt;Subscriber&lt;/code&gt; 实例。为了方便实现这样的一个 &lt;code&gt;Subscriber&lt;/code&gt;，Reactor 提供了一个名为 &lt;code&gt;BaseSubscriber&lt;/code&gt; 的可扩展的抽象类。&lt;/p&gt;
&lt;p&gt;下面来实现一个，我们将其命名为 &lt;code&gt;SampleSubscriber&lt;/code&gt;。如下示例演示了如何将其应用到一个 &lt;code&gt;Flux&lt;/code&gt; 序列上：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;SampleSubscriber&amp;amp;lt;Integer&amp;amp;gt; ss = new SampleSubscriber&amp;amp;lt;Integer&amp;amp;gt;();
Flux&amp;amp;lt;Integer&amp;amp;gt; ints = Flux.range(1, 4);
//
ints.subscribe(i -&amp;amp;gt; System.out.println(i),
    error -&amp;amp;gt; System.err.println(&amp;amp;quot;Error &amp;amp;quot; + error),
    () -&amp;amp;gt; {System.out.println(&amp;amp;quot;Done&amp;amp;quot;);},
    s -&amp;amp;gt; s.request(10));
//
ints.subscribe(ss);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如下示例演示了 &lt;code&gt;SampleSubscriber&lt;/code&gt; 继承自 &lt;code&gt;BaseSubscriber&lt;/code&gt; 的一个最简化实现：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;package io.projectreactor.samples;

import org.reactivestreams.Subscription;
import reactor.core.publisher.BaseSubscriber;

public class SampleSubscriber&amp;amp;lt;T&amp;amp;gt; extends BaseSubscriber&amp;amp;lt;T&amp;amp;gt; {

	public void hookOnSubscribe(Subscription subscription) {
		System.out.println(&amp;amp;quot;Subscribed&amp;amp;quot;);
		request(1);
	}

	public void hookOnNext(T value) {
		System.out.println(value);
		request(1);
	}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;BaseSubscriber&lt;/code&gt; 还提供了一个 &lt;code&gt;requestUnbounded()&lt;/code&gt; 方法来切换到无限消费模式（相当于 &lt;code&gt;request(Long.MAX_VALUES)&lt;/code&gt;），另外也提供了一个 &lt;code&gt;cancel()&lt;/code&gt; 方法。&lt;/p&gt;
&lt;p&gt;除了 &lt;code&gt;hookOnSubscribe&lt;/code&gt; 和 &lt;code&gt;hookOnNext&lt;/code&gt;，&lt;code&gt;BaseSubscriber&lt;/code&gt; 还提供了其他钩子方法（方法体为空，提供继承重写）：&lt;code&gt;hookOnComplete&lt;/code&gt;、&lt;code&gt;hookOnError&lt;/code&gt;、&lt;code&gt;hookOnCancel&lt;/code&gt; 以及 &lt;code&gt;hookFinally&lt;/code&gt;（当事件/消息序列（流）终止时，一定会调用该方法，调用时会传入一个 &lt;code&gt;SignalType&lt;/code&gt; 类型参数表示终止的类型）。&lt;/p&gt;
&lt;h4&gt;3.3.2 关于反压和调整请求量的方式&lt;/h4&gt;
&lt;p&gt;在 Reactor 中实现反压，是通过向上游算子发送一个 &lt;code&gt;请求（request）&lt;/code&gt;来逐级传播消费者的压力，直到数据源。当前请求的总量有时又称为当前的“需求量” 或者 “待满足（pending）的请求量”。需求量的上限是 &lt;code&gt;Long.MAX_VALUE&lt;/code&gt;，表示一个无限量的请求（意思是“尽快产出数据“ - 反压也就失效了）。&lt;/p&gt;
&lt;p&gt;最终的订阅者在订阅之前会发出首个请求，订阅所有消息/数据最直接的方式是即刻触发一个无限量（Long.MAX_VALUE）的请求：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;subscribe()&lt;/code&gt; 以及大部分基于 lambda 表达式的变种方法（除了那个接受 &lt;code&gt;Consumer&amp;lt;Subscription&amp;gt;&lt;/code&gt; 类型参数的方法）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;block()&lt;/code&gt;、&lt;code&gt;blockFirst()&lt;/code&gt; 和 &lt;code&gt;blockLast()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;调用 &lt;code&gt;toIterable()&lt;/code&gt; 或 &lt;code&gt;toStream()&lt;/code&gt; 进行遍历&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;对首个请求进行定制的最简单方式是以一个 &lt;code&gt;BaseSubscriber&lt;/code&gt; 派生类实例来 &lt;code&gt;subscribe&lt;/code&gt;，派生类重写 &lt;code&gt;BaseSubscriber&lt;/code&gt; 的 &lt;code&gt;hookOnSubscribe&lt;/code&gt; 方法，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux.range(1, 10)
    .doOnRequest(r -&amp;amp;gt; System.out.println(&amp;amp;quot;request of &amp;amp;quot; + r))
    .subscribe(new BaseSubscriber&amp;amp;lt;Integer&amp;amp;gt;() {

      @Override
      public void hookOnSubscribe(Subscription subscription) {
        request(1);
      }

      @Override
      public void hookOnNext(Integer integer) {
        System.out.println(&amp;amp;quot;Cancelling after having received &amp;amp;quot; + integer);
        cancel();
      }
    });&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面这个代码片段输出如下内容：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;request of 1
Cancelling after having received 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;改变下游需求量的算子&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;谨记：订阅时指定的需求量，上游操作链中的每个算子都可以对其作出调整。一个典型案例是 &lt;code&gt;buffer(N)&lt;/code&gt; 算子：如果它收到一个 &lt;code&gt;request(2)&lt;/code&gt; 请求，它会理解为2个缓冲区的请求量。因为缓冲区需要 N 个元素才认为是满的，所以 &lt;code&gt;buffer&lt;/code&gt; 算子将请求量调整成了 &lt;code&gt;2 x N&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;你也许也注意到某些算子存在这样的变种 - 接受一个名为 &lt;code&gt;prefetch&lt;/code&gt; 的 &lt;code&gt;int&lt;/code&gt; 类型参数。这是另外一类修改下游请求量的算子。这类算子（比如 &lt;code&gt;flatMap&lt;/code&gt;）通常是处理内部序列（inner sequences），从每个进入的元素派生出一个 &lt;code&gt;Publisher&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;预取（prefetch）&lt;/code&gt;是调整内部序列请求量的一个方式。如果未指定，多数这类算子会以 32 为初始需求量。&lt;/p&gt;
&lt;p&gt;这类算子通常也会实现一个&lt;strong&gt;填补优化方案&lt;/strong&gt;：算子一旦看到 25% 的预取请求量已完成，就会向上游再发起 25% 的请求量。这是一个启发式优化，如此这类算子就可以主动地为即将到来的请求量做好准备。&lt;/p&gt;
&lt;p&gt;最后，再介绍一对直接用于调整请求量的算子：&lt;code&gt;limitRate&lt;/code&gt; 和 &lt;code&gt;limitRequest&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;limitRate(N)&lt;/code&gt; 把下游的请求量拆分成多个更小量的请求向上游传播。例如，一个 &lt;code&gt;100&lt;/code&gt; 的请求传到算子 &lt;code&gt;limitRate(10)&lt;/code&gt;，则会变成 10 次请求，一次请求 10，传播到上游。注意：&lt;code&gt;limitRate&lt;/code&gt; 实际上以这种形式实现了前面提到的填补优化方案。&lt;/p&gt;
&lt;p&gt;这个算子有一个变种，允许开发者调整预取填补量（即算子变种的 &lt;code&gt;lowTide&lt;/code&gt; 参数）：&lt;code&gt;limitRate(highTide, lowTide)&lt;/code&gt;。&lt;code&gt;lowTide&lt;/code&gt; 参数设定为 &lt;code&gt;0&lt;/code&gt; 时，会导致严格限制一次请求 &lt;code&gt;highTide&lt;/code&gt; 个，而不是经填补策略进一步调整过的一次请求量。&lt;/p&gt;
&lt;p&gt;此外，&lt;code&gt;limitRequest(N)&lt;/code&gt; 则是限制了下游最大的需求总量。它会累加请求量直到 &lt;code&gt;N&lt;/code&gt;。如果一次请求没有让需求总量超过 &lt;code&gt;N&lt;/code&gt;，则这次请求会完整地传播到上游（译注：意思是如果一次请求让需求总量超过了 &lt;code&gt;N&lt;/code&gt;，这次请求的请求量会被裁剪）。如果数据源发出的数据总量达到了限制的总量，&lt;code&gt;limitRequest&lt;/code&gt; 则认为这个序列可以结束了，向下游发送一个 &lt;code&gt;onComplete&lt;/code&gt; 信号，并取消数据源。&lt;/p&gt;
&lt;h3&gt;3.4 动态地（programmatically）创建一个序列&lt;/h3&gt;
&lt;h4&gt;3.4.1 同步的 generate&lt;/h4&gt;
&lt;p&gt;动态创建一个 &lt;code&gt;Flux&lt;/code&gt; 最简单的方式是借助 &lt;code&gt;generate&lt;/code&gt; 方法，该方法接受一个生成器函数。&lt;/p&gt;
&lt;p&gt;这一方式可以实现&lt;strong&gt;同步的&lt;/strong&gt;且&lt;strong&gt;一个接一个&lt;/strong&gt;地下发数据，这意味着接收方（sink）是一个 &lt;code&gt;SynchronousSink&lt;/code&gt;，其 &lt;code&gt;next()&lt;/code&gt; 方法在一次回调方法调用中最多只能调用一次。可以在其后再调用 &lt;code&gt;error(Throwable)&lt;/code&gt; 或 &lt;code&gt;complete()&lt;/code&gt;，视你的需求而定。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;generate&lt;/code&gt; 方法变种的这个应该是最有用的：允许保持一个状态，在调用接收方的 &lt;code&gt;next&lt;/code&gt; 方法时可以基于这个状态来决定下发什么数据。那么这个生成器函数就成了一个 &lt;code&gt;BiFunction&amp;lt;S, SynchronousSink&amp;lt;T&amp;gt;, S&amp;gt;&lt;/code&gt; 实例，其中 &lt;code&gt;&amp;lt;S&amp;gt;&lt;/code&gt; 即是状态对象的类型。对于初始状态，可以提供一个 &lt;code&gt;Supplier&amp;lt;S&amp;gt;&lt;/code&gt; 来获取，这样生成器函数每轮调用都会返回一个新的状态。&lt;/p&gt;
&lt;p&gt;例如，可以使用一个 &lt;code&gt;int&lt;/code&gt; 实例作为状态：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;// 基于状态的 generate 方法使用示例
Flux&amp;amp;lt;String&amp;amp;gt; flux = Flux.generate(
    () -&amp;amp;gt; 0, // 1
    (state, sink) -&amp;amp;gt; {
        sink.next(&amp;amp;quot;3 x &amp;amp;quot; + state + &amp;amp;quot; = &amp;amp;quot; + 3*state); // 2
        if (state == 10) sink.complete(); // 3
        return state + 1; // 4
    });&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;以 0 作为初始状态。&lt;/li&gt;
&lt;li&gt;基于状态（state）决定下发什么消息/数据。&lt;/li&gt;
&lt;li&gt;基于状态决定何时可以停止流/序列。&lt;/li&gt;
&lt;li&gt;返回一个新状态，下次调用时可以使用（除非在这次调用时已经终止序列）。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;也可以使用一个 &lt;code&gt;&amp;lt;S&amp;gt;&lt;/code&gt; 类型的可变对象。比如，上面的示例可以使用一个 &lt;code&gt;AtomicLong&lt;/code&gt; 实例作为状态来重写，每轮调用都会改变它的值：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; flux = Flux.generate(
    AtomicLong::new,
    (state, sink) -&amp;amp;gt; {
        long i = state.getAndIncrement();
        sink.next(&amp;amp;quot;3 x &amp;amp;quot; + i + &amp;amp;quot; = &amp;amp;quot; + 3*i);
        if (i == 10) sink.complete();
        return state;
    });&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;&lt;p&gt;如果状态对象在序列终止时需要清理一些资源，则应该使用 &lt;code&gt;generate(Supplier&amp;lt;S&amp;gt;, BiFunction, Consumer&amp;lt;S&amp;gt;)&lt;/code&gt; 变种方法来清理最后的状态实例。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;如下示例使用的 &lt;code&gt;generate&lt;/code&gt; 方法接受一个 &lt;code&gt;Consumer&lt;/code&gt; 类型参数：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; flux = Flux.generate(
    AtomicLong::new,
    (state, sink) -&amp;amp;gt; {
        long i = state.getAndIncrement();
        sink.next(&amp;amp;quot;3 x &amp;amp;quot; + i + &amp;amp;quot; = &amp;amp;quot; + 3*i);
        if (i == 10) sink.complete();
        return state;
    }, (state) -&amp;amp;gt; System.out.println(&amp;amp;quot;state: &amp;amp;quot; + state));&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;3.4.2 异步多线程的 create&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;create&lt;/code&gt; 是动态创建一个 &lt;code&gt;Flux&lt;/code&gt; 的更高级的方式，适用于每轮下发多个数据，甚至是从多个线程中下发数据。&lt;/p&gt;
&lt;p&gt;这个方法会向回调方法传入一个 &lt;code&gt;FluxSink&lt;/code&gt; 实例参数，在回调方法体中可以调用这个参数的 &lt;code&gt;next&lt;/code&gt;、&lt;code&gt;error&lt;/code&gt; 和 &lt;code&gt;complete&lt;/code&gt; 方法。与 &lt;code&gt;generate&lt;/code&gt; 不同，它没有基于状态的变种方法。另外，回调方法中，可以多线程地触发事件（trigger multi-threaded events）。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt;create&lt;/code&gt; 非常适用于将一个已有的 API （比如：一个基于监听器的异步 API）桥接到反应式上下文中。&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt;create&lt;/code&gt; 并不会自动并行化执行你的代码，也不会让处理过程自动变成异步的，即使它可以配合异步 API 使用。如果在 &lt;code&gt;create&lt;/code&gt; 的 lambda 表达式中发生阻塞，就会存在死锁或者其它副作用的风险。即使借助 &lt;code&gt;subscribeOn&lt;/code&gt;，也要当心 &lt;code&gt;create&lt;/code&gt; lambda 表达式中长时间的阻塞（比如无限循环调用 &lt;code&gt;sink.next(t)&lt;/code&gt;）锁住流水线处理： （译注：异步的）数据请求可能根本得不到执行，因为（译注：线程池只有一个线程）同一个线程一直被无限循环占用着。使用 &lt;code&gt;subscribeOn(Scheduler, false)&lt;/code&gt; 变种方法：&lt;code&gt;requestOnSeparateThread = false&lt;/code&gt; 将使用 &lt;code&gt;Scheduler&lt;/code&gt; 的线程来执行 &lt;code&gt;create&lt;/code&gt; 方法的回调，在原始的线程中执行 &lt;code&gt;request&lt;/code&gt;，从而让数据仍然可以流动起来。（译注：此处逻辑有点绕，也可能是因为 subscribeOn 方法本身语义就不太直观）。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;假设我们要使用一个基于监听器的 API，它按块处理数据，提供两类事件：（1）来了一块数据，（2）处理可以结束了（终止事件），如下 &lt;code&gt;MyEventListener&lt;/code&gt; 接口定义所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;interface MyEventListener&amp;amp;lt;T&amp;amp;gt; {
    void onDataChunk(List&amp;amp;lt;T&amp;amp;gt; chunk);
    void processComplete();
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们使用 &lt;code&gt;create&lt;/code&gt; 将这个 API 桥接到一个 &lt;code&gt;Flux&amp;lt;T&amp;gt;&lt;/code&gt; 实例上：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; bridge = Flux.create(sink -&amp;amp;gt; {
    myEventProcessor.register(  // 4
        new MyEventListener&amp;amp;lt;String&amp;amp;gt;() { // 1
            public void onDataChunk(List&amp;amp;lt;String&amp;amp;gt; chunk) {
                for(String s : chunk) {
                    sink.next(s); // 2
                }
            }
            
            public void processComplete() {
                sink.complete(); // 3
            }
        }
    );
});&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;桥接到 &lt;code&gt;MyEventListener&lt;/code&gt; API&lt;/li&gt;
&lt;li&gt;数据块中每个元素都成了 &lt;code&gt;Flux&lt;/code&gt; 中的元素。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;processComplete&lt;/code&gt; 事件转换成了 &lt;code&gt;onComplete&lt;/code&gt; 事件。&lt;/li&gt;
&lt;li&gt;所有这些逻辑都是在 &lt;code&gt;myEventProcessor&lt;/code&gt; 执行时异步完成的。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;此外，因为 &lt;code&gt;create&lt;/code&gt; 可以桥接异步 API，并管理反压，通过指定一个 &lt;code&gt;OverflowStrategy&lt;/code&gt; 策略，可以调整如何智能地处理反压：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;IGNORE&lt;/code&gt; 完全忽略下游的反压请求。这一策略在下游的队列满时（when queues get full downstream）会导致 &lt;code&gt;IllegalStateException&lt;/code&gt; 异常抛出。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ERROR&lt;/code&gt; 在下游处理不过来时会下发（onError）一个 &lt;code&gt;IllegalStateException&lt;/code&gt; 异常消息。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DROP&lt;/code&gt; 如果下游还没准备好接收当前事件，则直接丢弃。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BUFFER&lt;/code&gt; （默认策略）如果下游处理不过来，则将所有事件放入缓冲区。（缓冲区大小无限制，所以可能会导致内存溢出&lt;code&gt;OutOfMemoryError&lt;/code&gt;）&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt;Mono&lt;/code&gt; 也有一个 &lt;code&gt;create&lt;/code&gt; 生成器方法。Mono 的 create 方法传入回调的 &lt;code&gt;MonoSink&lt;/code&gt; 参数不允许下发多个消息，在第一个消息之后它会丢弃所有的消息。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4&gt;3.4.3 异步单线程的 push&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;push&lt;/code&gt; 的功能介于 &lt;code&gt;generate&lt;/code&gt; 和 &lt;code&gt;create&lt;/code&gt; 之间，适用于处理来自单个生产者的事件。&lt;code&gt;push&lt;/code&gt; 也可以是异步的，也可以使用 &lt;code&gt;create&lt;/code&gt; 支持的超限策略来管理反压，然而，同时（at a time）只能有一个生产线程调用 &lt;code&gt;next&lt;/code&gt;。&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; bridge = Flux.push(sink -&amp;amp;gt; {
    myEventProcessor.register(
        new SingleThreadEventListener&amp;amp;lt;String&amp;amp;gt;() { // 1
            
            public void onDataChunk(List&amp;amp;lt;String&amp;amp;gt; chunk) {
                for (String s: chunk) {
                    sink.next(s); // 2
                }
            }
            
            public void processComplete() {
                sink.complete(); // 3
            }
            
            public void processError(Throwable e) {
                sink.error(e); // 4
            }
        }
    );
});&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;桥接到 &lt;code&gt;SingleThreadEventListener&lt;/code&gt; 的 API。&lt;/li&gt;
&lt;li&gt;在单个监听器线程中使用 &lt;code&gt;next&lt;/code&gt; 向下游（sink - 接收方）推送事件。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;complete&lt;/code&gt; 事件也是由同一个监听器线程发出的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;error&lt;/code&gt; 事件也是由同一个监听器线程发出的。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;推/拉 混合模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;多数 Reactor 算子，比如 &lt;code&gt;create&lt;/code&gt;，都遵从 &lt;strong&gt;推/拉（push/pull）&lt;/strong&gt; 混合模型。这意味着尽管大部分的处理过程都是异步的（暗指“推”的方式），也存在小部分逻辑是 &lt;em&gt;拉（pull）&lt;/em&gt;方式：数据请求。&lt;/p&gt;
&lt;p&gt;消费者从数据源&lt;em&gt;拉取&lt;/em&gt;数据，意指：数据源在消费者首次请求后才会发出数据，然后只有要数据就会推送给消费者，不过数据量不会超过消费者请求的量。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;push()&lt;/code&gt; 和 &lt;code&gt;create()&lt;/code&gt; 都可以配置（set up）一个 &lt;code&gt;onRequest&lt;/code&gt; 事件消费者来管理请求量，并且确保仅当存在已发起的请求，数据才会推送给下游。&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux&amp;amp;lt;String&amp;amp;gt; bridge = Flux.create(sink -&amp;amp;gt; {
    myMessageProcessor.register(
        new MyMessageListener&amp;amp;lt;String&amp;amp;gt;() {
            
            public void onMessage(List&amp;amp;lt;String&amp;amp;gt; messages) {
                for (String s: messages) {
                    sink.next(s); // 3
                }
            }
        }
    );
    sink.onRequest(n -&amp;amp;gt; {
        List&amp;amp;lt;String&amp;amp;gt; messages = myMessageProcessor.getHistory(n); // 1
        for (String s: messages) {
            sink.next(s); // 2
        }
    });    
});&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;译者注：上面这个示例有点问题，实际并不存在这样一个 create 方法，并且 sink.onRequest 实际代表一个无限量（n = Long.MAX_VALUE）的请求。&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;在请求发起后，拉取消息。&lt;/li&gt;
&lt;li&gt;如果即刻有消息了，则推送给下游。&lt;/li&gt;
&lt;li&gt;后续异步到达的消息也会推送给下游。&lt;/li&gt;&lt;/ol&gt;
&lt;h3&gt;3.5 多线程 和 调度器 （Threading and Schedulers）&lt;/h3&gt;
&lt;p&gt;Reactor，与 RxJava 类似，可以认为是&lt;strong&gt;并发无关的&lt;/strong&gt;，也就是说，Reactor 并不强制使用并发（a concurrency&lt;/p&gt;
&lt;p&gt;model），而是，让开发者按需决定是否使用并发。然而，Reactor 也提供一些功能方便开启并发。&lt;/p&gt;
&lt;p&gt;获取到一个 &lt;code&gt;Flux&lt;/code&gt; 或 &lt;code&gt;Mono&lt;/code&gt; 处理流，并不意味着它在一个专用（dedicated）的线程（&lt;code&gt;Thread&lt;/code&gt;） 中运行。相反，多数算子也是运行在前一个算子运行的线程中。除非特意指定，首个（topmost）算子（数据源）就运行在执行 &lt;code&gt;subscribe()&lt;/code&gt; 方法调用的线程中。如下示例在一个新建线程中运行一个 &lt;code&gt;Mono&lt;/code&gt; 处理流。&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;public static void main(String[] args) {
    final Mono&amp;amp;lt;String&amp;amp;gt; mono = Mono.just(&amp;amp;quot;Hello &amp;amp;quot;); // 1
    
    new Thread(() -&amp;amp;gt; mono
        .map(msg -&amp;amp;gt; msg + &amp;amp;quot;thread &amp;amp;quot;)
        .subscribe(v -&amp;amp;gt; // 2 
            System.out.println(v + Thread.currentThread().getName()) // 3
        )
    ).join();
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;&lt;code&gt;Mono&amp;lt;String&amp;gt;&lt;/code&gt; 是在主（&lt;code&gt;main&lt;/code&gt;）线程中装配的（assembled）。&lt;/li&gt;
&lt;li&gt;然而， 订阅操作发生在 &lt;code&gt;Thread-0&lt;/code&gt; 线程中。&lt;/li&gt;
&lt;li&gt;因而，&lt;code&gt;map&lt;/code&gt; 和 &lt;code&gt;onNext&lt;/code&gt; 的回调（译注：&lt;code&gt;onNext&lt;/code&gt; 的回调即 subscribe 方法传入的 lambda 表达式）实际上也是在 &lt;code&gt;Thread-0&lt;/code&gt; 上执行。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;上述的代码会输出如下内容：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;hello thread Thread-0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reactor 中，运行模型以及实际的运行过程发生在什么地方由使用什么 &lt;code&gt;Scheduler&lt;/code&gt; 决定。&lt;a href=&apos;https://projectreactor.io/docs/core/release/api/reactor/core/scheduler/Scheduler.html&apos;&gt;Scheduler&lt;/a&gt; 类似于 &lt;code&gt;ExecutorService&lt;/code&gt;，负有调度职责，但具备一个专用的抽象，功能更强大，充当一个时钟的角色，可用的实现更多。&lt;/p&gt;
&lt;p&gt;&lt;a href=&apos;https://projectreactor.io/docs/core/release/api/reactor/core/scheduler/Schedulers.html&apos;&gt;Schedulers&lt;/a&gt; 类提供了一些静态方法来访问这些运行上下文：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;当前线程（&lt;code&gt;Schedulers.immediate()&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;单个可复用的线程（&lt;code&gt;Schedulers.single()&lt;/code&gt;）。注意：这个方法会为所有调用方（译注：调用 Schedulers.single()）复用同一个线程，指导 &lt;code&gt;Scheduler&lt;/code&gt; 销毁（disposed）。如果期望每次调用返回一个专用线程，则应该使用 &lt;code&gt;Schedulers.newSingle()&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;一个弹性的线程池（&lt;code&gt;Schedulers.elastic()&lt;/code&gt;）。这个 Scheduler 会按需创建新的工作者线程池（worker pool），并复用空闲的工作者线程池。如果工作者线程池空闲时间太长（默认 60s）则会被销毁。对于 I/O 阻塞工作而言这是一个好选择。&lt;code&gt;Schedulers.elastic()&lt;/code&gt; 可以简便地为阻塞处理过程提供独立的线程（its own thread），这样阻塞操作就不会占用（tie up）其他资源。详情请参考 &lt;a href=&apos;https://projectreactor.io/docs/core/release/reference/#faq.wrap-blocking&apos;&gt;如何包装一个同步阻塞的调用？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;固定数量工作者的（译注：我暂时的理解 - 工作者（worker）也是一个线程池）一个池，专门为并行处理工作做过调优（Schedulers.parallel()）。它会创建和 CPU 核心数量相同的工作者。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;此外，也可以使用 &lt;code&gt;Schedulers.fromExecutorService(ExecutorService)&lt;/code&gt; 基于已有的 ExecutorService 创建一个 Scheduler。（也可以基于一个 Executor 来创建，但不建议这么干（译注：因为 Executor 不能销毁释放））&lt;/p&gt;
&lt;p&gt;也可以使用 &lt;strong&gt;newXXX&lt;/strong&gt; 这类方法创建各种调度器（scheduler）类型的全新实例。例如，使用 &lt;code&gt;Schedulers.newElastic(yourScheduleName)&lt;/code&gt; 创建一个名为 &lt;code&gt;yourScheduleName&lt;/code&gt; 的全新的弹性调度器（elastic scheduler）。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt;elastic&lt;/code&gt; 调度器用于兼容处理不可避免的历史遗留的阻塞性代码，但 &lt;code&gt;single&lt;/code&gt; 和 &lt;code&gt;parallel&lt;/code&gt; 调度器不行，因而，如果在 &lt;code&gt;single&lt;/code&gt; 或 &lt;code&gt;parallel&lt;/code&gt; 调度器上使用 Reactor 的阻塞性 API（&lt;code&gt;block()&lt;/code&gt;、&lt;code&gt;blockFirst()&lt;/code&gt;、&lt;code&gt;blockLast()&lt;/code&gt;，或者进行 &lt;code&gt;toIterable()&lt;/code&gt; 或 &lt;code&gt;toStream()&lt;/code&gt; 迭代），会导致抛出 &lt;code&gt;IllegalStateException&lt;/code&gt; 异常。&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;如果自定义调度器所创建的线程实例实现了 &lt;code&gt;NonBlocking&lt;/code&gt; 标记性接口（marker interface），那么这个调度器也可以被标记为”仅适用于非阻塞性使用（non blocking only）“。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;某些算子默认会从 &lt;code&gt;Schedulers&lt;/code&gt; 选择一个特定的调度器来使用（通常也支持选择其他的）。例如，调用工厂方法 &lt;code&gt;Flux.interval(Duration.ofMills(300))&lt;/code&gt; 会生成一个 &lt;code&gt;Flux&amp;lt;Long&amp;gt;&lt;/code&gt; 实例 - 每 300 ms 输出一个滴答事件。这个方法底层实现默认使用 &lt;code&gt;Schedulers.parallel()&lt;/code&gt;。如下代码行演示了如何将调度器修改成类似于 &lt;code&gt;Schedulers.single()&lt;/code&gt; 的调度器新实例：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux.interval(Duration.ofMillis(300), Schedulers.newSingle(&amp;amp;quot;test&amp;amp;quot;));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reactor 提供了两种方式来切换反应式链中的执行上下文（或者说 &lt;code&gt;调度器&lt;/code&gt;）：&lt;code&gt;publishOn&lt;/code&gt; 和 &lt;code&gt;subscribeOn&lt;/code&gt;。两者都是接受一个 &lt;code&gt;Scheduler&lt;/code&gt; 类型参数并将执行上下文切换到这个调度器。不过，链中 &lt;code&gt;publishOn&lt;/code&gt; 所处的位置很关键，而 &lt;code&gt;subscribeOn&lt;/code&gt; 处于哪个位置都无所谓。要理解这个差别的原因，得先理解 &lt;a href=&apos;https://projectreactor.io/docs/core/release/reference/#reactive.subscribe&apos;&gt;订阅之前实际什么都没有发生&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Reactor，串接算子，就是将很多 &lt;code&gt;Flux&lt;/code&gt; 和 &lt;code&gt;Mono&lt;/code&gt; 的实现一个套一个，逐层封装。一旦订阅，就创建了一个 &lt;code&gt;Subscriber&lt;/code&gt; 对象链，沿链回溯即可找到第一个发布者。这些实现细节是隐藏在接口背后，开发者可见的是最外层的那个 &lt;code&gt;Flux&lt;/code&gt;（或 &lt;code&gt;Mono&lt;/code&gt;）以及 &lt;code&gt;Subscription&lt;/code&gt;（译注：Reactor 中 Subscription 是一个接口类型，是 &lt;code&gt;Subscriber&lt;/code&gt; 接口中 &lt;code&gt;onSubscribe&lt;/code&gt; 方法参数的类型 - &lt;code&gt;public void onSubscribe(Subscription s)&lt;/code&gt;，用于向生产者请求数据 或者 取消订阅），但这些算子特定的链中消费者是幕后功臣。&lt;/p&gt;
&lt;p&gt;有了上面这些认知，现在我们可以进一步了解 &lt;code&gt;publishOn&lt;/code&gt; 和 &lt;code&gt;subscribeOn&lt;/code&gt; 这两个算子：&lt;/p&gt;
&lt;h4&gt;3.5.1 publishOn 方法&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;publishOn&lt;/code&gt; 和其他算子的用法一样，用在订阅链的中间环节，接收来自上游的信号，然后向下游重放这些信号，不过下发事件回调（&lt;code&gt;onEvent&lt;/code&gt;、&lt;code&gt;onError&lt;/code&gt;、&lt;code&gt;onComplete&lt;/code&gt;）是在关联 &lt;code&gt;Scheduler&lt;/code&gt; 的一个工作者上执行的。因此，这个算子会影响后续算子在哪执行（直到订阅链上又串接了另一个 &lt;code&gt;publishOn&lt;/code&gt;）：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;将执行上下文切换到 &lt;code&gt;Scheduler&lt;/code&gt; 选择的一个线程上&lt;/li&gt;
&lt;li&gt;根据规范（as per the specification），&lt;code&gt;onNext&lt;/code&gt; 是按时序依次调用下发事件的，所以是占用一个线程（译注：这句不太理解，onNext happen in sequence, so this uses up a single thread）&lt;/li&gt;
&lt;li&gt;除非算子工作在一个特定的 &lt;code&gt;Scheduler&lt;/code&gt; 上（译注：某些算子的内部实现决定了这一点），&lt;code&gt;publishOn&lt;/code&gt; 之后的算子都是在同一个线程上执行&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Scheduler s = Schedulers.newParallel(&amp;amp;quot;parallel-scheduler&amp;amp;quot;, 4); // 1

final Flux&amp;amp;lt;String&amp;amp;gt; flux = Flux
    .range(1, 2)
    .map(i -&amp;amp;gt; 10 + i) // 2
    .publishOn(s) // 3
    .map(i -&amp;amp;gt; &amp;amp;quot;value &amp;amp;quot; + i); // 4

new Thread(() -&amp;amp;gt; flux.subscribe(System.out::println));&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;创建一个新的 &lt;code&gt;Scheduler&lt;/code&gt;，内含 4 个线程&lt;/li&gt;
&lt;li&gt;第一个 &lt;code&gt;map&lt;/code&gt; 运行在 &lt;第5步&gt; 的匿名线程上&lt;/li&gt;
&lt;li&gt;&lt;code&gt;publishOn&lt;/code&gt; 将整个序列的后续处理切换到从 &lt;第1步&gt; 选出的线程上&lt;/li&gt;
&lt;li&gt;第二个 &lt;code&gt;map&lt;/code&gt; 运行在上面说的从 &lt;第1步&gt; 选出的线程上&lt;/li&gt;
&lt;li&gt;这个匿名线程是 &lt;em&gt;订阅&lt;/em&gt; 操作发生的地方。打印语句发生在 &lt;code&gt;publishOn&lt;/code&gt; 切换的最新执行上下文上&lt;/li&gt;&lt;/ol&gt;
&lt;h4&gt;3.5.2 subscribeOn 方法&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;subscribeOn&lt;/code&gt; 在构造反向链时应用于订阅处理过程（译注：所谓构造反向链时，是指调用 subscribe 方法时）。因此，无论你将 &lt;code&gt;subscribeOn&lt;/code&gt; 放在算子链的何处，&lt;strong&gt;它始终会影响源头下发数据的执行上下文&lt;/strong&gt;。然而，这并不会影响 &lt;code&gt;publishOn&lt;/code&gt; 之后算子调用的行为，它们仍然会切换到 &lt;code&gt;publishOn&lt;/code&gt; 指定的执行上下文。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;从订阅操作发生时整个算子链所在的线程切换到新的线程&lt;/li&gt;
&lt;li&gt;从指定 &lt;code&gt;Scheduler&lt;/code&gt; 中选择一个线程&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;只有链中最早的 &lt;code&gt;subscribeOn&lt;/code&gt; 调用会发生实际作用。&lt;/p&gt;&lt;/blockquote&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Scheduler s = Schedulers.newParallel(&amp;amp;quot;parallel-scheduler&amp;amp;quot;, 4); // 1

final Flux&amp;amp;lt;String&amp;amp;gt; flux = Flux
    .range(1, 2)
    .map(i -&amp;amp;gt; 10 + i) // 2
    .subscribeOn(s) // 3
    .map(i -&amp;amp;gt; &amp;amp;quot;value &amp;amp;quot; + i); // 4

new Thread(() -&amp;amp;gt; flux.subscribe(System.out::println)); // 5 &lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;创建一个新的 &lt;code&gt;Scheduler&lt;/code&gt;，内含 4 个线程&lt;/li&gt;
&lt;li&gt;第一个 &lt;code&gt;map&lt;/code&gt; 运行在这 4 个线程中的某个线程上&lt;/li&gt;
&lt;li&gt;...因为 &lt;code&gt;subscribeOn&lt;/code&gt; 将整个序列处理链从订阅操作发生时的执行上下文（第5步）切换到了新的上下文&lt;/li&gt;
&lt;li&gt;第二个 &lt;code&gt;map&lt;/code&gt; 和第一个 &lt;code&gt;map&lt;/code&gt; 运行在同一个线程上&lt;/li&gt;
&lt;li&gt;这个匿名线程是 &lt;em&gt;订阅操作&lt;/em&gt; 一开始发生的地方的，但是 &lt;code&gt;subscribeOn&lt;/code&gt; 即刻将上下文切换到调度器4个线程中的一个上&lt;/li&gt;&lt;/ol&gt;
&lt;h2&gt;4. 高级特性和概念&lt;/h2&gt;
&lt;h3&gt;4.1 使用 ConnectableFlux 将消息广播到多个订阅者&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;以后有空再翻译&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;4.2 3种分批处理方式&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;以后有空再翻译&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;4.3 使用 ParallelFlux 并行化处理&lt;/h3&gt;
&lt;p&gt;如今多核架构已是下里巴人，相应地，轻松实现并行化工作的工具手段很关键。Reactor 提供了一个特殊类型 - &lt;code&gt;ParallelFlux&lt;/code&gt; - 帮助实现并行化处理。&lt;code&gt;ParallelFlux&lt;/code&gt; 提供的算子是为并行化工作优化过的。&lt;/p&gt;
&lt;p&gt;对任意 &lt;code&gt;Flux&lt;/code&gt; 实例调用 &lt;code&gt;parallel()&lt;/code&gt;算子就能得到一个 &lt;code&gt;ParallelFlux&lt;/code&gt; 实例。这个方法本身并不能实现并行化工作，而是将工作负载拆分到多个“轨道”（默认“轨道”数量等于 CPU 核数）[^1]。&lt;/p&gt;
&lt;p&gt;为了告知产出的 ParallelFlux 实例每个“轨道”在哪执行（以及如何并行执行“轨道”），则必须使用 &lt;code&gt;runOn(Scheduler)&lt;/code&gt;。注意：对于并行工作，推荐使用一个专用调度器 - &lt;code&gt;Schedulers.parallel()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;对比如下两个示例，第一个示例的代码如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux.range(1, 10)
    .parallel(2) // 1
    .subscribe(i -&amp;amp;gt; System.out.println(Thread.currentThread().getName() + &amp;amp;quot; -&amp;amp;gt; &amp;amp;quot; + i));&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;&lt;li&gt;这里强制指定了“轨道”数量，而不依赖于 CPU 核数。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;第二个示例的代码如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;Flux.range(1, 10)
    .parallel(2)
    .runOn(Schedulers.parallel())
    .subscribe(i -&amp;amp;gt; System.out.println(Thread.currentThread().getName() + &amp;amp;quot; -&amp;amp;gt; &amp;amp;quot; + i));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一个示例输出如下内容：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;main -&amp;amp;gt; 1
main -&amp;amp;gt; 2
main -&amp;amp;gt; 3
main -&amp;amp;gt; 4
main -&amp;amp;gt; 5
main -&amp;amp;gt; 6
main -&amp;amp;gt; 7
main -&amp;amp;gt; 8
main -&amp;amp;gt; 9
main -&amp;amp;gt; 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第二个示例正确地在两个线程上实现了并行化，输入如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;parallel-1 -&amp;amp;gt; 1
parallel-2 -&amp;amp;gt; 2
parallel-1 -&amp;amp;gt; 3
parallel-2 -&amp;amp;gt; 4
parallel-1 -&amp;amp;gt; 5
parallel-2 -&amp;amp;gt; 6
parallel-1 -&amp;amp;gt; 7
parallel-1 -&amp;amp;gt; 9
parallel-2 -&amp;amp;gt; 8
parallel-2 -&amp;amp;gt; 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果数据序列[^2]已经在并行化处理，而你又想将其转回一个 “常规的” &lt;code&gt;Flux&lt;/code&gt; 实例，然后串行执行算子链余下的部分，则可以使用 &lt;code&gt;ParallelFlux&lt;/code&gt; 的 &lt;code&gt;sequential()&lt;/code&gt; 方法。&lt;/p&gt;
&lt;p&gt;注意：如果直接使用一个 &lt;code&gt;Subscriber&lt;/code&gt; 类型参数而不是 lambda 表达式来调用 &lt;code&gt;subscribe&lt;/code&gt; 方法，那么内部实现会隐式地调用 &lt;code&gt;sequential()&lt;/code&gt; 方法。&lt;/p&gt;
&lt;p&gt;由此也要注意：&lt;code&gt;subscribe(Subscriber&amp;lt;T&amp;gt;)&lt;/code&gt; 会合并所有数据“轨道”，而 &lt;code&gt;subscribe(Consumer&amp;lt;T&amp;gt;)&lt;/code&gt; 是运行所有的数据“轨道”。如果以 lambda 表达式调用 &lt;code&gt;subscribe&lt;/code&gt; 方法，那么每个 lambda 表达式都会被复制成多个实例（数量等于“轨道”数量）去执行[^3]。&lt;/p&gt;
&lt;p&gt;[^1]: 译注：这里的“轨道”其实不太直白。在实现上，&lt;code&gt;ParallelFlux&lt;/code&gt; 会将最后 &lt;code&gt;subscribe&lt;/code&gt; 的 onNext 回调按并行度（默认等于 CPU 核数 N）复制成 N 个，那么最终调用 ParallelFlux 的 N 个 Subscriber，从 ParallelFlux 实例到一个 Subscriber 的数据流路径可以理解为一个“轨道”，ParallelFlux 在接收到上游消息后按照 round-robin 方式选择一个 Subscriber 调用其 &lt;code&gt;onNext&lt;/code&gt; 下发消息，但 &lt;code&gt;onNext&lt;/code&gt; 是运行在什么线程上，是由 runOn 算子决定的，如果不使用 runOn 算子，那么所有 Subscriber 的 &lt;code&gt;onNext&lt;/code&gt; 方法调用都是同步运行在主线程上的。&lt;/p&gt;
&lt;p&gt;[^2]: 译注：原文中用了多个词来表达相近的意思：sequence（序列）、stream（流）、flow（流），阅读时可以相互替代理解。此外，还有 event（事件）、data（数据）、message（消息），在当前上下文中，可以看成是等价的。&lt;/p&gt;
&lt;p&gt;[^3]: 译注：这话写得真蠢。详细解释见脚注 1。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Java 单测伴侣 - mockito</title>
            <description>&lt;p&gt;其实工作以来，我很少写测试/单测代码，一方面是大部分互联网公司团队对测试的要求不高，另一方面是想写好测试代码还挺难的，挺花时间，其中最麻烦的是待测代码可能会访问外部资源（比如数据库、HTTP API），如果不能方便地进模拟访问这些外部资源，那么测试起来会非常麻烦。&lt;/p&gt;
&lt;p&gt;但，对于复杂逻辑，如果不经过严格测试，发布到生产环境，又有些不放心，没底气，或者在代码重构时，如果没有覆盖全面的测试，很难评估代码变动带来的影响。&lt;/p&gt;
&lt;p&gt;直到遇到 &lt;a href=&apos;https://site.mockito.org/&apos;&gt;mockito&lt;/a&gt;，我才觉得是时候认真写写测试代码了。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&apos;https://site.mockito.org/&apos;&gt;mockito&lt;/a&gt; 提供两种对象模拟方式：&lt;strong&gt;mock&lt;/strong&gt; 和 &lt;strong&gt;spy&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;简单来说，mock 模拟的对象是一个完全假的对象，只是具备指定类型的接口，以 &lt;code&gt;java.util.List&lt;/code&gt; 为例：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.Mockito.mock;

List mockedList = mock(List.class);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;虽然 List 是一个 interface，也可以模拟出一个对象实例，这个 mockedList 对象具备 List 接口定义的所有方法，但所有方法都不具备实际的行为操作，对于有返回值的方法，则默认返回方法返回类型的默认值，没有返回值的方法，则纯粹是一个空方法。比如：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;// mockedList 并不会真的把 1 存下来
mockedList.add(1);
// 所以，size() 返回默认值，输出 0
System.out.println(mockedList.size());
// 输出 null
System.out.println(mockedList.get(0));
// 输出 null
System.out.println(mockedList.get(1));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于模拟出来的对象，可以任意指定其方法的返回值，比如：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.Mockito.when;

// 调用 size() 方法时，返回 10
when(mockedList.size()).willReturn(10);
when(mockedList.get(0)).willReturn(&amp;amp;quot;Hello World!&amp;amp;quot;);
when(mockedList.get(1)).thenReturn(&amp;amp;quot;您好！&amp;amp;quot;);

// 输出 10
System.out.println(mockedList.size());
// 输出 Hello World!
System.out.println(mockedList.get(0));
// 输出 您好！
System.out.println(mockedList.get(1));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当然我们写测试代码时，并不会使用 System.out.println，然后看输出，而是使用&lt;strong&gt;断言&lt;/strong&gt;：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.junit.Assert.assertEquals;

assertEquals(10, mockedList.size());
assertEquals(&amp;amp;quot;Hello World!&amp;amp;quot;, mockedList.get(0));
assertEquals(&amp;amp;quot;您好！&amp;amp;quot;, mockedList.get(1));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;断言方法非常多，不仅仅只是 assertEquals。&lt;/p&gt;
&lt;p&gt;对于同一个方法，可以模拟多次调用返回不同的值：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;// 会覆盖之前 mock 的行为：when(mockedList.size()).willReturn(10);
// 或者这么写：when(mockedList.size()).willReturn(0, -1, 10);
when(mockedList.size()).thenReturn(0).thenReturn(-1).thenReturn(10);
assertEquals(0, mockedList.size());
assertEquals(-1, mockedList.size());
assertEquals(10, mockedList.size());
// 第 3 次之后的 mockedList.size() 调用都返回 10
assertEquals(10, mockedList.size());

Iterator iterator = mock(Iterator.class);
// 或者这么写：when(iterator.next()).thenReturn(0, 1, 10, 1000);
when(iterator.next()).thenReturn(0).thenReturn(1).thenReturn(10).thenReturn(1000);
assertEquals(0, iterator.next());
assertEquals(1, iterator.next());
assertEquals(10, iterator.next());
assertEquals(1000, iterator.next());
// 第 4 次之后的 iterator.next() 调用都返回 1000
assertEquals(1000, iterator.next());&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;还可以模拟异常抛出：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;List mockedList = mock(List.class);

when(mockedList.get(-1000)).thenThrow(new RuntimeException(&amp;amp;quot;参数异常！&amp;amp;quot;));
try {
    mockedList.get(-1000);
} catch (Exception e) {
    assertTrue(e instanceof RuntimeException);
    assertEquals(&amp;amp;quot;参数异常！&amp;amp;quot;, e.getMessage());
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也可以基于复杂的逻辑来构造返回值：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

List&amp;amp;lt;Integer&amp;amp;gt; mockedList = mock(List.class);
when(mockedList.get(anyInt())).thenAnswer(new EchoAnswer());

assertTrue(1 == mockedList.get(1));
assertTrue(10 == mockedList.get(10));

public class EchoAnswer implements Answer&amp;amp;lt;Integer&amp;amp;gt; {

    public Integer answer(InvocationOnMock var) {
        return var.getArgument(0);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;除了 &lt;code&gt;when(...).thenReturn(...)&lt;/code&gt; 风格的测试模拟方式，还有 BDD（Behavior Driven Development 行为驱动开发）风格的：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.BDDMockito.given;

// given
given(mockedList.get(0)).willReturn(100);
// when
int v = (int) mockedList.get(0);
// then
assertEquals(100, v);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果方法没有返回值，或者其它奇葩的需求，则没法使用 when.thenReturn / willReturn 这样的模拟方法，可以使用 &lt;code&gt;doReturn(...).when(...)...&lt;/code&gt;：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.Mockito.doThrow;
import static org.mockito.Mockito.doReturn;

ArrayList mockedList = mock(ArrayList.class);
// clear 方法无返回值
doThrow(new RuntimeException(&amp;amp;quot;清除失败&amp;amp;quot;)).when(mockedList).clear();

try {
    mockedList.clear();
} catch (Exception e) {
    assertTrue(e instanceof RuntimeException);
    assertEquals(&amp;amp;quot;清除失败&amp;amp;quot;, e.getMessage());
}

// 没有意义，因为没法使用 断言 来验证，实际运行时会抛异常
doReturn(10).when(mockedList).clear();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从示例代码可以看出，&lt;code&gt;doReturn(...).when(...)....&lt;/code&gt; 不会做类型校验，mockedList.clear() 返回值类型为 void，但我们模拟让其返回 10；所以，正常情况应该尽可能使用 &lt;code&gt;when(...).thenReturn(...)&lt;/code&gt; 或 &lt;code&gt;given(...).willReturn(...)&lt;/code&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;前述代码示例中，模拟方法的参数都做了硬编码，实际情况通常都不是这么测试，而是模拟方法的参数符合一定的要求即可，比如：在某个范围之内、符合类型的任何值：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.Mockito.anyInt;

/*
以任何 int 类型的参数调用 mockedList.get 方法，都返回 100

如果写成 when(mockedList.get(0)).thenReturn(100)，则只有以 0 为参数调用 mockedList.get 方法，才会返回100，其他参数值，返回的都是默认值 0
*/
when(mockedList.get(anyInt())).thenReturn(100);

assertEquals(100, mockedList.get(0));
assertEquals(100, mockedList.get(1000));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可用的参数匹配器，见 org.mockito.ArgumentMatchers 类的静态方法列表，也可以自己实现 ArgumentMatcher 接口：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;package org.mockito;

public interface ArgumentMatcher&amp;amp;lt;T&amp;amp;gt; {
    boolean matches(T var1);
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import org.mockito.ArgumentMatcher;
import static org.mockito.Mockito.intThat;

when(mockedList.get(intThat(new LimitedInt()))).thenReturn(10);

assertEquals(null, mockedList.get(-1));
assertEquals(10, mockedList.get(1));
assertEquals(10, mockedList.get(99));
assertEquals(null, mockedList.get(100));

public class LimitedInt implements ArgumentMatcher&amp;amp;lt;Integer&amp;amp;gt; {

    public boolean matches(Integer var) {
        return var &amp;amp;gt; 0 &amp;amp;amp;&amp;amp;amp; var &amp;amp;lt; 100;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果被模拟的方法包含多个参数，那么这些参数要么全部使用匹配器，要么全部不使用。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;模拟某些类（A）的方法，通常会将 mock 出来的对象注入到依赖该类实例的其他类（B）中，来替代真实的依赖，这种方式的目的是为了测试类 B 的行为是否符合预期。&lt;/p&gt;
&lt;p&gt;另一个测试需求是，测试某个类 A&apos; 在某个上下文环境中的行为是否符合预期，比如： A&apos; 的某个方法是否被调用过、调用过几次、调用参数是否符合预期、几个方法之间的调用次序是否符合预期、方法调用耗时是否符合预期等等。&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.times;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.verifyZeroInteractions;

List mocked = mock(List.class);

Caller caller = new Caller();
caller.setList(mocked);

// 调用 0 次
caller.run(0);
// 验证是否从来没调用过 mocked.size()
verify(mocked, never()).size();
// 验证 没有和 mocked 产生过任何交互
// 因为 Caller.run 中调用了 list.isEmpty()，实际产生了交互，所以这行测试会失败
verifyZeroInteractions(mocked);

// 调用 10 次
caller.run(10);
// 验证是否调用 mocked.size() 10 次
verify(mocked, times(10)).size();

// 再调用一次
caller.run(1);
// 所以是 11 次了
verify(mocked, times(11)).size();

@Data
public class Caller {
    List list;

    public void run(int count) {
        for (int idx=0; idx &amp;amp;lt; count; idx++) {
            list.size();
        }
        //
        list.isEmpty();
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;List mocked = mock(List.class);

mocked.add(1);
mocked.add(2);

verify(mocked).add(1);

// 是否有其他交互没有验证过？因为 mocked 还调用过 mocked.add(2)，所以这句测试会失败
verifyNoMoreInteractions(mocked);&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import org.mockito.InOrder;

// 也可以验证调用次序
List mocked1 = mock(List.class);
List mocked2 = mock(List.class);

mocked1.size();
mocked1.isEmpty();
mocked2.isEmpty();

// 会记录 mocked1、mocked2 中方法的调用/交互次序，要求：与 mocked1 的交互先于 mocked2
InOrder inOrder = inOrder(mocked1, mocked2);
// mocked1、mocked2 的交互顺序必须和 inOrder.verify 之间的顺序一致
inOrder.verify(mocked1).size();
inOrder.verify(mocked1).isEmpty();
inOrder.verify(mocked2).isEmpty();&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;也可以验证某个方法被调用时所使用的参数是否符合预期：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import org.mockito.ArgumentCaptor;

List mockedlist = mock(List.class);

Caller caller = new Caller();
caller.setList(mockedlist);
caller.run();

// 捕获 mockedList.add 的调用参数
ArgumentCaptor&amp;amp;lt;Integer&amp;amp;gt; argumentCaptor = ArgumentCaptor.forClass(Integer.class);
verify(mockedlist).add(argumentCaptor.capture());
assertTrue(100 == argumentCaptor.getValue());

@Data
public class Caller {
    List list;

    public void run() {
        list.add(100);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;前面的内容都是以 mock 为例，我们再来说说 spy，与 mock 的区别：&lt;/p&gt;
&lt;p&gt;mock 出来的对象是一个完全假的对象，但 spy 通常是基于一个具体的类或类实例，对其篡改某些方法，对于被篡改方法之外的方法，其行为都和调用真实对象的方法一样，不过并没有调用真实对象的方法，也不会对真实对象产生影响：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;// 基于一个实际的类实例
List&amp;amp;lt;Integer&amp;amp;gt; realList = new ArrayList&amp;amp;lt;&amp;amp;gt;(10);
List&amp;amp;lt;Integer&amp;amp;gt; spy = spy(realList);
        
spy.add(1);

// 被窃听的对象并没有发生变化
assertEquals(0, realList.size());
// 间谍对象确实将 1 存了下来
assertEquals(1, spy.size());
// 这句会抛出 java.lang.IndexOutOfBoundsException，因为 realList 还是为空
assertTrue(1 == realList.get(0));
assertTrue(1 == spy.get(0));&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;也可以基于一个具体的类来构造 spy，但这样无法使用带参数的构造方法，也无法指定类型参数：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;List&amp;amp;lt;Integer&amp;amp;gt; = spy(ArrayList.class);
assertEquals(0, spy.size());
spy.add(100);
assertEquals(1, spy.size());
assertTrue(100 == spy.get(0));

// 篡改方法
when(spy.size()).thenReturn(-1);
assertEquals(-1, spy.size());&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;实际上，mock 也可以基于具体的类来构造，这时可以指定某些方法实际调用具体类的方法。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;除了使用 mock、spy 方法来构造模拟对象，还可以通过注解来构造，但这样的话得指定 JUnit 的 Runner 为 &lt;code&gt;org.mockito.junit.MockitoJUnitRunner&lt;/code&gt;：&lt;/p&gt;
&lt;pre class=&quot;language-java&quot;&gt;&lt;code&gt;import org.junit.Test;
import org.junit.runner.RunWith;
import org.mockito.Mock;
import org.mockito.Spy;
import org.mockito.junit.MockitoJUnitRunner;

import java.util.ArrayList;
import java.util.List;

import static org.mockito.Mockito.when;
import static org.junit.Assert.assertTrue;

@RunWith(MockitoJUnitRunner.class)
public class testTester {

    @Mock
    private List&amp;amp;lt;Integer&amp;amp;gt; mocked;

    @Spy
    private ArrayList&amp;amp;lt;Integer&amp;amp;gt; spyed;

    @Test
    public void test() {
        when(mocked.isEmpty()).thenReturn(false);
        when(spyed.isEmpty()).thenReturn(false);

        assertTrue(!mocked.isEmpty());
        assertTrue(!spyed.isEmpty());

        mocked.add(0);
        spyed.add(0);

        assertTrue(0 == mocked.size());
        assertTrue(1 == spyed.size());
    }
}&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        
        <item>
            <title>《Python 编程之美》译者序</title>
            <description>&lt;p&gt;从毕业至今，在互联网行业从事软件研发工作，将近五年。这五年间，做过后端开发、前端开发、大数据处理等，使用过的编程语言包括：Python、PHP、Go、Java、JavaScript 等。&lt;/p&gt;
&lt;p&gt;虽说编程语言各异，但我使用它们来写各种项目的代码却一直坚持两点：代码可读性和自解释性/自文档性（self-documentation）。这很大程度上应该是受到 Python 语言设计哲学的影响 - 追求简单易读易懂的代码。&lt;/p&gt;
&lt;p&gt;很多人可能会认为这两点其实是一点 - 代码可读性，但我想做点区分：代码可读性突出对代码阅读者视觉上的影响，是否存在不必要的理解干扰，比如：必要的空行、变量定义与使用之间的距离、函数体/逻辑分支是否过长、逻辑表达是否直观等等。可读性高的代码通常都非常漂亮、赏心悦目。自解释性代码则更突出语义层面，比如：变量名称/函数名称/类名是否恰当、函数/方法/API 是否单一职责、工程目录结构/包/模块拆分是否符合“高内聚低耦合”原则等等。长期追求这两点，可以极大地提升个人，特别是团队的工作效率和工作质量。&lt;/p&gt;
&lt;p&gt;本书作者 Kenneth Reitz 于 2011 年发布 Requests 这个 HTTP 请求工具库，提出“for humans”的理念，强调软件/工具库应该对人类友好易用，这一理念本质上是对 Python 哲学（特别是上述两点）的一种引申和发扬。之后 Reitz 在一些 Python大会上做技术分享，宣扬“for humans”理念，对 Python 社区产生巨大影响。我在第一次用过 Requests 库之后，便很少使用 Python 标准库中的 urllib 和 urllib2，现在标准库文档中也特别建议开发者使用 Requests。&lt;/p&gt;
&lt;p&gt;因为对“for humans”理念的认同，也因为经常使用 Requests，所以当 Reitz 在 Github 上邀请我翻译 Requests 文档中文版时，我欣然接受，和另一个 Python 开发者共同翻译了 Requests 文档的首个官方中文翻译版。这“另一个 Python 开发者”也就是本书的另一个译者。&lt;/p&gt;
&lt;p&gt;在 Reitz 发起 “The Hitchhiker&apos;s Guide to Python!” 项目（也就是本书的社区开源版）后，我一直持续跟进阅读，收获巨大。后来得知这本开源书籍正式出版，欣喜若狂，辗转咨询多人，联系到刘皎老师 ，申请了本书的翻译工作。&lt;/p&gt;
&lt;p&gt;但是，后来发现翻译的工作量远远超出预估，除了个人的一些主观原因，主要因为本书内容的广度和深度：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;广度：本书由 Python 社区数百人共同创作而成，可以视作 Python 小百科全书。1-3章指导读者按照自己的需求选择安装配置 Python 版本/发行版、开发环境等。7-11章则针对不同的应用场景，从多个维度甄选对比了大量的 Python 库，读者可以“按图索骥”地做出自己的选择，从而节约大量的时间精力。因为译者的 Python 开发经验主要集中在 Web 开发和数据处理，对于很多应用场景下的 Python 库不太熟悉，所以翻译之前花费了大量时间来学习理解。&lt;/li&gt;
&lt;li&gt;深度：针对 Python 中手的核心需求，本书探讨了大量的最佳实践。其中4-5章通过大量示例具体地阐释了“Python 之禅”的句句箴言，如何编写高质量的 Python 代码，并精选若干高质量的知名 Python 开源项目，详细介绍如何通过阅读源码来提升编程技术水平。虽说 Python 社区几乎人人皆知“Python 之禅”，但如何落地到开发实践估计极少有人说得清楚。对照书中的实例阐释，译者几经调整推敲“Python 之禅”的译文，最终敲定的译文也不是特别令自己满意。&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;相比原计划，本书最终延期近一年才得以翻译完成。除了歉意，我内心满是感谢：感谢邦杰中途友情加入，帮忙翻译了4-6章初稿，这三章的难度和长度都非常大；感谢编辑老师刘皎对我拖稿的次次容忍和耐心等待；感谢妻儿的理解，我对你们缺少了太多的陪伴。&lt;/p&gt;
&lt;p&gt;虽说我已尽自己所能地保证译文质量，但错误瑕疵难免，在此也请读者原谅。希望你们阅读愉快！&lt;/p&gt;
&lt;p&gt;至此，我如释重负。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;夏永锋&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;写于上海&lt;/em&gt;&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Base64编码原理与应用</title>
            <description>&lt;p&gt;2015年，我们在青云平台上实现了“百度云观测”应用。青云应用本质上是一个iframe，在向iframe服务方发送的请求中会携带一些数据，青云平台会使用&lt;code&gt;Base64 URL&lt;/code&gt;对这些数据进行编码，其提供的编码解码算法示例如下：&lt;/p&gt;
&lt;pre class=&quot;language-php&quot;&gt;&lt;code&gt;// php版本
function base64_URL_encode($data) {
  return rtrim(strtr(base64_encode($data), &amp;amp;apos;+/&amp;amp;apos;, &amp;amp;apos;-_&amp;amp;apos;), &amp;amp;apos;=&amp;amp;apos;);
}
function base64_URL_decode($data) {
  return base64_decode(str_pad(strtr($data, &amp;amp;apos;-_&amp;amp;apos;, &amp;amp;apos;+/&amp;amp;apos;), 
                            strlen($data) % 4, &amp;amp;apos;=&amp;amp;apos;, STR_PAD_RIGHT));
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看出，&lt;code&gt;Base64 URL&lt;/code&gt; 是标准Base64编码的一个变种，分别用 &lt;code&gt;-&lt;/code&gt;、&lt;code&gt;_&lt;/code&gt; 替换标准Base64编码结果中的 &lt;code&gt;+&lt;/code&gt; 、 &lt;code&gt;/&lt;/code&gt; ，并删除结果最后的 &lt;code&gt;=&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;在实现 “百度云观测” 青云应用时，我在想：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;为什么要使用Base64编码？&lt;/li&gt;
&lt;li&gt;Base64编码算法是什么样的？&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;本文是围绕这两个问题思考和实践的结果。&lt;/p&gt;
&lt;p&gt;我认为，理解Base64或其他类似编码的关键有两点：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;计算机最终存储和执行的是01二进制序列，这个二进制序列的含义则由解码程序/解释程序决定&lt;/li&gt;
&lt;li&gt;很多场景下的数据传输要求数据只能由简单通用的字符组成，比如HTTP协议要求请求的首行和请求头都必须是ASCII编码&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;以青云应用为例，简单解释这两点。青云平台通过POST一个表单来获取iframe，表单有 &lt;code&gt;payload&lt;/code&gt; 和 &lt;code&gt;signature&lt;/code&gt; 两项， &lt;code&gt;payload&lt;/code&gt; 原本是一个JSON对象，其中的键值可能包含一些特殊字符，比如 &lt;code&gt;&amp;amp;&lt;/code&gt;、&lt;code&gt;/&lt;/code&gt; 等，由于青云设计的一种通用的请求交互方案，需要考虑iframe服务方服务器端的各种可能实现，有些服务器端实现没有考虑表单值有这些特殊字符，或者POST请求被中间服务器转换成GET请求再次发出，对于URL来说，&lt;code&gt;&amp;amp;&lt;/code&gt;、&lt;code&gt;/&lt;/code&gt;都是具有特殊含义的字符，所以需要对请求数据进行特殊编码避免这些字符出现 - 数据发送方对数据按规则进行编码，接收方对应地按规则解码数据。&lt;/p&gt;
&lt;h2&gt;Base64编码原理&lt;/h2&gt;
&lt;p&gt;Base64编码之所以称为Base64，是因为其使用64个字符来对任意数据进行编码，同理有Base32、Base16编码。标准Base64编码使用的64个字符为：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/XHFMRvxfez4OVtr.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;500&apos;&gt;
&lt;p&gt;这64个字符是各种字符编码（比如ASCII编码）所使用字符的子集，基本，并且可打印。唯一有点特殊的是最后两个字符，因对最后两个字符的选择不同，Base64编码又有很多变种，比如Base64 URL编码。&lt;/p&gt;
&lt;p&gt;Base64编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续6比特（2的6次方=64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。&lt;/p&gt;
&lt;p&gt;假设我们要对 &lt;code&gt;Hello!&lt;/code&gt; 进行Base64编码，按照ASCII表，其转换过程如下图所示：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/tJnClQsjc4WMGhB.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;可知 &lt;code&gt;Hello!&lt;/code&gt; 的Base64编码结果为 &lt;code&gt;SGVsbG8h&lt;/code&gt; ，原始字符串长度为6个字符，编码后长度为8个字符，每3个原始字符经Base64编码成4个字符，编码前后长度比4/3，这个长度比很重要 - 比原始字符串长度短，则需要使用更大的编码字符集，这并不我们想要的；长度比越大，则需要传输越多的字符，传输时间越长。Base64应用广泛的原因是在字符集大小与长度比之间取得一个较好的平衡，适用于各种场景。&lt;/p&gt;
&lt;p&gt;是不是觉得Base64编码原理很简单？&lt;/p&gt;
&lt;p&gt;但这里需要注意一个点：Base64编码是每3个原始字符编码成4个字符，如果原始字符串长度不能被3整除，那怎么办？使用0值来补充原始字符串。&lt;/p&gt;
&lt;p&gt;以 &lt;code&gt;Hello!!&lt;/code&gt; 为例，其转换过程为：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/5URB8nVis9ljwYe.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;&lt;em&gt;注：图表中蓝色背景的二进制0值是额外补充的。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Hello!!&lt;/code&gt; Base64编码的结果为 &lt;code&gt;SGVsbG8hIQAA&lt;/code&gt; 。最后2个零值只是为了Base64编码而补充的，在原始字符中并没有对应的字符，那么Base64编码结果中的最后两个字符 &lt;code&gt;AA&lt;/code&gt; 实际不带有效信息，所以需要特殊处理，以免解码错误。&lt;/p&gt;
&lt;p&gt;标准Base64编码通常用 &lt;code&gt;=&lt;/code&gt; 字符来替换最后的 &lt;code&gt;A&lt;/code&gt;，即编码结果为 &lt;code&gt;SGVsbG8hIQ==&lt;/code&gt;。因为 &lt;code&gt;=&lt;/code&gt; 字符并不在Base64编码索引表中，其意义在于结束符号，在Base64解码时遇到 &lt;code&gt;=&lt;/code&gt; 时即可知道一个Base64编码字符串结束。&lt;/p&gt;
&lt;p&gt;如果Base64编码字符串不会相互拼接再传输，那么最后的 &lt;code&gt;=&lt;/code&gt; 也可以省略，解码时如果发现Base64编码字符串长度不能被4整除，则先补充 &lt;code&gt;=&lt;/code&gt; 字符，再解码即可。&lt;/p&gt;
&lt;p&gt;为了理解Base64编码解码过程，个人实现了一个非常简陋的Base64编码解码程序，见：&lt;a href=&apos;https://github.com/youngsterxyf/xiaBase64&apos;&gt;youngsterxyf/xiaBase64&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;由于Base64应用广泛，所以很多编程语言的标准库都内置Base64编码解码包，如：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;PHP：&lt;a href=&apos;http://php.net/manual/en/function.base64-encode.php&apos;&gt;base64_encode&lt;/a&gt;、&lt;a href=&apos;http://php.net/manual/en/function.base64-decode.php&apos;&gt;base64_decode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python：&lt;a href=&apos;https://docs.python.org/2/library/base64.html&apos;&gt;base64包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Go：&lt;a href=&apos;https://golang.org/pkg/encoding/base64/&apos;&gt;encoding/base64&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;Base64编码应用&lt;/h2&gt;
&lt;p&gt;本文开始提到的青云应用例子只是Base64编码的应用场景之一。由于Base64编码在字符集大小与编码后数据长度之间做了较好的平衡，以及Base64编码变种形式的多样，使得Base64编码的应用场景非常广泛。下面举2个常用常见的例子。&lt;/p&gt;
&lt;h3&gt;HTML内嵌Base64编码图片&lt;/h3&gt;
&lt;p&gt;前端在实现页面时，对于一些简单图片，通常会选择将图片内容直接内嵌在页面中，避免不必要的外部资源加载，增大页面加载时间，但是图片数据是二进制数据，该怎么嵌入呢？&lt;a href=&apos;http://caniuse.com/#search=Data%20URI&apos;&gt;绝大多数现代浏览器&lt;/a&gt;都支持一种名为 &lt;code&gt;Data URLs&lt;/code&gt; 的特性，允许使用Base64对图片或其他文件的二进制数据进行编码，将其作为文本字符串嵌入网页中。以百度搜索首页为例，其中语音搜索的图标是个背景图片，其内容以 &lt;code&gt;Data URLs&lt;/code&gt; 形式直接写在css中，这个css内容又直接嵌在HTML页面中，如下图所示：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/oa6rsPSwgMzv87l.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;&lt;code&gt;Data URLs&lt;/code&gt; 格式为：&lt;code&gt;url(data:文件类型;编码方式,编码后的文件内容)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;当然，也可以直接基于image标签嵌入图片，如下所示：&lt;/p&gt;
&lt;pre class=&quot;language-html&quot;&gt;&lt;code&gt;&amp;amp;lt;img alt=&amp;amp;quot;Embedded Image&amp;amp;quot; src=&amp;amp;quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIA...&amp;amp;quot; /&amp;amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;但请注意：如果图片较大，图片的色彩层次比较丰富，则不适合使用这种方式，因为其Base64编码后的字符串非常大，会明显增大HTML页面，影响加载速度。&lt;/p&gt;
&lt;h3&gt;MIME（多用途互联网邮件扩展）&lt;/h3&gt;
&lt;p&gt;我们的电子邮件系统，一般是使用SMTP（简单邮件传输协议）将邮件从客户端发往服务器端，邮件客户端使用POP3（邮局协议，第3版本）或IMAP（交互邮件访问协议）从服务器端获取邮件。&lt;/p&gt;
&lt;p&gt;SMTP协议一开始是基于纯ASCII文本的，对于二进制文件（比如邮件附件中的图像、声音等）的处理并不好，所以后来新增MIME标准来编码二进制文件，使其能够通过SMTP协议传输。&lt;/p&gt;
&lt;p&gt;举例来说，我给自己发封邮件，正文为空，带一个名为hello.txt的附件，内容为 &lt;code&gt;您好！世界！&lt;/code&gt;。导出邮件源码，其关键部分如下图所示：&lt;/p&gt;
&lt;img src=&apos;https://i.loli.net/2020/06/14/c8wIeoij9HWt4Ph.jpg&apos; title=&apos;&apos; alt=&apos;&apos; width=&apos;600&apos;&gt;
&lt;p&gt;&lt;code&gt;MIME-Version: 1.0&lt;/code&gt;：表示当前使用MIME标准1.0版本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Content-Type: text/plain; name=&amp;quot;hello.txt&amp;quot;&lt;/code&gt;：表示附件文件名为 &lt;code&gt;hello.txt&lt;/code&gt; ，格式为纯文本。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Content-Transfer-Encoding: base64&lt;/code&gt;：表示附件文件内容使用base64编码后传输。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;5oKo5aW977yM5LiW55WM77yB&lt;/code&gt;：则是文件内容 &lt;code&gt;您好，世界！&lt;/code&gt; Base64编码后的结果。&lt;/p&gt;
&lt;p&gt;不过，MIME使用的不是标准Base64编码。&lt;/p&gt;
&lt;h2&gt;切忌误用&lt;/h2&gt;
&lt;p&gt;可能会有人在不理解Base64编码的情况下，将其误用于数据加密或数据校验。&lt;/p&gt;
&lt;p&gt;Base64是一种数据编码方式，目的是让数据符合传输协议的要求。标准Base64编码解码无需额外信息即完全可逆，即使你自己自定义字符集设计一种类Base64的编码方式用于数据加密，在多数场景下也较容易破解。&lt;/p&gt;
&lt;p&gt;对于数据加密应该使用专门的&lt;strong&gt;目前还没有有效方式快速破解的&lt;/strong&gt;加密算法。比如：对称加密算法&lt;code&gt;AES-128-CBC&lt;/code&gt;，对称加密需要密钥，只要密钥没有泄露，通常难以破解；也可以使用非对称加密算法，如 &lt;code&gt;RSA&lt;/code&gt;，利用极大整数因数分解的计算量极大这一特点，使得使用公钥加密的数据，只有使用私钥才能快速解密。&lt;/p&gt;
&lt;p&gt;对于数据校验，也应该使用专门的消息认证码生成算法，如 &lt;code&gt;HMAC&lt;/code&gt; - 一种使用单向散列函数构造消息认证码的方法，其过程是不可逆的、唯一确定的，并且使用密钥来生成认证码，其目的是防止数据在传输过程中被篡改或伪造。将原始数据与认证码一起传输，数据接收端将原始数据使用相同密钥和相同算法再次生成认证码，与原有认证码进行比对，校验数据的合法性。&lt;/p&gt;
&lt;p&gt;那么针对各大网站被脱库的问题，请问应该怎么存储用户的登录密码？&lt;/p&gt;
&lt;p&gt;答案是：在注册时，根据用户设置的登录密码，生成其消息认证码，然后存储用户名和消息认证码，不存储原始密码。每次用户登录时，根据登录密码，生成消息认证码，与数据库中存储的消息认证码进行比对，以确认是否为有效用户，这样即使网站被脱库，用户的原始密码也不会泄露，不会为用户使用的其他网站带来账号风险。&lt;/p&gt;
&lt;p&gt;当然，使用的消息认证码算法其哈希碰撞的概率应该极低才行，目前一般在HMAC算法中使用SHA256。对于这种方式需要注意一点：防止用户使用弱密码，否则也可能会被暴力破解。现在的网站一般要求用户密码6个字符以上，并且同时有数字和大小写字母，甚至要求有特殊字符。&lt;/p&gt;
&lt;p&gt;另外，也可以使用加入随机salt的哈希算法来存储校验用户密码。这里暂不细述。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;Base64兼顾字符集大小和编码后数据长度，并且可以灵活替换字符集的最后两个字符，以应对多样的需求，使其适用场景非常广泛。&lt;/p&gt;
&lt;p&gt;当然，很多场景下有多种编码方式可选择，并非Base64编码不可，视需求，权衡利弊而定。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>编程名言集锦（译）</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;http://quotes.cat-v.org/programming/&apos;&gt;Programming Quotes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;译者：&lt;a href=&apos;https://github.com/youngsterxyf&apos;&gt;youngsterxyf&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;C.A.R. Hoare, The 1980 ACM Turing Award Lecture&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies and the other way is to make it so complicated that there are no obvious deficiencies.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;有两种软件设计的方式：一种是使它足够简单以致于明显没有缺陷，另一种则是使它足够复杂以致于没有明显的缺陷。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;E.W.Dijkstra&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The computing scientist&apos;s main challenge is not to get confused by the complexities of his own making.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;计算科学家的主要挑战是不要被他自己造成的复杂性搞糊涂了。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Gordon Bell&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The cheapest, fastest, and most reliable components are those that aren&apos;t there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最廉价，最快速，并且最可靠的部件是那些还没被使用的。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;http://genius.cat-v.org/ken-thompson/&apos;&gt;Ken Thompson&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;One of my most productive days was throwing away 1000 lines of code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我最多产的一天抛弃了1000行代码。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;http://genius.cat-v.org/ken-thompson/&apos;&gt;Ken Thompson&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;When in doubt, use brute force.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;若无把握，暴力破解。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Jeff Sickel&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Deleted code is debugged code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;需要调试的代码都应该删除。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Brian W. Kernighan, P. J. Plauger&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;调试的难度两倍于一开始的写代码。因此，如果你尽可能巧妙地编写代码，根据定义，说明你还不具备足够的智商来调试它。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Brian W. Kernighan&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The most effective debugging tool is still careful thought, coupled with judiciously placed print statements.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最有效的调试工具是静下心来仔细思考，辅之审慎地放置打印语句。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Brian W. Kernighan&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Controlling complexity is the essence of computer programming.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;计算机编程的本质是控制复杂度。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;David Gelernter&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Beauty is more important in computing than anywhere else in technology because software is so complicated. Beauty is the ultimate defence against complexity.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;相比其他技术领域，美对于计算来说更为重要，因为软件超乎寻常的复杂，而美是对复杂性的一种终极防御。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Doug Gwyn&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;UNIX was not designed to stop its users from doing stupid things, as that would also stop them from doing clever things.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;UNIX并不会阻止用户干蠢事，因为那样也会阻碍用户做些聪明的事情。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;John Carmack&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;If you&apos;re willing to restrict the flexibility of your approach, you can almost always do something better.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;限制方法的灵活性几乎总会让你把事情做得更好。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;John Osterhout&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;A program that produces incorrect result twice as fast is infinitely slower.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;结果不对，程序再快都顶个屁用。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Geer et al.&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The central enemy of reliability is complexity.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;可靠的最大敌人是复杂。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Edsger W. Dijkstra&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Simplicity is prerequisite for reliability.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;简单是可靠的先决条件。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Peter Deutsch&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The Eight Fallacies of Distributed Computing&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Essentially everyone, when they first build a distributed application, makes the following eight assumptions. All prove to be false in the long run and all cause big trouble and painful learning experiences.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;1. The network is reliable&lt;/p&gt;
&lt;p&gt;2. Latency is zero&lt;/p&gt;
&lt;p&gt;3. Bandwidth is infinite&lt;/p&gt;
&lt;p&gt;4. The network is secure&lt;/p&gt;
&lt;p&gt;5. Topology doesn&apos;t change&lt;/p&gt;
&lt;p&gt;6. There is one administrator&lt;/p&gt;
&lt;p&gt;7. Transport cost is zero&lt;/p&gt;
&lt;p&gt;8. The network is homogeneous&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;分布式计算的八大谬误&lt;/p&gt;
&lt;p&gt;实际上，每个人，当他第一次构建分布式应用时，都会作出如下八个假设。长远来看，这些假设都被证明是错误的，并且都造成了巨大的麻烦和沉痛的经验教训。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;网络可靠&lt;/li&gt;
&lt;li&gt;零延迟&lt;/li&gt;
&lt;li&gt;带宽无限&lt;/li&gt;
&lt;li&gt;安全网络&lt;/li&gt;
&lt;li&gt;拓扑不变&lt;/li&gt;
&lt;li&gt;有个管理者&lt;/li&gt;
&lt;li&gt;传输代价为零&lt;/li&gt;
&lt;li&gt;网络同构&lt;/li&gt;&lt;/ol&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Jon Bentley, Doug Mcllroy&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The key to performance is elegance, not battalions of special cases.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;性能的关键是优雅，而不是大堆的特殊情况。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Bill Gates&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Measuring programming progress by lines of code is like measuring aircraft building progress by weight.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;以代码行数来衡量程序设计的进度，就好比以重量来衡量飞机的制造进度。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;John Johnson&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;First, solve the problem. Then, write the code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;首先，解决问题。而后，编写代码。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Ken Thompson&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;You can&apos;t trust code that you did not totally create yourself.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;你不能信任非你完全自己写的代码。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Sean Parent&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Good code is short, simple, and symmetrical - the challenge is figuring out how to get there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;好的代码，短小、简洁，并且匀称 - 而真正的挑战在于弄清如何达到这些目标。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Voltaire&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The best is the enemy of the good.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;追求完美是优秀软件的敌人。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Dr. Pamela Zave&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;The purpose of software engineering is to control complexity, not to create it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;软件工程的目标是控制复杂度，而不是增加复杂性。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Olin Shivers&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;I object to doing things that computers can do.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我反对去做那些计算机可以做的事情。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;merb motto&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;No code is faster than no code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;没有什么代码会比没有代码速度更快。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Dave Parnas&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;As a rule, software systems do not work well until they have been used, and have failed repeatedly, in real applications.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;一般说来，软件系统只有得到实际应用，并且经历多次失败，才能工作得很好。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;RnRS&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;程序语言的设计不应该是特性的堆叠，而应该去除那些使得额外的特性显得必要的弱点和局限。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Ryan Singer&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;So much complexity in software comes from trying to make one thing do two things.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;软件中如此多的复杂性皆来自于想在做一件事的同时多做几件事。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;未完待续&lt;/em&gt;&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>面向分布式系统工程师的分布式系统理论（译）</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;http://the-paper-trail.org/blog/distributed-systems-theory-for-the-distributed-systems-engineer/&apos;&gt;Distributed systems theory for the distributed systems engineer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gwen Shapira，大腕级的解决方案架构师（SA），如今 Cloudera 的全职工程师，在&lt;a href=&apos;https://twitter.com/gwenshap/status/497203248332165121&apos;&gt; Twitter 上提的一个问题&lt;/a&gt;引起了我的思考。&lt;/p&gt;
&lt;p&gt;如果是以前，我可能会回答“嗯，这里有篇 FLP 论文，这里有篇 Paxos 论文，这里还有篇拜占庭将军问题的论文...”，我会罗列一箩筐重要的材料，如果你一头扎进去，至少花费 6 个月的时间才能过一遍这些材料。然而我已逐渐明白推荐大量的理论性的论文通常恰恰是着手学习分布式系统理论的错误方式（除非你在做一个 PhD 项目）。论文通常比较深入难懂，需要认真地研习，通常还需要&lt;em&gt;大量的时间投入（significant experience）&lt;/em&gt;来理清这些论文的重要贡献，以及在整个理论体系中的位置。要求工程师具备这样的专业水平又有多大的意义呢？&lt;/p&gt;
&lt;p&gt;但是，很遗憾，对分布式系统理论方面的重大研究成果和思想进行概括、归纳、背景分析的‘导引’性质的优秀材料非常缺乏；特别是没有居高临下态度的材料。对这块空白区域的思考让我想到了另一个有趣的问题：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;一个分布式系统工程师应该知道些什么分布式系统理论？&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;在这种情况下，一知半解（a little theory）并不会是一件多危险的事情。因此我尝试整理一个列表，罗列出作为一个分布式系统工程师的我认为能够直接应用于我日常工作的一些基本概念；或者让分布式系统工程师完全有能力设计一个新系统的“筹码”。如果你认为我漏掉了一些东西，请联系我。&lt;/p&gt;
&lt;h4&gt;入门第一步&lt;/h4&gt;
&lt;p&gt;以下 4 篇材料出色地解释了构建分布式系统会遇到的一些挑战，共同概述了一系列分布式系统工程师必须要解决的技术上的难题，为之后章节中更深入的研究做好准备。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;http://book.mixu.net/distsys/&apos;&gt;好玩又实在的分布式系统理论&lt;/a&gt;是一本简短的书籍，其内容覆盖了分布式系统领域的一些基本议题，包括时间的作用及不同的复制策略。&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/&apos;&gt;为分布式系统领域新人整理的笔记&lt;/a&gt; - 不是理论对理论地讲述，而是做一个非常好非常实用的平衡，让你对其余材料的阅读能够落地。&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628&apos;&gt;分布式系统研究综述报告&lt;/a&gt; - 一篇经典的论文，解释了为什么不能将所有远程交互都模拟成和本地对象一样。&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing&apos;&gt;关于分布式计算的若干谬论&lt;/a&gt; - 分布式计算方面的8点谬论，提醒系统设计者可能会忘记的几类事情。&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;失败和时间&lt;/h4&gt;
&lt;p&gt;分布式系统工程师需要面对的许多困难最终都可以归咎于两个潜在的原因：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;进程可能会失败&lt;/li&gt;
&lt;li&gt;不存在一种好的方式来周知目前为止进程已经做了些什么&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;进程之间对于&lt;em&gt;时间&lt;/em&gt;的认知能共享些什么？哪些失败的场景是能够检测到？什么算法和原语可能被正确地实现？这三个问题有着非常深层的联系。多数时候，我们会假设两个不同节点之间对于时间概念或时间以什么样的速度逝去没有任何可共享的认知。&lt;/p&gt;
&lt;p&gt;你应该知道：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;失败模式的（部分）分层：&lt;a href=&apos;http://www.cse.psu.edu/~gcao/teach/513-00/c7.pdf&apos;&gt;崩溃停止-&gt;排除（omission）&lt;/a&gt;-&gt;&lt;a href=&apos;http://en.wikipedia.org/wiki/Byzantine_fault_tolerance&apos;&gt;拜占庭容错&lt;/a&gt;。你应该理解：在高层次上可能发生的问题在低层次上肯定可能发生，在低层次上不可能发生的问题在高层次上也肯定不可能发生。&lt;/li&gt;
&lt;li&gt;在没有任何共享时钟的情况下如何判断在另一个事件之前是否产生了某事件。这意味着你需要理解 &lt;a href=&apos;http://web.stanford.edu/class/cs240/readings/lamport.pdf&apos;&gt;Lamport 时钟&lt;/a&gt;及其一般化的&lt;a href=&apos;http://en.wikipedia.org/wiki/Vector_clock&apos;&gt;向量时钟&lt;/a&gt;，也需要阅读一下&lt;a href=&apos;http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf&apos;&gt;这篇 Dynamo 论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;单个失败发生的可能性对于我们实现正确的分布式系统到底会有多大的影响（请阅读下面关于 FLP 结果的笔记）？&lt;/li&gt;
&lt;li&gt;不同的时间模型：同步、部分同步和异步（若我找到好的参考文献会添加链接）&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;容错的基本矛盾&lt;/h4&gt;
&lt;p&gt;一个系统，若要不降级而容忍某些错误的发生，就必须能够好像那些错误没有发生一样地运作。这通常意味着系统的这些部分必须能够冗余地工作，但是非绝对必要地做更多的工作通常会在性能和资源耗用方面产生一些消耗。这是为系统添加容错带来的基本矛盾。&lt;/p&gt;
&lt;p&gt;你应该知道：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;确保单拷贝可串行化（single-copy serialisability）的仲裁（quorum）技术。可阅读 &lt;a href=&apos;https://ecommons.library.cornell.edu/bitstream/1813/6323/1/82-483.pdf&apos;&gt;Skeen 的原始论文&lt;/a&gt;，但可能更建议阅读&lt;a href=&apos;http://en.wikipedia.org/wiki/Quorum_(distributed_computing&apos;&gt;这个 Wikipedia 词条&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;关于&lt;a href=&apos;http://the-paper-trail.org/blog/consensus-protocols-two-phase-commit/&apos;&gt;两阶段提交&lt;/a&gt;、&lt;a href=&apos;http://the-paper-trail.org/blog/consensus-protocols-three-phase-commit/&apos;&gt;三阶段提交&lt;/a&gt;和 &lt;a href=&apos;http://the-paper-trail.org/blog/consensus-protocols-paxos/&apos;&gt;Paxos&lt;/a&gt; 算法，以及为什么它们有不同的容错性质。&lt;/li&gt;
&lt;li&gt;最终一致性，及其他技术是如何以弱化对系统行为的保证为代价来尝试避免这种矛盾的。这篇 &lt;a href=&apos;http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf&apos;&gt;Dynamo 论文&lt;/a&gt;是一个很好的起点，同时 Pat Helland 的经典之作 &lt;a href=&apos;http://www.ics.uci.edu/~cs223/papers/cidr07p15.pdf&apos;&gt;Life Beyond Transactions&lt;/a&gt; 也是必读的。&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;基本的原语&lt;/h4&gt;
&lt;p&gt;分布式系统中很少有大家一致认同的基本构建块，但越来越多地在出现。你应该以下的问题是什么，以及在哪可以找到它们的解决方案：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;群首选举（leader election）（例如 &lt;a href=&apos;http://en.wikipedia.org/wiki/Bully_algorithm&apos;&gt;Bully 算法&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;一致的快照（例如 Chandy 和 Lamport 所写的&lt;a href=&apos;http://research.microsoft.com/en-us/um/people/lamport/pubs/chandy.pdf&apos;&gt;经典论文&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;共识（阅读上文提到的关于 2PC 和 Paxos 的博文）&lt;/li&gt;
&lt;li&gt;分布式状态机复制（看看 &lt;a href=&apos;http://en.wikipedia.org/wiki/State_machine_replication&apos;&gt;Wikipedia&lt;/a&gt; 就可以，但 &lt;a href=&apos;http://research.microsoft.com/en-us/um/people/blampson/58-Consensus/Acrobat.pdf&apos;&gt;Lampson 的论文&lt;/a&gt;更权威，只是枯燥了点）。&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;基础结论&lt;/h4&gt;
&lt;p&gt;某些客观事实是需要内化于心的，以下是几个关键点（a flavour）（当然还有更多）：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;如果进程之间可能丢失某些消息，那么不可能在实现一致性存储的同时能响应所有的请求。这就是 &lt;a href=&apos;http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf&apos;&gt;CAP 定理&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;以这样一种方式（a.始终是正确的、b.始终能终止-若在一个可能因失败崩溃停止（crash-&lt;code&gt;*&lt;/code&gt; stop failures）的异步系统中有（甚至仅）一台机器失效时（FLP 的结果））。我希望在&lt;a href=&apos;http://www.slideshare.net/HenryRobinson/pwl-nonotes&apos;&gt;洛杉矶题为 Papers We Love 报告&lt;/a&gt;的第一部分幻灯片-进行证明之前-已经合理地解释了这个结论。&lt;em&gt;建议：没有实际的必要理解这个证明。&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;一般而言，消息交互少于两轮是不可能达成共识（Consensus）。&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;真实系统&lt;/h4&gt;
&lt;p&gt;最重要的练习是重复地阅读新兴的、真实系统的描述，并尝试评价它们的设计决策。一遍又一遍地这样去做。一些建议：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Google:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf&apos;&gt;GFS&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/archive/spanner-osdi2012.pdf&apos;&gt;Spanner&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41344.pdf&apos;&gt;F1&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/archive/chubby-osdi06.pdf&apos;&gt;Chubby&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf&apos;&gt;BigTable&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41378.pdf&apos;&gt;MillWheel&lt;/a&gt;、&lt;a href=&apos;http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf&apos;&gt;Omega&lt;/a&gt;、&lt;a href=&apos;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf&apos;&gt;Dapper&lt;/a&gt;、&lt;a href=&apos;http://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf&apos;&gt;Paxos Made Live&lt;/a&gt;、&lt;a href=&apos;http://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/abstract&apos;&gt;The Tail At Scale&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Not Google:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&apos;http://research.microsoft.com/en-us/projects/dryad/eurosys07.pdf&apos;&gt;Dryad&lt;/a&gt;、&lt;a href=&apos;https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf&apos;&gt;Cassandra&lt;/a&gt;、&lt;a href=&apos;http://ceph.com/papers/weil-ceph-osdi06.pdf&apos;&gt;Ceph&lt;/a&gt;、&lt;a href=&apos;https://ramcloud.stanford.edu/wiki/display/ramcloud/RAMCloud+Papers&apos;&gt;RAMCloud&lt;/a&gt;、&lt;a href=&apos;http://hyperdex.org/papers/&apos;&gt;HyperDex&lt;/a&gt;、&lt;a href=&apos;http://www.mpi-sws.org/~druschel/courses/ds/papers/cooper-pnuts.pdf&apos;&gt;PNUTS&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>通过示例学习Git内部构造（译）</title>
            <description>&lt;p&gt;原文：&lt;a href=&apos;http://teohm.github.io/blog/2011/05/30/learning-git-internals-by-example/&apos;&gt;Learning Git Internals by Example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;状态：草稿&lt;/p&gt;
&lt;p&gt;计划修订本文，未来可能会简化一些...&lt;/p&gt;
&lt;h2&gt;动机&lt;/h2&gt;
&lt;p&gt;从Subversion和Mercurial切换到Git之后的几个月，我始终觉得Git在本质上是不同于Subversion和Mercurial的，但没法确切地说出区别。&lt;/p&gt;
&lt;p&gt;我经常在Github上看到tree、parent等术语，也搞不清楚它们确切的含义。&lt;/p&gt;
&lt;p&gt;因此我决定花些时间学学Git。&lt;/p&gt;
&lt;p&gt;我会尝试概述，并阐述一路走来学到的关于Git的关键信息...但这仅是有助于我回答Git与其他源码控制工具区别的Git内部构造基本知识。&lt;/p&gt;
&lt;h2&gt;实体、引用、索引（Objects，References，The Index）&lt;/h2&gt;
&lt;p&gt;要理解Git内部构造的核心，我们应理解三个东西： &lt;strong&gt;实体&lt;/strong&gt;、&lt;strong&gt;引用&lt;/strong&gt;、 &lt;strong&gt;索引&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我发现这个模型非常优雅。用一个小小的图表就能完全展现，也易于理解记忆。&lt;/p&gt;
&lt;img src=&apos;../assets/git-internals/big-picture.png&apos; title=&apos;Big Picture&apos; alt=&apos;Big Picture&apos; width=&apos;500&apos;&gt;
&lt;h3&gt;实体&lt;/h3&gt;
&lt;p&gt;你提交到一个Git代码仓库中的所有文件，包括每个提交的说明信息（the commit info）都在目录 &lt;code&gt;.git/objects/&lt;/code&gt;中存储为&lt;strong&gt;实体&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一个实体以一个40字符长度的字符串（该实体内容的SHA1哈希值）来标识。&lt;/p&gt;
&lt;p&gt;实体有&lt;strong&gt;4类&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;em&gt;blob&lt;/em&gt; - 存储文件内容。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;tree&lt;/em&gt; - 存储目录结构和文件名。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;commit&lt;/em&gt; - 存储提交的说明，组成Git的提交图谱。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;tag&lt;/em&gt; - 存储带注释的标签（tag）。&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;下文的示例会阐明这些实体是如何相互关联的。&lt;/p&gt;
&lt;h3&gt;引用&lt;/h3&gt;
&lt;p&gt;Git中，一个&lt;em&gt;分支（branch）&lt;/em&gt;、&lt;em&gt;远程分支（remote branch）&lt;/em&gt;或一个&lt;em&gt;标签（tag）&lt;/em&gt;（也称为轻量标签）仅是&lt;strong&gt;指向一个实体的一个指针&lt;/strong&gt;，这里的实体通常是一个commit实体。&lt;/p&gt;
&lt;p&gt;这些引用以文本文件的形式存储在目录&lt;code&gt;.git/refs/&lt;/code&gt;中。&lt;/p&gt;
&lt;h4&gt;符号引用（Symbolic References）&lt;/h4&gt;
&lt;p&gt;Git有一种特殊的引用，称为&lt;em&gt;符号引用&lt;/em&gt;。它并不直接指向一个实体，而是&lt;strong&gt;指向另一个引用&lt;/strong&gt;。举例来说，&lt;code&gt;.git/HEAD&lt;/code&gt;就是一个符号引用。它指向你正在工作的当前分支。&lt;/p&gt;
&lt;h3&gt;索引&lt;/h3&gt;
&lt;p&gt;索引是一个暂存区，以二进制文件的形式存储为文件&lt;code&gt;.git/index&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;当&lt;code&gt;git add&lt;/code&gt;一个文件，Git将该文件的信息添加到索引中。当&lt;code&gt;git commit&lt;/code&gt;，Git仅提交索引文件中列出的文件。&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;示例&lt;/h2&gt;
&lt;p&gt;我们来演练一个简单的示例，创建一个Git代码仓库，提交一些文件，看看幕后&lt;code&gt;.git&lt;/code&gt;目录中都发生了些什么。&lt;/p&gt;
&lt;h3&gt;初始化新的代码仓库&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git init canai&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/init.png&apos; title=&apos;初始化代码仓库后&apos; alt=&apos;初始化代码仓库后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;创建了空目录&lt;code&gt;.git/objects/&lt;/code&gt;和&lt;code&gt;.git/refs/&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;还没有索引（Index）文件。&lt;/li&gt;
&lt;li&gt;创建了符号索引文件&lt;code&gt;HEAD&lt;/code&gt;。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ cat .git/HEAD
ref: refs/heads/master&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;添加新文件&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ echo &amp;amp;quot;A roti canai project.&amp;amp;quot; &amp;amp;gt;&amp;amp;gt; README
$ git add README&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/new-file.png&apos; title=&apos;添加新文件后&apos; alt=&apos;添加新文件后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;创建了索引（Index）文件。它有一个SHA1哈希值指向一个blob实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git ls-files --stage
100644 5f89c6f016cad2d419e865df380595e39b1256db 0 README&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个blob实体。README文件的内容存储在该blob中。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# .git/objects/5f/89c6f016cad2d419e865df380595e39b1256db
$ git cat-file blob 5f89c6
A roti canai project.&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;首次提交&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git commit -m&amp;amp;apos;first commit&amp;amp;apos;
[master (root-commit) d9976cf] first commit
1 files changed, 1 insertions(+), 0 deletions(-)
create mode 100644 README&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/first-commit.png&apos; title=&apos;首次提交后&apos; alt=&apos;首次提交后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;创建了分支‘master’引用，指向‘master’分支中最新的commit实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ cat .git/refs/heads/master 
d9976cfe0430557885d162927dd70186d0f521e8&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了第一个commit实体，指向代码仓库根目录tree实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# .git/objects/d9/976cfe0430557885d162927dd70186d0f521e8
$ git cat-file commit d9976cf
tree 0ff699bbafc5d17d0637bf058c924ab405b5dcfe
author Huiming Teo &amp;amp;lt;huiming@favoritemedium.com&amp;amp;gt; 1306739524 +0800
committer Huiming Teo &amp;amp;lt;huiming@favoritemedium.com&amp;amp;gt; 1306739524 +0800

first commit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了tree实体。该tree代表目录“canai”。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# .git/objects/0f/f699bbafc5d17d0637bf058c924ab405b5dcfe
$ git ls-tree 0ff699
100644 blob 5f89c6f016cad2d419e865df380595e39b1256db  README&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;添加一个修改过的文件&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ echo &amp;amp;quot;Welcome everyone.&amp;amp;quot; &amp;amp;gt;&amp;amp;gt; README
$ git add README&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/modified-file.png&apos; title=&apos;添加一个修改过的文件后&apos; alt=&apos;添加一个修改过的文件后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;更新了索引（Index）文件。注意到了吗？它记录了一个新blob。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git ls-files --stage
100644 1192db4c15e019da7fc053225d09dea14bc3ac07 0 README&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个新的blob实体。README的整个内容被存入一个新的blob。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# .git/objects/11/92db4c15e019da7fc053225d09dea14bc3ac07
$ git cat-file blob 1192db
A roti canai project.
Welcome everyone.&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;向子目录中添加文件&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ mkdir doc
$ echo &amp;amp;quot;[[TBD]] manual toc&amp;amp;quot; &amp;amp;gt;&amp;amp;gt; doc/manual.txt
$ git add doc&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/subdir.png&apos; title=&apos;向子目录添加文件后&apos; alt=&apos;向子目录添加文件后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;更新了索引（Index）文件。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git ls-files --stage
100644 1192db4c15e019da7fc053225d09dea14bc3ac07 0 README
100644 ea283e4fb22719fad512405d41dffa050cd16f9a 0 doc/manual.txt&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个新的blob实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;# .git/objects/ea/283e4fb22719fad512405d41dffa050cd16f9a
$ git cat-file blob ea283
[[TBD]] manual toc&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;第二次提交&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git commit -m&amp;amp;apos;second commit&amp;amp;apos;
[master 556eaf3] second commit
 2 files changed, 2 insertions(+), 0 deletions(-)
 create mode 100644 doc/manual.txt&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/second-commit.png&apos; title=&apos;第二次提交后&apos; alt=&apos;第二次提交后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;更新了分支“master”引用，指向该分支中最新的commit实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ cat .git/refs/heads/master 
556eaf374886d4c07a1906b9fdcaba195292b96&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了第二个commit实体。注意它的“parent”是指向首个commit实体。这样形成了一个提交图谱。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git cat-file commit 556e
tree 7729a8b15b747bce541a9752a8f10d57daf221b6
parent d9976cfe0430557885d162927dd70186d0f521e8
author Huiming Teo &amp;amp;lt;huiming@favoritemedium.com&amp;amp;gt; 1306743598 +0800
committer Huiming Teo &amp;amp;lt;huiming@favoritemedium.com&amp;amp;gt; 1306743598 +0800

second commit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个新的代码仓库根目录tree实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git ls-tree 7729
100644 blob 1192db4c15e019da7fc053225d09dea14bc3ac07  README
040000 tree 6ff17d485bf857514f299f0bde0e2a5c932bd055  doc&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个新的子目录tree实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git ls-tree 6ff1
100644 blob ea283e4fb22719fad512405d41dffa050cd16f9a  manual.txt&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;添加一个注释标签（annotated tag）&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git tag -a -m&amp;amp;apos;this is annotated tag&amp;amp;apos; v0.1 d9976&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/annotated-tag.png&apos; title=&apos;添加一个注释标签后&apos; alt=&apos;添加一个注释标签后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;创建了一个标签引用，指向一个tag实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ cat .git/refs/tags/v0.1 
c758f4820f02acf20bb3f6d7f6098f25ee6ed730&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;创建了一个tag实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git cat-file tag c758
object d9976cfe0430557885d162927dd70186d0f521e8
type commit
tag v0.1
tagger Huiming Teo &amp;amp;lt;huiming@favoritemedium.com&amp;amp;gt; 1306744918 +0800

this is annotated tag&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;添加一个新的（轻量的）标签&lt;/h3&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ git tag root-commit d9976&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&apos;../assets/git-internals/new-tag.png&apos; title=&apos;添加一个新的轻量标签后&apos; alt=&apos;添加一个新的轻量标签后&apos; width=&apos;500&apos;&gt;
&lt;p&gt;发生了什么呢？&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;创建了一个标签引用，指向一个commit实体。&lt;/li&gt;&lt;/ul&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;$ cat .git/refs/tags/root-commit 
d9976cfe0430557885d162927dd70186d0f521e8&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;补充阅读&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;http://book.git-scm.com/index.html&apos;&gt;Git社区书&lt;/a&gt;“第7章：内部构造探究”&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://progit.org/book/ch9-0.html&apos;&gt;Pro Git&lt;/a&gt;“第9章：Git内部构造”。&lt;/li&gt;&lt;/ul&gt;
&lt;h2&gt;接下来做什么呢？&lt;/h2&gt;
&lt;p&gt;寻找适合分布式团队、长期项目的一个最小化git工作流。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>工作中的技术人</title>
            <description>&lt;p&gt;工作入职半个月，有些事情不太顺利，还没有正式上手工作，也许大公司的节奏便是如此，但我内心是比较急的，希望能尽快地上手做实际的工作，而不是学习和等待。&lt;/p&gt;
&lt;p&gt;这半个月里，主要是熟悉工作环境，学习了解工作相关的技术。虽说学习，但其实大部分相关技术以前都了解或使用过，只是经验还不够。&lt;/p&gt;
&lt;p&gt;第一周，除了常规的入职事宜，搭建了开发测试环境，并阅读理解工作中使用的web框架。对于这个框架，有太多的吐槽点，严格地说算不上是个框架，可能是因为写得比较早。对于框架，我认为最重要的是为多人协作完成一件事情提供实现上的规范，其次是代码复用，减少工作量。但这个框架除了一些供复用的代码，就啥都没有了。&lt;/p&gt;
&lt;p&gt;第二周，学习巩固PHP基础，一直没认真地学习过PHP，只是在实习的时候做了一些开发，稍微了解了下Yii框架和Zend框架，觉得太复杂了点。除此之外，初步了解组内的运维工作，特别是整个系统的架构。&lt;/p&gt;
&lt;p&gt;经过一番思考，基于自己的理解，昨天编写了一个玩具性质的MVC web框架原型&lt;a href=&apos;https://github.com/youngsterxyf/minibean&apos;&gt;minibean&lt;/a&gt;，该框架以路由转发和控制器为核心，所有非静态文件请求的处理都以Application类对象为入口，按照一定规则对请求URI经路由转发找到对应的控制器类，控制器对象中调用模型与视图的类对象等。以后随着开发经验的增加以及对其他开源成熟框架的学习，会不断地完善该框架。在编写该框架的过程中，深感自己经验的不足，特别是对于Model层，以后可能会刻意阅读某些开源框架的Model层实现。&lt;/p&gt;
&lt;p&gt;目前组内开发工作还很初步，还没有一个正规的开发流程，也没有明确的开发规范。这样虽然没法从已有的工作中学习很多，但也许有机会参与到这些事情创造过程中，收获会更大。&lt;/p&gt;
&lt;p&gt;经过和老同事讨论，以后开发工作涉及的语言和工具包括：Nginx、MySQL、PHP、Redis/Memcached、SVN等，对于这些东西，我都是需要深入学习加强的（当然首先是要解决业务需求）。另外，鉴于原有的那个框架实在不怎么样，以后新的工作可能会选择Zend Framework作为开发框架。&lt;/p&gt;
&lt;p&gt;对于工作环境，我觉得不太满意的地方主要是技术氛围不太浓厚，以后有机会和大家一起建立起好的技术氛围，搞搞技术分享讨论什么的。另外，有点憋屈的是，觉着自己被小看了，老同事老觉得应届毕业生啥的不懂，所以也不急着分配具体的工作给我，老让我学习学习再学习。个人认为最好的学习方式是给个具体的需求，具体的问题让我去解决，有经验的同事只需对结果把把关就可以。当我在这过程中遇到搞不定的问题再向他们请教，以这种方式来上手熟悉工作也许更好。我个人也比较喜欢直接丢个实际的问题让我去解决。&lt;/p&gt;
&lt;p&gt;对于今后的自己，我有两点忠告：&lt;/p&gt;
&lt;p&gt;1、时刻警惕迷失&lt;/p&gt;
&lt;p&gt;虽然工作很重要，要解决业务需求，工作所涉及的技术也应该扎实掌握，深入理解，但不能把自己局限在此，也不能让自己迷失在过于细节的地方。公司提供了一个完善的平台，但这个平台在我看来自足得有些封闭，所以需警惕，要不断地和同事，和公司外面的人交流学习。要经常自我反思，回顾自己走过的路，要让自己的大脑空闲下来花些时间整体规划即将要做的事情。&lt;/p&gt;
&lt;p&gt;2、保持锐气&lt;/p&gt;
&lt;p&gt;初步觉着有些同事没什么工作生活技术的激情，可能是长时间工作的缘故，也可能是因为我还太年轻。但目前我还不愿意自己进入那种状态。自己以后应该更加主动积极地对待工作。我喜欢称呼自己为“技术人”，因为“技术人”不仅是搞技术的，更重要的是对技术有热情，有使命感。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>弄清问题，再求解决</title>
            <description>&lt;p&gt;今天同事问我：是否有什么python库或工具能够将网页内容转换成图片格式的。他在做这方面的事情，还没有好的方法，因为觉得我对python比较熟悉，所以问一下。&lt;/p&gt;
&lt;p&gt;但是我从一开始我就犯错误了。其实我至少应该问一下：为什么要解决这个问题？也就是业务需求是什么？并且稍微一想这个问题其实比较含糊。现在的web页面可以很简单，也可能很复杂。那么这个问题里的“网页”是什么样的网页呢？是任何可能的网页么？目的是需要通过图片来展示网页的哪个部分的信息还是整个网页？这些问题我都没问，也没仔细考虑。&lt;/p&gt;
&lt;p&gt;在没有明确需求的情况下，我就认为是将任何形式的网页完整地转换成图片，但又没弄清如果是这种情况问题的难度有多大。&lt;/p&gt;
&lt;p&gt;在听完问题后，我就想到可能有两种方法：1. 先将网页转换成pdf，然后转换成图片，因为我对于将网页转换成pdf格式的方法有点印象；2.可能存在python实现的工具直接将网页转换成图片格式。你是否发现我的思路有个误区：问题的解决方案需要python代码实现，我假设了需要将这个功能嵌入到一个大的程序中。&lt;/p&gt;
&lt;p&gt;然后我就开始蒙头google找方案。经过一番“艰苦卓绝”的查找，发现：1.确实有如xhtml2pdf等工具能将网页转换成pdf格式，但貌似对于中文的支持不是很好；2.没有好的python库或工具能够直接将网页转换成图片格式，有的方案要收费，有的方案需要调用第三方API，而公司的数据明显是不能让第三方获得的。&lt;/p&gt;
&lt;p&gt;在查找解决方案的过程中，我也逐步意识到上述的那些问题，特别是若假设需要将任何形式的网页转换成图片格式，这个难度非常大，为什么呢？因为现在很多网页的部分内容都是由JS生成的，若你的程序只是简单地从服务器获取网页，该网页含有的JS代码并不会执行，将该网页转换成图片格式，图片所包含的信息与浏览器中展示的并不相同。所以你的程序起码需要包含一个JS解释器。OK，难度一下子就上去了。在我逐步了解其中的难度后，我开始尝试换个角度来考虑问题，反思同事所要解决的业务需求是什么。&lt;/p&gt;
&lt;p&gt;在与他的进一步沟通之后，我才知道：一些总结汇报邮件中需要添加数据统计图，而原有的数据统计图在Web监控页面中，由Raphaeljs库绘制成SVG矢量图。由于无法期望邮件的接收者是从网页版邮箱阅读邮件（他们很可能使用各种邮件客户端如Outlook查看邮件），所以发送带有JS的HTML格式的邮件是没用的。&lt;/p&gt;
&lt;p&gt;在了解业务需求后，我们就明白了其实问题本质上不是要将网页转换成图片，而是要获得&lt;strong&gt;图片格式的数据可视化结果&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那么问题就简单多了，可能从以下三个角度寻找解决方案：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;将网页完整地转换成图片格式&lt;/li&gt;
&lt;li&gt;将网页中的SVG内容转换成图片格式&lt;/li&gt;
&lt;li&gt;使用本地的数据可视化工具将统计数据源，即Raphaeljs绘制SVG矢量图的JSON数据源绘制成图片&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;这三种方案中第二种最佳，为什么呢？因为第一种需要做一些额外的转换工作，自己实现的难度较大，第三种方案与Web监控页面所使用的是不同的数据可视化工具，所以产生的结果一般是不相同的，除非Raphaeljs支持图片格式的输出，那么应该就可以使用nodejs来实现。&lt;/p&gt;
&lt;p&gt;经过网络查找，发现第一种方案与第二种方案都有现成的工具。&lt;/p&gt;
&lt;p&gt;第一种方案：&lt;a href=&apos;https://github.com/ariya/phantomjs&apos;&gt;phantomjs&lt;/a&gt;可以完成，phantomjs包含了webkit，所以解释JS什么的就不再是个问题了，它有个&lt;a href=&apos;https://github.com/ariya/phantomjs/wiki/Screen-Capture&apos;&gt;Screen Capture&lt;/a&gt;的功能模块支持将网页完整地转换成图片格式，但由于要做很多额外的工作，所以效率比较低。&lt;/p&gt;
&lt;p&gt;第二种方案是从Highcharts的&lt;a href=&apos;http://www.highcharts.com/demo/&apos;&gt;Demo&lt;/a&gt;中挖掘出来的，如图所示：&lt;/p&gt;
&lt;img src=&apos;../assets/highchartjsdemo.png&apos; title=&apos;highchartjs_demo&apos; alt=&apos;highchartjs_demo&apos; width=&apos;100%&apos;&gt;
&lt;p&gt;Demo中可以输出多种图片格式，通过chrome浏览器的开发者工具可以发现其实现是向服务器export.highcharts.com发送一个请求，请求中包含网页中生成的SVG矢量图数据、目标图片格式等信息，服务器对该请求进行处理后返回目标格式图片。那么服务器端是如何将SVG转换成图片格式的呢？在Highcharts的&lt;a href=&apos;http://docs.highcharts.com/&apos;&gt;文档&lt;/a&gt;中有个名为&lt;code&gt;Export module&lt;/code&gt;的部分，其中说明了实现原理以及如何搭建这样的一个格式转换服务器。从文档可以看出这个实现方法的核心是借助了&lt;a href=&apos;http://xmlgraphics.apache.org/batik/tools/rasterizer.html&apos;&gt;batik-rasterizer.jar&lt;/a&gt;这个Java工具包，它能将SVG转换成图片或PDF格式。&lt;/p&gt;
&lt;p&gt;从上述该问题解决方案的寻找过程可以看出，&lt;strong&gt;很多时候并不是问题有多复杂或有多难，而是我们根本没有明确业务需求，没有搞清楚真正需要解决的问题，而对模糊的问题描述自以为是地作出一些假设，然后蒙头去解决错误的问题，从而浪费了很多时间&lt;/strong&gt;。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>回顾2012，展望2013</title>
            <description>&lt;p&gt;过去的一年里发生了很多事情，很大一部分原来就已在&lt;a href=&apos;http://youngsterxyf.github.io/2012/01/01/2011-summary/&apos;&gt;2011年终-回顾与展望&lt;/a&gt;一文中提及---实习、找工作、毕业，除此之外还有：我和女朋友定亲了，总算朝着婚姻近了一步，哈哈。&lt;/p&gt;
&lt;h2&gt;实习&lt;/h2&gt;
&lt;p&gt;关于实习有太多的话想说。7个月的时间里浓缩了太多的欢乐，太欢乐了。原本以为我的读书生涯就要这么平淡无奇地结束了，没想到在这个结尾处竟然给了我个大惊喜，所谓惊喜并不是这份实习有多牛逼，而是遇到了一群欢乐的人，一群“重口味”的人，一群彪悍的人，而其中绝大部分是女人，噢，女生更恰当些。&lt;/p&gt;
&lt;p&gt;在G1C1，我快乐地写代码，上班是种享受，我想以后很可能不会再有这样的享受了。在G1C1，我逐步地发展成为一个吃货，所以毫无疑问地胖了，原本我以为自己会一直瘦下去。另外，我也黑了，因为经过了无数次地“被黑”，但她们说我应该高兴才对，她们“黑”我是因为“爱”我。关于“黑”这件事情，刚入职的时候，我是很同情wenbin的，因为见他被“黑”得体无完肤的，可我也没逃脱作为一个G1C1码农的宿命，wenbin走后，替代了他的角色，而我走后，comen则替代了我的角色，可惜之后就不会再有了。&lt;/p&gt;
&lt;p&gt;吃，是Google的特色，故在G1C1也不例外。因为G1C1不与其他部门在一起，所以没法吃食堂。但我们也不亏，老大带着我们吃遍了办公室方圆几里地叫得出名的饭店，吃饱吃好，并且一天一换。并且吃饭的场景实在不得不让码农感到幸福，男女比例经常是6：1，与“交大男女七比一，一对情侣三对基”的情形那是恰好相反，而且我们实验室甚至比七比一的情况还严重。所以我一直来回感受两种极端。又由于女生的食量多半偏小，作为男生，最后“扫盘”的工作那自然是义不容辞的，胖也就必然的结果了。&lt;/p&gt;
&lt;p&gt;因为工作内容比较多元化，所以G1C1实习生的专业背景与就读学校覆盖面很大，可以说是“一锅大杂烩”。专业不同，并没有妨碍交流，反而使工作氛围更加活跃，具备专业特色的阐述方式与内容相互碰撞交融。&lt;/p&gt;
&lt;p&gt;很想逐个介绍我所知道的G1C1er们，可苦于胸无半点墨水，只好作罢。但你们应知道，应相信，现在，以后，我都会一直念着你们，想着你们。感谢和你们一起度过的美好青春时光。&lt;/p&gt;
&lt;h2&gt;找工作&lt;/h2&gt;
&lt;p&gt;我的求职经历并不顺利，主要原因是对于求职的“求”字在认识上有所偏差。我不喜欢“求”，我认为找工作就和找对象一样，我想找你，你认可我，才行。我就这样，你不要我，拉倒。这种想法导致我并没有认真准备笔试面试。其实找工作和学校里的应试是一样一样的，所以你得做题，各种应试的题目，除了一流的公司，一般公司考的都是老题目或者类似的题目。另外，要注意找工作的目标不是向公司证明你的能力，而是拿到offer，“不择手段”地拿到offer。&lt;/p&gt;
&lt;p&gt;对自己未来几年做了基本的定位之后，我没有参加银行、国企一类公司的招聘，集中应聘技术型的私企外企大小公司，由于裸考裸面，结果多半不太理想。这里对于应聘的公司不做评价，求职的具体过程也不详述，只是真心感谢那些认可我赏识我的人，还有虽然拒了我但真心帮助我的人，谢谢你们。&lt;/p&gt;
&lt;p&gt;最后，腾讯收了我，虽然待遇和职位的工作内容不是很理想，但我想应该是个不错的机会，值得以后好好努力工作，感谢当时的几个面试官，也就是我以后的同事，当然还要特别感谢yuye同学，你也算是一个“奇葩”吧，哈哈，没有任何贬义哦。&lt;/p&gt;
&lt;p&gt;我之所以选择技术作为我的职业目标，一方面当然是因为我本来学的就是技术，但更重要的是因为做技术比较纯粹，我希望自己以后心里能一直很踏实安心。&lt;/p&gt;
&lt;h2&gt;毕业&lt;/h2&gt;
&lt;p&gt;说到毕业论文，一个字足以概括---“水”。哈哈，但幸好顺利毕业了，虽然过程很痛苦，很煎熬。现在的我无所事事，坐等毕业，哈哈。&lt;/p&gt;
&lt;p&gt;这次毕业与本科毕业有什么本质区别呢？那就是这次我是真的要结束读书的生活了，正式进入社会，需要承担的责任也是完全不相同的，并且以后我应该不会继续深造求学了。&lt;/p&gt;
&lt;p&gt;回顾十几年的读书生涯，实在难舍。&lt;/p&gt;
&lt;h2&gt;定亲&lt;/h2&gt;
&lt;p&gt;和女朋友相关的文字，我写得很少，自己也觉得有点对不住女朋友。其原因一方面是从我们认识到现在4年多的时间里，我正逐步地趋向沉默，文字表达越来越少；另一方面是我觉得幸福其实是一件挺私密的事情，不能多晒。所以，我不说并不是因为我不幸福，其实我一直幸福得偷着乐呢，哈哈。&lt;/p&gt;
&lt;p&gt;定亲的过程并没有想的那么顺利，要考虑很多问题，要和双方父母亲人沟通，特别是要和女朋友沟通，从恋爱逐步走向婚姻，会遇到很多很多现实的细节。面对现实，会产生矛盾，但没有什么大不了的，相爱的人请记住，矛盾没有什么大不了的，不要夸张了问题而放弃了情感。&lt;/p&gt;
&lt;h2&gt;谈谈自己&lt;/h2&gt;
&lt;p&gt;回顾从大一到现在的几年时间，我的变化应该算是蛮大的，我逐步地不再关注那些大层面上的事情，不去想那些形而上的问题。那些事情，那些问题，想不透，看不穿，也不会真的有解。这辈子，我们能做的其实很少很少，做些力所能及的事情也算是不浪费生命吧。也许有人认为这是“认命”了，认为我在逐渐成为一个“单向度”的人，但其实我还是想趁活着多扑腾几下的，按照自己的想法去活，只不过想法换了个方面。&lt;/p&gt;
&lt;h2&gt;未来一年&lt;/h2&gt;
&lt;p&gt;也得为这新的一年做点规划，定点目标啥的。可以从工作、健康、情感、技术等几个方面来谈吧。&lt;/p&gt;
&lt;p&gt;对于工作，我希望自己能够踏实些，逐步精通工作相关的技术，多认识一些人，对所处的技术行业有个更清晰的认识。&lt;/p&gt;
&lt;p&gt;身心健康应该始终放在第一位，要多运动锻炼，坚持跑步什么的，多出去走走，断然拒绝“宅”的习惯，特别是要多和女朋友出去玩，这个可以做个详细的计划。时间是靠自己安排的，理由也会有千万，但都不应接受其成为不注意身心健康的原因。&lt;/p&gt;
&lt;p&gt;情感方面，希望自己能够一如既往，并且更加耐心，细心，注意交流沟通。另外，要尽快完成婚姻的流程。&lt;/p&gt;
&lt;p&gt;说到技术，还是老毛病，有太多东西想学，不过现在学习方式在逐步进入良性循环，心态上也不会那么盲目了。接下来的一年里，自己必须不避重就轻，要攻关把核心技术练扎实，多写代码，多整理总结。&lt;/p&gt;
&lt;p&gt;过去的一年里，书籍阅读不多，也不见得是件坏事。不过新的一年里，应该要精读一些书，包括文史与技术类的，多思考。&lt;/p&gt;
&lt;p&gt;上述也说不上是规划，只是对自己提点要求和期望，希望自己在各方面都有所进步。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Python装饰器入门（译）</title>
            <description>&lt;p&gt;原文: &lt;a href=&apos;http://www.thumbtack.com/engineering/a-primer-on-python-decorators/&apos;&gt;A primer on Python decorators&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python允许你，作为程序员，使用函数完成一些很酷的事情。在Python中，函数是&lt;a href=&apos;http://en.wikipedia.org/wiki/First-class_function&apos;&gt;一等对象(first-class object)&lt;/a&gt;，这就意味着你可以像使用字符串，整数，或者任何其他对象一样使用函数。例如，你可以将函数赋值给变量:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; def square(n):
...     return n * n;
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; square(4)
16
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; alias = square
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; alias(4)
16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然而，一等函数的真正威力在于你可以把函数传给其他函数，或者从其他函数中返回函数。Python的内置函数map利用了这种能力：给map传个函数以及一个列表，它会依次以列表中每个元素为参数调用你传给它的那个函数，从而生成一个新的列表。如下所示的例子中应用了上面的那个square函数:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; number = [1, 2, 3, 4, 5]
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; map(square, numbers)
[1, 4, 9, 16, 25]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果一个函数接受其他函数作为参数，以及/或者返回一个函数，那么它就被称为&lt;a href=&apos;http://en.wikipedia.org/wiki/Higher-order_function&apos;&gt;高阶函数&lt;/a&gt; 。虽然map函数只是简单地使用了我们传给它的函数，而没有改变这个函数，但我们也可以使用高阶函数去改变其他函数的行为。&lt;/p&gt;
&lt;p&gt;例如，假设有这样一个函数，会被调用很多次，以致运行代价非常昂贵:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; def fib(n):
...      &amp;amp;quot;Recursively (i.e., dreadfully) calculate the nth Fibonacci number.&amp;amp;quot;
...      return n if n in [0, 1] else fib(n - 2) + fib(n - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们一般会保存计算过程中每次递归调用的结果，这样，对于函数调用树中经常出现某个n，当需要计算n对应的结果时，就不需要重复计算了。有多种方式可以做到这点。例如，我们可以将这些结果存在一个字典中，当以某个值为参数调用fib函数时，就先到这个字典去查一下其结果是否已经计算出来了。&lt;/p&gt;
&lt;p&gt;但这样的话，每次我们想要调用fib函数，都需要重复那段相同的字典检查样板式代码。相反，如果让fib函数自己在内部负责存储其结果，那么在其他代码中调用fib，就非常方便，只要简单地调用它就行了。这样一种技术被称为&lt;a href=&apos;http://en.wikipedia.org/wiki/Memoization&apos;&gt;memoization&lt;/a&gt;(注意没有字母r的哦)。&lt;/p&gt;
&lt;p&gt;我们可以把这种memoization代码直接放入fib函数，但是Python为我们提供了另外一种更加优雅的选择。因为可以编写修改其他函数的函数，那么我们可以编写一个通用的memoization函数，以一个函数作为参数，并返回这个函数的memoization版本:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def memoize(fn):
    stored_results = {}

    def memoized(*args):
        try:
            # try to get the cached result
            return stored_results[args]
        except KeyError:
            # nothing was cached for those args. let&amp;amp;apos;s fix that.
            result = stored_results[args] = fn(*args)
            return result
    return memoized&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如上， &lt;code&gt;memoize&lt;/code&gt; 函数以另一个函数作为参数，函数体中创建了一个字典对象用来存储函数调用的结果：键为被memoized包装后的函数的参数，值为以键为参数调用函数的返回值。 &lt;code&gt;memoize&lt;/code&gt; 函数返回一个新的函数，这个函数会首先检查在 &lt;code&gt;stored_results&lt;/code&gt; 字典中是否存在与当前参数对应的条目；如果有，对应的存储值会被返回；否则，就调用经过包装的函数，存储其返回值，并且返回给调用者。memoize返回的这种新函数常被称为&quot;包装器&quot;函数，因为它只是另外一个真正起作用的函数外面的一个薄层。&lt;/p&gt;
&lt;p&gt;很好，现在有了一个memoization函数，我们可以把fib函数传给它，从而得到一个经过包装的fib，这个版本的fib函数不需要重复以前那样的繁重工作:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def fib(n):
    return n if n in [0, 1] else fib(n - 2) + fib(n - 1)
fib = memoize(fib)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过高阶函数memoize，我们获得了memoization带来的好处，并且不需要对fib函数自己做出任何改变，以免夹杂着memoization的代码而模糊了函数的实质工作。但是，你也许注意到上面的代码还算有点别扭，因为我们必须写3遍fib。由于这种模式-传递一个函数给另一个函数，然后将结果返回给与原来那个函数同名的函数变量-在使用包装器函数的代码中极为常见，Python为其提供了一种特殊的语法：装饰器:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;@memoize
def fib(n):
    return n if n in [0, 1] else fib(n - 2) + fib(n -1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里，我们说memoize函数装饰了fib函数。需要注意的是这仅是一种语法上的简便写法(译注：就是我们常说的&quot;语法糖&quot;)。这段代码与前面的代码片段做的是同样的事情：定义一个名为fib的函数，把它传给memoize函数，将返回结果存为名为fib的函数变量。特殊的(看起来有点奇怪的)@语法只是减少了冗余。&lt;/p&gt;
&lt;p&gt;你可以将多个装饰器堆叠起来使用，它们会自底向上地逐个起作用。例如，假设我们还有另一个用来帮助调试的高阶函数:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def make_verbose(fn):
    def verbose(*args):
        # will print (e.g.) fib(5)
        print &amp;amp;apos;%s(%s)&amp;amp;apos; % (fb.__name__, &amp;amp;apos;, &amp;amp;apos;.join(repr(arg) for arg in args))
        return fn(*args)   # actually call the decorated function

    return verbose&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面的两个代码片段做的是同样的事情:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;@memoize
@make_verbose
def fib(n):
    return n if n in [0, 1] else fib(n - 2) + fib(n - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;def fib(n):
    return n if n in [0, 1] else fib(n - 2) + fib(n - 1)
fib = memoize(make_verbose(fib))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有趣的是，Python并没有限制你在@符号后只能写一个函数名：你也可以调用一个函数，从而能够高效地传递参数给装饰器。假设我们并不满足于简单的memoization，还想将函数的结果存储到&lt;a href=&apos;http://memcached.org/&apos;&gt;memcached&lt;/a&gt;中。如果你已经写了一个 &lt;code&gt;memcached&lt;/code&gt; 装饰器函数，那么可以(例如)传递一个服务器地址给它:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;@memcached(&amp;amp;apos;127.0.0.1:11211&amp;amp;apos;)
def fib(n):
    return n if n in [0, 1] else fib(n - 2) + fib(n - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;非装饰器语法的写法会如下展开:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;fib = memcached(&amp;amp;apos;127.0.0.1:11211&amp;amp;apos;)(fib)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python配备有一些作为装饰器使用的非常有用的函数。例如，Python有一个 &lt;code&gt;classmethod&lt;/code&gt; 函数，可以创建大致类似于java的静态方法:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;class Foo(object):
    SOME_CLASS_CONSTANT = 42

    @classmethod
    def add_to_my_constant(cls, value):
        # Here, `cls` will just be Foo, buf if you called this method on a
        # subclass of Foo, `cls` would be that subclass instead.
        return cls.SOME_CLASS_CONSTANT + value

Foo.add_to_my_constant(10)  # =&amp;amp;gt; 52

# unlike in Java, you can also call a classmethod on an instance
f = Foo()
f.add_to_my_constant(10)    # =&amp;amp;gt; 52&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;旁注：文档字符串&lt;/h2&gt;
&lt;p&gt;Python函数可以包含更多的信息，而不仅仅是代码：它们也包含有用的帮助信息，比如函数名称，文档字符串:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; def fib(n):
...     &amp;amp;quot;Recursively (i.e., dreadfully) calculate the nth Fibonacci number.&amp;amp;quot;
...     return n if n in [0, 1] else fib(n - 2) + fib(n - 1)
...
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__name__
&amp;amp;apos;fib&amp;amp;apos;
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__doc__
&amp;amp;apos;Recursively (i.e., dreadfully) calculate the nth Fibonacci number.&amp;amp;apos;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python内置函数&lt;a href=&apos;http://docs.python.org/library/functions.html#help&apos;&gt;help&lt;/a&gt;输出的就是这些信息。但是，当函数被包装之后，我们看到就是包装器函数的名称和文档字符串了:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib = memoized(fib)
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__name__
&amp;amp;apos;memoized&amp;amp;apos;
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__doc__&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;那样的信息并没有什么用处。幸运的是，Python包含一个名为 &lt;code&gt;functools.wraps&lt;/code&gt; 的助手函数，能够把函数的帮助信息拷贝到其包装器函数:&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;&lt;code&gt;import functools
def memoize(fn):
    stored_results = {}
        
    @functools.wraps(fn)
    def memoized(*args):
        # (as before)

    return memoized&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用装饰器帮助你编写装饰器会使很多事情令人非常满意。现在，如果使用更新过的memoize函数重试前面的代码，我们将会看到得到保留的文档:&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib = memoized(fib)
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__name__
&amp;amp;apos;fib&amp;amp;apos;
&amp;amp;gt;&amp;amp;gt;&amp;amp;gt; fib.__doc__
&amp;amp;apos;Recursively (i.e., dreadfully) calculate the nth Fibonacci number.&amp;amp;apos;&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        
        <item>
            <title>关于技术的学习方法</title>
            <description>&lt;p&gt;关于学习，时间短与效果好始终是一对矛盾的统一体。&lt;/p&gt;
&lt;p&gt;很多时候，要想在最短的时间内完成一件事情，最好的方法就是依葫芦画瓢，但这样的话，即使完成了事情，也只是知其然而不知其所以然，长久来看，对于学习者的能力不会有多大的提高。&lt;/p&gt;
&lt;p&gt;从长远来看，要想自己基础扎实，能力强，那就得一步一步的来，从基础知识开始，一点一点的搞懂，但这种方式需要花费很多时间，短时间内效果不明显。而且，可能效果没有预期的那么好。&lt;/p&gt;
&lt;p&gt;那么，如果做个权衡呢？&lt;/p&gt;
&lt;p&gt;我想，也许最好的学习方式是：先依葫芦画瓢地实践，获得一些直观感受，最好还有一些疑问。在实践完成之后，在整理自己的疑问，以及实践中涉及的知识要点，通过查阅图书或者网络资料，逐个知识点巩固，逐个解决疑问，并整理成文。这个整理总结的过程可能需要较长的时间。&lt;/p&gt;
&lt;p&gt;这种方式的优势在于：1.能让你快速地完成事情；2.实践中用到的知识多半会在以后的实践中经常用到，掌握的就是一些最重要的东西，而不会学习一些很少使用的深奥偏门知识。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>学习的"道"与"术"</title>
            <description>&lt;p&gt;读研以来，一直觉得自己的学习方法不够高效。试图将要学习的东西进行分类，然后以不同的方法学习之。那么该如何分类呢？我觉得以&quot;道&quot;与&quot;术&quot;区分之比较合适。&lt;/p&gt;
&lt;p&gt;何为&quot;道 &quot;？汉语辞典中有两条解释：&lt;strong&gt;1.指法则、规律；2.学术或宗教的思想体系&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;何为&quot;术&quot;？：&lt;strong&gt;技艺&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;字面理解，“术”更为具体，是完成一件事情的具体过程。而“道”者则是指导实践的思想，是能够举一反三的事物规律。&lt;/p&gt;
&lt;p&gt;那么是否“道”比“术”更重要呢？我想未必。任何理论，任何“道”都最终来源于“术”的实践过程，也最终需要在“术”上得到实施，才能体现其价值。“道”与“术”两者相辅相成。那么在我们学习一门学问的过程中，就存在一个“道”与“术”何者为先的问题，即从“道”还是“术”入手学习？&lt;/p&gt;
&lt;p&gt;孟岩说过这么一段话：&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;我主张,在具备基础之后,学习任何新东西,都要抓住主线,突出重点。对于关键理论的学习,要集中精力,速战速决。而旁枝末节和非本质性的知识内容,完全可以留给实践去零敲碎打。&lt;/p&gt;
&lt;p&gt;原因是这样的,任何一个高级的知识内容,其中都只有一小部分是有思想创新、有重大影响的,而其它很多东西都是琐碎的、非本质的。因此,集中学习时必须把握住真正重要那部分,把其它东西留给实践。对于重点知识,只有集中学习其理论,才能确保体系性、连贯性、正确性,而对于那些旁枝末节,只有边干边学能够让你了解它们的真实价值是大是小,才能让你留下更生动的印象。如果你把精力用错了地方,比如用集中大块的时间来学习那些本来只需要查查手册就可以明白的小技巧,而对于真正重要的、思想性东西放在平时零敲碎打,那么肯定是事倍功半,甚至适得其反。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;虽然这段话并没有明确区分学习的“道”与“术”，以及何者为先的问题。但却大致说明了何为正确的学习方法。&lt;/p&gt;
&lt;p&gt;从“道”与“术”的角度来理解，那关键的，核心的，创新的部分即为“道”，“大道”。而那细节的则是“术”的部分，是需要长时间的实践的，也许只有在实践中遇到的细节才是有意义的。&lt;/p&gt;
&lt;p&gt;但那“具备基础之后”的“基础”是什么呢？我想应该是：1.明确问题是什么。这一点是再怎么强调都不为过的。要解决一个问题却没有真正明确问题到底是什么，那你努力多半是白费的。2.这东西是用来干什么用的，是用来解决什么问题的？对于工科学生来说，学习新东西的时候，这一点是需要首先明确的，只有明确了“干什么用的”，才能抓住学习的重点，提高学习的效率。3. 与以前的类似的东西相比，其区别是什么？一样东西，一种理论其价值往往在于对前人的突破，这突破的地方才是我们真正要掌握的。在学习一样东西之前，不妨多问问自己为什么要学这个，这东西对自己有多大的提升？不断地重复学习类似的东西，多半是没有意义的。&lt;/p&gt;
&lt;p&gt;具备了基础之后，对于关键理论的学习，是不是只要抱着书本，理论对理论的学习就行呢？我想这是万万不可的。特别是对于着重于实践性的学问，比如编程，理论对理论地学习，只会让你吃力不讨好。绝大多数的创新理论，核心理论，都不是一下子就能理解的，特别是当你对这一领域的学问并不熟悉的情况下，它需要在反复的实践中逐步地加深理解。&lt;/p&gt;
&lt;p&gt;那么对于关键内容的学习，我觉得这样学习会比较合适：先快速地，在尽可能短的时间内把关键内容浑沦吞枣地过一遍，能理解多少是多少，目的是为了获得一个理论的一个Big Picture，明确理论的各个部分之间的大致关系。然后对于每个部分，以及部分之间的关系，逐个地通过实践来验证你的理解，但这个实践过程并不属于“术”，因为它不是为了技艺，而只是为了验证自己对理论的理解。这一验证过程结束之后，你应该就能够对关键理论有个整体的正确的理解了。&lt;/p&gt;
&lt;p&gt;然后，你就放开手去干吧，去解决那些现实中的问题！&lt;/p&gt;
&lt;p&gt;也许，你会说，不用这么复杂吧？是的，如果你只是为了解决一个实际的问题，而这个问题也存在相近的解决方案，你不想也不需要弄懂这个问题，好吧，那你就直接去找解决方案吧。但你解决这个问题之后，你学到了什么呢？当你再次遇到一个本质上一样的问题的时候，你还是能快速地解决么？没有真正弄懂问题，没有弄懂问题背后的知识，那你就准备着为类似的问题重复地去寻找解决方案吧。恩，看起来有点傻哦。&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Python学习路线(针对具备一定编程经验者)</title>
            <description>&lt;p&gt;相比C,C++,JAVA等编程语言，Python是易学的。但要想深入地理解Python，并熟练地编写Python风格的Python代码。我想还是有一长段路程要走的。下面即是我的一点经验总结，主要是为了整理自己学习的思路。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;花1-2天的时间阅读一本好的Python入门书籍，并在亲手实践书中的代码。推荐入门书籍：《A byte of Python》(中文翻译《简明Python教程》)或《Practical Programming:An Introduction to Computer Science Using Python》(中文翻译《Python实践教程》)或者其他的比较薄的入门书籍。&lt;/li&gt;
&lt;li&gt;抛开书籍，用Python去写一切你想写的程序。这时最好的参考文档即为：(1).Python命令解释器中的help(),dir()辅助方法；(2).Python官网文档：&lt;a href=&apos;http://docs.python.org/&apos;&gt;http://docs.python.org/&lt;/a&gt; 。遇到不清楚的地方就用这两个方法查，再不行就去google一下。&lt;/li&gt;
&lt;li&gt;两三个月之后，积累一点的代码量，再重新找本讲解比较详细的书，重新梳理一下自己对Python的理解，纠正自己实践中一些不好的方式。推荐书籍：《Beginning Python: From Novice to Professional》(中文翻译《Python基础教程》)，《Learning Python》(中文翻译《Python学习手册》)，《Dive into Python》，《Core Python Programming》等。另外，也应该在编码的过程中重复地去查阅Python标准函数库，标准库里已有模块实现的功能就不要自己实现。&lt;/li&gt;
&lt;li&gt;之后，根据实际需要，去了解使用一下Python的各个方面的函数库(比如http://docs.python.org/modindex.html中罗列出来的，以及matplotlib, numpy等用于科学计算，图形图像处理的)，特别是诸多的Web框架(django, web2py, cherrypy, tornado等)，可以先从简单的开始。如果是对Python的底层实现感兴趣，那么就该去看看Python源码，阅读一下《Python源码剖析》; 如果对文本处理感兴趣，可以阅读一下《Text processing in Python》等； 如果对网络感兴趣，可以阅读《Foundations of Python Network Programming》，尝试实现一个简单的web server ...&lt;/li&gt;
&lt;li&gt;Python相关的开源函数库非常非常的多，各个方面的都有，所以学习者应该尝试着去用它们，了解它们，而不是啥都要自己来实现。因为Python擅长的就是快速开发，而且站在前人的肩膀上，我们才能站得更高，看得更远。当然如果你想加深自己对某个方面的理解，也可以尝试去实现一些简单的模块。&lt;/li&gt;
&lt;li&gt;总之一句话：学习Python的关键就是用！而且是要多用别人的。动手实践才是王道！那么多优秀的开源函数库不要浪费了！&lt;/li&gt;&lt;/ol&gt;</description>
        </item>
        
        <item>
            <title>2011年终-回顾与展望</title>
            <description>&lt;p&gt;昨晚实验室聚餐，和师兄们喝醉了，明年的这个时候，我也就和他们一样将要毕业。时间，总是往前看觉得很漫长，可回过头去看看，一年也就是瞬间的事情。&lt;/p&gt;
&lt;p&gt;2011，我从研一走向研二，2012，我将从研二走向研三，继而毕业，工作。&lt;/p&gt;
&lt;p&gt;回顾过去一年，于我自己而言，过得很平淡，也许是大学以来最平淡的一年，只能说也许，因为对于2011，我记不得太多的事情。&lt;/p&gt;
&lt;p&gt;这一年里，我，一个技术男，比以前更宅，话也相对少了很多，直接表现为QQ空间或者校内上的文字写得很少。很少和别人谈论自己，因为我觉得纠结于那个“小我”是件很“小青年”的事情。人与人之间不可避免的隔膜导致了个人的事情不管多大在别人眼里都是微不足道的，在别人的心里掀不起半点波澜，说过了也就忘了。所以那些关于自己的，还是放在心里比较好，毋须说些没意义的。&lt;/p&gt;
&lt;p&gt;这一年里，我想得挺多，但真正做了或者说做好的却很少。这是件严重的事情。特别在技术上，东看西看，东学西学，眼界确实开阔很多，也养成了较为良好的技术趣味。但从技术能力上来说，真不好说，我都不知道自己有几斤几两。&lt;/p&gt;
&lt;p&gt;这一年里，最大的收获，也许是对“学习”的重新认识，以及试图从“学知识”向“做事情”转变(这里的“做事情”特指“解决实际问题”)，以前的自己太喜欢太沉迷于学东西，而忽略了自主地做事情。“生有涯，而知无涯”的无奈是必须面对的，对于有限的人生来说，知识必须对自己有用才值得学，特别是技术相关的知识，那怎么知道哪些知识对自己有用呢？得“边用边学”，需要用的时候再学。所以应多做事情，应找实际的问题，尝试去解决，在解决问题的过程中学习。解决实际问题才是根本，才是目的，而不是学习。学生时代习惯了学，习惯了边学边用，但对于研究生，对于以后的工作来说，光顾着学是没用的，而且一味的学也是非常难以深入的，要对某个方面有个深入的理解，必须通过做事情，发现问题，解决问题。&lt;/p&gt;
&lt;p&gt;这一转变过程让我非常纠结，而且到目前为止还算不上成功。可能对于很多人来说，这个过程可能很简单，但对于我来说，好奇心强，喜欢学东西的来说，确实极其艰难的。每做一件事情，都很可能陷入学习的状态，而不会及时地适可而止。&lt;/p&gt;
&lt;p&gt;这一过程希望在2012年有个很好的进展。&lt;/p&gt;
&lt;p&gt;这一年里，我逐步意识到自己存在的另一个大问题---不够自信，其外在表现为和别人一起时，极其容易将自己的观点让位于别人的观点，即使我并不认为自己的观点存在什么问题，或者别人的观点比我的更正确。我只是不想凌驾于别人之上，不想让自己影响到别人。这听起来似乎不关乎自信，但我想本质上是的。“不够自信”这看起来也似乎不是什么大问题，但我想对于一个成年人，特别是一个男人，自信是非常重要的，对于事业也是件非常重要的事情。以前自己活得太谨慎，不喜欢在公共场合表现自己，觉得提高自己的内在修为才是重要的。但其实活在社会中，适时适当地表现自己，表达自己的观点，坚持自己的看法是非常重要的。有个词叫“内圣外王”，以后我得更加注重“外王”，内外兼修。&lt;/p&gt;
&lt;p&gt;这一年里，我活得又很随性(和“活得谨慎”矛盾而共存)，不太注意自己的言行，脏话口头禅也比较多，虽然在我看来，我的脏话全是在表达一种情绪，仅此而已。但我想在别人来看，其感受也许并不是这样的。我们年轻人还是要注意自己的形象的，呵呵。另外，尽量少评论别人，特别是别人的缺点。&lt;/p&gt;
&lt;p&gt;这一年里，我很懒散，自己都有些看不过去了，“这样的日子，再好的，没有了”，我想不是件好事情。能够自我安慰点是还读了点书，整理一下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;社科文艺类&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;佛祖在一号线&lt;/li&gt;
&lt;li&gt;书虫小札&lt;/li&gt;
&lt;li&gt;复杂性思想导论&lt;/li&gt;
&lt;li&gt;人生&lt;/li&gt;
&lt;li&gt;给研究生的学术建议&lt;/li&gt;
&lt;li&gt;边城&lt;/li&gt;
&lt;li&gt;朱镕基答记者问&lt;/li&gt;
&lt;li&gt;窗里窗外&lt;/li&gt;
&lt;li&gt;联大八年&lt;/li&gt;
&lt;li&gt;笑谈大先生&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;技术类&lt;/strong&gt; (部分是按需选取章节阅读)&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Shell脚本学习指南&lt;/li&gt;
&lt;li&gt;Linux内核设计与实现&lt;/li&gt;
&lt;li&gt;鸟哥的Linux私房菜&lt;/li&gt;
&lt;li&gt;PHP和MySQL Web开发（原书第3版）&lt;/li&gt;
&lt;li&gt;分布式处理实践&lt;/li&gt;
&lt;li&gt;Java Collections&lt;/li&gt;
&lt;li&gt;可爱的Python&lt;/li&gt;
&lt;li&gt;浪潮之巅&lt;/li&gt;
&lt;li&gt;CSS Web设计快速上手&lt;/li&gt;
&lt;li&gt;编程人生&lt;/li&gt;
&lt;li&gt;C陷阱与缺陷&lt;/li&gt;
&lt;li&gt;调试九法&lt;/li&gt;
&lt;li&gt;黑客与画家&lt;/li&gt;
&lt;li&gt;Python UNIX 和Linux 系统管理指南&lt;/li&gt;
&lt;li&gt;版本控制之道&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;比较而言，数量上不算少，也不算多，但主要问题还是质量上，觉得自己有些浑沦吞枣，读得太匆忙。这是以后的阅读需要注意的，克制焦躁的心理，慢慢阅读消化。&lt;/p&gt;
&lt;p&gt;新的一年，具体来说，我需要面对的几件大事情，大致包括:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;小论文&lt;/li&gt;
&lt;li&gt;实习&lt;/li&gt;
&lt;li&gt;毕业论文，包括答辩&lt;/li&gt;
&lt;li&gt;找工作&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;2012年底，我将要毕业了(虽然，形式上是2013年毕业)，我就要工作了，这也许是和高考相当的一件大事，希望自己能够举重若轻，踏实准备，顺利应对。(再具体点是不是应该说多多coding，多做事情！哈哈)&lt;/p&gt;</description>
        </item>
        
        <item>
            <title>Linux添加定时任务</title>
            <description>&lt;p&gt;在Linux下如果希望某个任务定时地执行，一般是使用cron服务器，将任务添加到cron任务列表中。&lt;/p&gt;
&lt;h4&gt;启动，关闭，重启cron(需超级用户权限)&lt;/h4&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;/etc/init.d/cron start
/etc/init.d/cron stop
/etc/init.d/cron restart&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注:archlinux下为/etc/rc.d/crond start|stop|restart&lt;/p&gt;
&lt;h4&gt;查看用户设置的定时任务列表&lt;/h4&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;crontab [-u xxx] -l       #  xxx为用户名&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;编辑用户的定时任务列表(超级用户权限)&lt;/h4&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;crontab -u xxx -e&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;删除用户的定时任务列表(超级用户权限)&lt;/h4&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;crontab -u xxx -r&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;定时任务的编辑规则&lt;/h4&gt;
&lt;p&gt;cron的定时任务由两部分组成：（1）设置的时间（2）该时间下要执行的任务命令。&lt;/p&gt;
&lt;p&gt;时间分5个部分，依次为：&lt;/p&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;minute              0-59
hour                0-23 
day of month        1-31
month               1-12
day of week         0-7 (0 or 7 is Sun, or use names)&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;示例（每天临晨2点备份数据库）&lt;/h4&gt;
&lt;pre class=&quot;language-text&quot;&gt;&lt;code&gt;0 2 * * * mysqldump -hhostname -uusername -ppassword databasename &amp;amp;gt; backupfile.sql&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;使设置生效&lt;/h4&gt;
&lt;p&gt;设置完成后，重启cron即可使设置的计划任务定时执行了&lt;/p&gt;
&lt;h3&gt;详细内容参考&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&apos;http://fanqiang.chinaunix.net/system/linux/2005-06-13/3306.shtml&apos;&gt;http://fanqiang.chinaunix.net/system/linux/2005-06-13/3306.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://fanqiang.chinaunix.net/adm/storage/2005-03-23/2985.shtml&apos;&gt;http://fanqiang.chinaunix.net/adm/storage/2005-03-23/2985.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&apos;http://now-code.com/archives/196&apos;&gt;http://now-code.com/archives/196&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>